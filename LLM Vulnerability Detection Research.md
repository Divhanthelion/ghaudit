# **LLM-Assisted Vulnerability Detection: A Comprehensive Analysis of Comparative Efficacy, Engineering Strategies, and Economic Viability in Rust Environments**

## **Executive Summary**

The integration of Large Language Models (LLMs) into the domain of software security represents a pivotal shift from deterministic, rule-based analysis to probabilistic, semantic reasoning. Traditional Static Application Security Testing (SAST) tools, while indispensable for identifying syntactic irregularities and known insecure patterns, historically suffer from high false-positive rates and an inherent inability to comprehend business logic or complex control flow vulnerabilities. The emergence of frontier models—specifically OpenAI's GPT-4, Anthropic's Claude 3, and Meta's Llama 3—offers a mechanism to bridge this gap, provided their deployment is engineered with architectural precision and domain-specific adaptation.

This report, spanning the theoretical, experimental, and economic dimensions of AI-driven security, specifically addresses the enhancement of the src/ai/mod.rs module. The analysis indicates that while no single model currently serves as a panacea, a hybrid architecture leveraging the superior context handling of Claude 3.5 Sonnet or the reasoning breadth of GPT-4o, combined with the cost-efficiency and customizability of fine-tuned Llama 3 models, outperforms traditional methods in specific vulnerability classes.

The investigation reveals that the efficacy of LLMs in security analysis is non-linearly correlated with prompt complexity and the integration of external verifiers. We demonstrate that techniques such as Chain-of-Thought (CoT) prompting and "Role-Playing" significantly reduce hallucination rates, while the integration of compiler feedback loops (e.g., cargo check) is essential for resolving memory safety issues in Rust that LLMs cannot "see" in static text. Furthermore, the economic analysis suggests that while the raw compute cost of LLM analysis is orders of magnitude higher than SAST per line of code, the Total Cost of Ownership (TCO) shifts favorably when factoring in the engineering time saved by reduced false positives.

## ---

**1\. Introduction: The Evolution of Automated Vulnerability Detection**

### **1.1 The Deterministic Limit: Traditional SAST**

For over two decades, Static Application Security Testing (SAST) has served as the bedrock of automated software assurance. Tools such as CodeQL, SonarQube, and Checkmarx operate fundamentally on Abstract Syntax Trees (ASTs) and Control Flow Graphs (CFGs). They function by querying the codebase for known "bad patterns," such as unsanitized user inputs flowing into SQL execution sinks or the use of deprecated cryptographic functions.1

The existing heuristic fallback mechanism in the user's module (src/ai/mod.rs:214-282) likely reflects this deterministic approach: relying on regex-based pattern matching or simple AST traversal to flag potential issues. While computationally efficient, this approach faces a "binary outcome dilemma." To minimize the risk of missing vulnerabilities (false negatives), SAST rules are typically tuned to be aggressive, flagging any code structure that *resembles* a vulnerability. This results in high false-positive rates—often exceeding 50%—which forces security engineers to spend substantial time triaging alerts rather than remediating actual threats.3

Moreover, deterministic tools suffer from "context blindness." They excel at identifying local syntactic errors but struggle with multi-file logic flaws or vulnerabilities that depend on the *intent* of the code rather than its structure. For instance, a SAST tool can detect a missing authorization check on a function, but it cannot determine whether that function is intended to be a public endpoint or an internal utility, a distinction that relies on understanding the broader business logic.5

### **1.2 The Semantic Shift: Enter Large Language Models**

Large Language Models (LLMs) introduce a semantic layer to automated analysis. Unlike SAST, which parses syntax, LLMs process code as natural language, allowing them to infer intent, variable relationships, and complex control flows that span multiple files. The theoretical advantage of LLMs lies in their ability to detect **Logic Bombs** and **Design Flaws**—categories traditionally reserved for manual human review.

In the context of Rust development, this semantic capability is particularly pertinent. While the Rust compiler's borrow checker eliminates entire classes of memory safety bugs (e.g., dangling pointers, double frees) in safe code, "unsafe" blocks remain a critical attack surface. A traditional SAST tool might flag every unsafe block as a potential risk, creating noise. An LLM, conversely, can analyze the surrounding invariants—such as whether a raw pointer dereference is protected by length checks—to determine if the unsafe contract is actually violated.6

However, the probabilistic nature of LLMs introduces a new challenge: hallucination. Models may invent vulnerabilities that do not exist or recommend fixes that fail to compile. This necessitates a rigorous evaluation of model performance, prompt strategies, and integration architectures to harness their power while mitigating their unpredictability.4

### **1.3 Research Objectives**

This report addresses four critical questions to guide the evolution of the src/ai/mod.rs module:

1. **Comparative Efficacy**: How do state-of-the-art models (GPT-4, Claude 3, Llama 3\) compare in detecting vulnerabilities, specifically within benchmarks like the Juliet Test Suite and SEC-bench?  
2. **Prompt Engineering**: What techniques maximize accuracy and minimize hallucinations, particularly for Rust's strict safety guarantees?  
3. **Customization**: Is it viable to fine-tune open-weights models (Llama 3\) on vulnerability-labeled code to create specialized security agents?  
4. **Economic Viability**: How does the cost/benefit profile of LLM analysis compare to traditional SAST when accounting for token costs and engineering overhead?

## ---

**2\. Comparative Analysis of Large Language Models**

To engineer an effective vulnerability detection module, one must select the underlying model based on a matrix of reasoning capability, context window size, and cost. The current landscape is dominated by three primary families: OpenAI's GPT series, Anthropic's Claude series, and Meta's Llama series.

### **2.1 Benchmark Architectures: SEC-bench and Juliet**

The evaluation of these models relies on specialized benchmarks that simulate real-world security challenges.

* **The Juliet Test Suite**: Originally developed by NIST, this suite consists of thousands of synthetic test cases in C/C++ and Java, labeled with Common Weakness Enumeration (CWE) identifiers. While extensive, its synthetic nature often lacks the complexity of real-world software, making it a baseline rather than a definitive test of reasoning.8  
* **SEC-bench**: A novel framework designed to evaluate LLM agents on authentic security engineering tasks, specifically Proof-of-Concept (PoC) generation and vulnerability patching. Unlike Juliet, SEC-bench uses real vulnerabilities from open-source projects, providing a more rigorous assessment of a model's ability to navigate complex codebases.10  
* **CyberSecEval**: Developed by Meta, this benchmark evaluates a model's propensity to generate insecure code and its ability to assist in cyberattacks, providing insights into model safety alignment.11

### **2.2 Model Performance Analysis**

#### **2.2.1 Claude 3 (Sonnet & Opus)**

The Claude 3 family, particularly the **3.5 Sonnet** and **3.7 Sonnet** iterations, has emerged as a leader in security-specific tasks. In SEC-bench evaluations, Claude 3.7 Sonnet achieved a **34.0% success rate** in patching vulnerabilities, significantly outperforming other models in the cohort.10 This performance is attributed to its superior reasoning capabilities and "long-context recall," which allows it to maintain coherence over large codebases.

Claude's architecture supports a context window of up to 200k tokens (and theoretically higher), which is critical for vulnerability detection in Rust, where a vulnerability might hinge on a struct definition located in a different module than its usage. The "Needle in a Haystack" capability—retrieving specific details from massive prompts—makes Claude particularly well-suited for analyzing the comprehensive context required to identify cross-function taint propagation.13

#### **2.2.2 GPT-4 (GPT-4o)**

GPT-4 remains the industry standard for general reasoning and instruction following. In the SEC-bench evaluation, GPT-4o resolved approximately **26.2% of patches** when paired with the SWE-agent scaffolding, placing it behind Claude 3.7 Sonnet but ahead of many open-source alternatives.10

A key strength of GPT-4 is its **adversarial robustness**. Research indicates that while smaller models like CodeLlama may suffer accuracy degradation of up to 40% when subjected to adversarial noise (e.g., variable renaming, dead code insertion), GPT-4 typically experiences degradation of less than 10%.14 This robustness makes GPT-4o a reliable choice for the "Verifier" role in a hybrid architecture, where it can double-check the findings of less robust models.

#### **2.2.3 Llama 3 (70B & 405B)**

Llama 3 represents the pinnacle of open-weights modeling, offering a compelling alternative to proprietary APIs. The **Llama 3.1 70B** model serves as a massive cost-saver for high-volume analysis. While its zero-shot performance in vulnerability detection generally lags behind GPT-4 and Claude Opus, it rivals GPT-4 Turbo in standard coding benchmarks like HumanEval and MBPP.15

For security tasks, Llama 3 requires more specific guidance. Benchmarks indicate that while it captures syntactic vulnerabilities effectively, it struggles with complex reasoning chains without fine-tuning or few-shot prompting. However, its open nature allows for **fine-tuning on domain-specific datasets** (discussed in Chapter 4), which can elevate its performance to match or exceed generic proprietary models for specific tasks like Rust safety analysis.17

### **2.3 Synthesis of Capabilities**

The following table synthesizes the comparative strengths and weaknesses of the evaluated models based on the aggregated research data.

| Feature | GPT-4o | Claude 3.5/3.7 Sonnet | Llama 3.1 70B | Llama 3.1 405B |
| :---- | :---- | :---- | :---- | :---- |
| **Primary Strength** | Adversarial Robustness & General Reasoning | Deep Context Recall & Security Patching | Cost/Performance Ratio & Customizability | Deep Reasoning & Open Availability |
| **SEC-bench Patching** | \~26.2% 10 | **34.0%** 10 | N/A (Est. \~20-25%) | N/A (Est. High) |
| **PoC Generation** | \~3.8% 10 | **18.0%** 10 | Low Zero-Shot | Moderate |
| **Context Window** | 128k | **200k+** | 128k | 128k |
| **Deployment Model** | SaaS API | SaaS API | Self-Host / API | Self-Host / API |
| **Vulnerability Detection** | High Precision | High Recall (Low False Positive) | Moderate Precision (Needs Tuning) | High Precision |

The data suggests a tiered approach: **Claude 3.7 Sonnet** is the optimal choice for deep, high-stakes analysis where accuracy is paramount. **GPT-4o** serves as a robust generalist, particularly useful for initial triage or adversarial environments. **Llama 3 70B** is the workhorse for scalable, cost-sensitive scanning, provided it is properly instructed or fine-tuned.

## ---

**3\. Engineering the Prompt Architecture for Security**

The user's current prompt strategy (src/ai/mod.rs:440-464) relies on a basic system instruction: *"You are an expert security code reviewer..."*. While this establishes a persona, research indicates that it is insufficient for high-fidelity security analysis. To maximize accuracy and minimize hallucinations, the system prompt must evolve into a structured **Prompt Engineering Architecture** that leverages Chain-of-Thought (CoT) reasoning, few-shot learning, and feedback loops.

### **3.1 The Cognitive Science of Chain-of-Thought (CoT)**

Standard zero-shot prompting often leads LLMs to generate "gut reaction" verdicts based on surface-level keywords. For example, a model might flag the use of strcpy in C or unsafe in Rust as a vulnerability without analyzing the surrounding context that might make the usage safe. This is analogous to "System 1" thinking in human psychology—fast, intuitive, but prone to error.

**Chain-of-Thought (CoT)** prompting forces the model to engage in "System 2" thinking—slow, deliberate, and logical. By requiring the model to articulate its reasoning steps *before* declaring a verdict, we decouple the reasoning process from the final classification. Research shows that this decomposition significantly improves performance on complex reasoning tasks.19

For Rust vulnerability detection, the CoT process should be explicitly structured to mirror the mental model of a human security auditor:

1. **Identification**: List all external entry points and unsafe blocks.  
2. **Invariant Analysis**: For each unsafe block, explicitly state the invariants required (e.g., pointer validity, buffer length).  
3. **Data Flow Tracing**: Trace user-controlled inputs from entry points to sensitive sinks.  
4. **Verification**: Analyze whether the surrounding safe code upholds the required invariants.  
5. **Conclusion**: Only after these steps should the model determine if a vulnerability exists.7

### **3.2 Few-Shot Prompting: Calibrating the Model**

Zero-shot performance is highly variable and often uncalibrated. **Few-shot prompting**—the technique of providing examples of input code and the desired output format within the prompt—stabilizes the model's response structure and calibrates its detection threshold.19

For the user's module, the system prompt should include 2-3 carefully curated examples:

* **True Positive Example**: A complex **Use-After-Free (CWE-416)** scenario involving interior mutability or RefCell, showing the model how to trace the lifetime of the reference.  
* **True Positive Example**: A **Race Condition (CWE-362)** involving Arc\<Mutex\<T\>\> where the lock is dropped prematurely or accessed incorrectly.  
* **False Positive Example**: A scenario where unsafe is used correctly (e.g., slice::from\_raw\_parts with proper length checks), teaching the model *not* to flag every instance of unsafe code.22

### **3.3 The "Compiler-in-the-Loop" Architecture (Tool Use)**

A critical insight from recent research is that LLMs are poor compilers. They often "hallucinate" syntax errors or misunderstand the strict rules of the Rust borrow checker. Relying solely on the LLM's internal weights for syntax verification leads to a high rate of invalid fixes or false reports of compilation errors.

The most advanced prompt engineering technique for Rust is the integration of **Agentic Tool Use**. The LLM should not be an isolated oracle but an agent with access to the cargo toolchain.

* **Mechanism**: The architecture allows the LLM to emit a "Tool Call" (e.g., run\_cargo\_check) instead of a final answer. The system executes cargo check on the code and feeds the compiler's stderr output back into the LLM's context window.  
* **Benefit**: This feedback loop allows the LLM to align its semantic reasoning with the compiler's strict syntactic reality. If the LLM proposes a fix that violates borrowing rules, the compiler feedback allows it to self-correct before presenting the solution to the user.23

### **3.4 Proposed System Prompt Design**

Based on these principles, the SECURITY\_SYSTEM\_PROMPT in src/ai/mod.rs should be restructured as follows:

Rust

const SECURITY\_SYSTEM\_PROMPT: &str \= r\#"  
You are a specialized Rust Security Auditor Agent. Your goal is to identify cryptographic failures, memory safety violations, and logic flaws.

\#\#\# ANALYSIS PROTOCOL (Chain of Thought):  
1\. \*\*Surface Analysis\*\*: Identify all entry points (public functions) and 'unsafe' blocks.  
2\. \*\*Invariant Check\*\*: For every 'unsafe' block, explain the safety contract required by the raw pointers or FFI calls.  
3\. \*\*Data Flow\*\*: Trace user-controlled input from entry points to sensitive sinks (database, shell, memory allocation).  
4\. \*\*Compiler Verification\*\*: Specify if the potential vulnerability would be caught by the Rust compiler (borrow checker). If yes, mark as 'Compiler Enforced' and do not report as a security vulnerability unless it bypasses the compiler.

\#\#\# OUTPUT FORMAT:  
Respond strictly in JSON:  
{  
  "thoughts": "Step-by-step reasoning...",  
  "vulnerabilities":  
}

\#\#\# FEW-SHOT EXAMPLES:

\[Insert example of safe usage of unsafe (False Positive)\]  
"\#;

## ---

**4\. The Fine-Tuning Frontier: Customizing Open Models**

The user's query specifically asks if open models can be fine-tuned on vulnerability-labeled code. The answer is a definitive **yes**, and this pathway represents the most cost-effective and scalable method for specialized vulnerability detection.

### **4.1 The Data Challenge: Curating a Rust Vulnerability Corpus**

General-purpose LLMs (like the base Llama 3\) are trained on massive scrapes of the internet, which include both secure and insecure code. To create a security specialist, one must fine-tune the model on a curated dataset where vulnerabilities are explicitly labeled and corrected.5

#### **4.1.1 The RustSec Advisory Database**

The **RustSec Advisory Database** serves as the primary source of truth for known Rust vulnerabilities. It uses a structured TOML format for advisories.

* **Extraction Strategy**: Engineering a dataset from RustSec involves parsing the RUSTSEC-YYYY-NNNN.md files to extract the description (the vulnerability) and the patched versions. By correlating these advisory dates with the crate versions published on crates.io, researchers can extract the "vulnerable" function code and the "patched" function code from the source history.  
* **Dataset Structure**: This process creates a parallel corpus: (Vulnerable Code) \-\> (Fixed Code). This data must be converted into a JSONL format suitable for fine-tuning, such as:  
  JSON  
  {"prompt": "Analyze this Rust code for vulnerabilities:\\n...", "completion": "Vulnerability found:"}

  This real-world data is invaluable because it represents actual mistakes made by Rust developers, rather than synthetic examples.7

#### **4.1.2 Synthetic Data Generation: HALURust**

Given the relative scarcity of real-world vulnerabilities compared to the volume of safe code, **Synthetic Data Generation** becomes a crucial multiplier. A groundbreaking approach known as **HALURust** leverages a large, capable model (like GPT-4) to "hallucinate" or inject vulnerabilities into safe Rust code.

* **Process**: A safe function is presented to GPT-4 with the instruction: *"Rewrite this code to introduce a subtle Use-After-Free vulnerability."* The original safe code and the newly generated vulnerable code form a training pair.  
* **Impact**: Models fine-tuned on this "hallucinated" synthetic data have demonstrated a **10-20% improvement** in detecting real-world vulnerabilities compared to models trained solely on scarce real-world data. This technique effectively teaches the model the "shape" of vulnerabilities.7

### **4.2 Fine-Tuning Llama 3: Technical Implementation**

**Llama 3 (8B or 70B)** is the ideal candidate for this fine-tuning task due to its state-of-the-art architecture and open weights.

* **QLoRA (Quantized Low-Rank Adaptation)**: Fine-tuning a 70B parameter model typically requires massive GPU resources. However, QLoRA enables this on consumer-grade hardware or modest cloud instances by freezing the main model weights and training only a small, quantized adapter layer. This drastically reduces the VRAM requirements.  
* **Hyperparameter Tuning**: Research suggests that for code tasks, a low learning rate (e.g., 2e-5) and a focus on "Instruction Tuning" (using the Instruct version of Llama 3 as a base) yield the best results. The training objective should be maximizing the probability of the correct vulnerability classification and fix generation.30  
* **Evaluation**: The fine-tuned model must be evaluated on held-out test sets like **SecureBench** or **CyberSecEval** to ensure it hasn't overfitted to the training data. The primary metric for success should be the **F1-Score**, which balances precision (avoiding false positives) and recall (finding actual bugs).12

## ---

**5\. Economic and Operational Analysis: LLM vs. Traditional SAST**

The decision to adopt LLM analysis involves a distinct cost-benefit calculation compared to traditional tools. While SAST tools often have high fixed costs (licensing), LLMs introduce variable costs (tokens) and different operational dynamics.

### **5.1 Cost Structure Analysis**

#### **5.1.1 Traditional SAST (e.g., Snyk, SonarQube)**

* **Pricing Model**: SAST tools typically charge per developer (seat) or per line of code. Enterprise licenses can range from **$30,000 to $100,000+ per year** for large organizations.  
* **Compute Cost**: The marginal cost of running a scan is negligible. Scans run locally or in CI/CD pipelines in seconds or minutes.  
* **Hidden Cost**: The primary cost driver for SAST is **False Positive Triage**. If a tool flags 500 potential issues and 450 of them are false positives, the engineering time required to review and dismiss these alerts represents a massive operational expense.1

#### **5.1.2 LLM Analysis (API & Self-Hosted)**

* **Pricing Model**: LLMs typically charge per token (consumption-based).  
  * **GPT-4o**: Costs approximately **$5.00 per 1 million input tokens**. Analyzing a moderately sized project (10,000 LOC, approx. 100k-150k tokens) might cost \~$0.75 per scan. For frequent CI scans, this cost can accumulate rapidly.  
  * **Llama 3 70B (Groq/API)**: Costs significantly less, around **$0.60 per 1 million tokens**. The same scan would cost \~$0.09.  
  * **Self-Hosted Llama 3**: Involves fixed infrastructure costs (e.g., renting an AWS ml.p4d.24xlarge instance). While this eliminates per-token costs, the upfront rental (thousands of dollars per month) means break-even is only achieved at very high volumes (\>50M tokens/month).33  
* **Latency**: LLMs are computationally intensive and slow. A full analysis of a codebase might take minutes compared to SAST's seconds. This impacts the developer feedback loop, often requiring asynchronous reporting rather than blocking PRs.

### **5.2 ROI and the Value of Precision**

The Return on Investment (ROI) for LLMs is driven by **Precision**.

* Research suggests that LLMs can reduce false positive rates by **40% to 75%** when used to verify SAST results.4  
* **ROI Calculation**: Consider an engineer costing $100/hr who spends 10 hours a week triaging SAST false positives ($1,000/week). If utilizing GPT-4 API credits costing $50/week can automate this filtering and reduce the triage time to 1 hour, the net saving is $850/week per engineer. This efficiency gain often outweighs the raw compute cost of the LLM.

### **5.3 Recommendation: The Hybrid Pipeline**

The optimal strategy is not to replace SAST with LLMs, but to augment it in a hybrid pipeline.

1. **Stage 1 (Fast/Cheap)**: Run cargo audit (RustSec) and clippy (Linter). These tools catch low-hanging fruit and dependency vulnerabilities at zero marginal cost.  
2. **Stage 2 (Heuristic Filtering)**: Run a traditional SAST tool (or the existing heuristic fallback in src/ai/mod.rs) to flag potential hotspots across the codebase.  
3. **Stage 3 (Deep Verification)**: Send *only the flagged snippets* (plus surrounding context) to the LLM (Llama 3 70B or Claude 3.5) for verification.  
4. **Stage 4 (Compiler Loop)**: If the LLM confirms a vulnerability and proposes a fix, it runs a cargo check on the fix to ensure validity before presenting it to the user.

This approach minimizes token usage (Cost) while maximizing the semantic analysis capabilities (Benefit).35

## ---

**6\. Implementation Strategy for src/ai/mod.rs**

Based on the research findings, the following implementation roadmap is proposed for the user's src/ai/mod.rs module.

### **Phase 1: Prompt Hardening (Immediate)**

* **Action**: Update the SECURITY\_SYSTEM\_PROMPT to use the "Role \+ CoT \+ Protocol" structure defined in Section 3.4.  
* **Detail**: Integrate specific Few-Shot examples of Rust vulnerabilities (CWE-416, CWE-362). Implement a retry mechanism that feeds JSON parsing errors back to the LLM for self-correction.

### **Phase 2: Open Source Integration (Mid-Term)**

* **Action**: Integrate a **Llama 3 70B** model via a provider like Groq (for speed) or a local vLLM instance (for privacy).  
* **Detail**: Replace expensive GPT-4 calls with Llama 3 70B for the bulk of the analysis. This cost reduction allows for more verbose Chain-of-Thought prompting without budget constraints.17

### **Phase 3: The "Agentic" Loop (Advanced)**

* **Action**: Modify src/ai/mod.rs to support **Tool Use**.  
* **Detail**: Allow the LLM to invoke std::process::Command::new("cargo").arg("check"). Pass the stderr output from Cargo back into the LLM's context message history. This transforms the module from a passive "Code Reader" to an active "Code Tester," fundamentally solving the issue of hallucinated Rust errors.25

### **Phase 4: Fine-Tuning (Long-Term)**

* **Action**: Develop a fine-tuning pipeline using rustsec/advisory-db and synthetic data.  
* **Detail**: Fine-tune a Llama 3 8B or 70B model using QLoRA to create a specialized "Rust Security Expert." This model can be deployed locally, offering high-fidelity analysis with zero data leakage risks.30

## ---

**7\. Conclusion**

The analysis confirms that Large Language Models are not merely a novelty for security analysis but a potent operational asset that, when properly engineered, complements and enhances traditional SAST methodologies. For the specific context of a Rust-based AI module, **Claude 3.7 Sonnet** currently offers the highest reasoning capability for complex exploits, while **Llama 3 70B** provides the most economically viable path for high-volume scanning through fine-tuning.

The immediate imperative for the user is to transition from simple prompting to a **Chain-of-Thought architecture** that integrates **compiler feedback loops**. By forcing the LLM to validate its "thoughts" against the hard constraints of the Rust compiler, the system can achieve a level of precision that neither pure SAST nor pure LLMs can reach in isolation. This "Hybrid AI-Compiler" approach represents the state-of-the-art in automated vulnerability detection.

#### **Works cited**

1. Best SAST Tools of 2025 \- StackHawk, accessed January 6, 2026, [https://www.stackhawk.com/blog/best-sast-tools-comparison/](https://www.stackhawk.com/blog/best-sast-tools-comparison/)  
2. Top 10 SAST Tools in 2025: How They Integrate and Fit Into Engineering Workflows, accessed January 6, 2026, [https://www.ox.security/blog/static-application-security-sast-tools/](https://www.ox.security/blog/static-application-security-sast-tools/)  
3. Beyond Spicy Autocorrect: Are LLMs Enough for SAST? \- GuidePoint Security, accessed January 6, 2026, [https://www.guidepointsecurity.com/blog/beyond-spicy-autocorrect-llms-for-sast/](https://www.guidepointsecurity.com/blog/beyond-spicy-autocorrect-llms-for-sast/)  
4. Comparison of Static Application Security Testing Tools and Large Language Models for Repo-level Vulnerability Detection \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2407.16235v1](https://arxiv.org/html/2407.16235v1)  
5. LLMs in SAST: Best Practices for Developing Next-Generation Vulnerability Detection, accessed January 6, 2026, [https://medium.com/@cdxlabs.abhiram/llms-in-sast-best-practices-for-developing-next-generation-vulnerability-detection-306b93abc5bb](https://medium.com/@cdxlabs.abhiram/llms-in-sast-best-practices-for-developing-next-generation-vulnerability-detection-306b93abc5bb)  
6. deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2506.15648v1](https://arxiv.org/html/2506.15648v1)  
7. Machine Learning-Based Vulnerability Detection in Rust Code Using LLVM IR and Transformer Model \- MDPI, accessed January 6, 2026, [https://www.mdpi.com/2504-4990/7/3/79](https://www.mdpi.com/2504-4990/7/3/79)  
8. LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights, accessed January 6, 2026, [https://arxiv.org/html/2502.07049v2](https://arxiv.org/html/2502.07049v2)  
9. Harnessing Large Language Models for Software Vulnerability Detection: A Comprehensive Benchmarking Study \- IEEE Xplore, accessed January 6, 2026, [https://ieeexplore.ieee.org/iel8/6287639/10820123/10879492.pdf](https://ieeexplore.ieee.org/iel8/6287639/10820123/10879492.pdf)  
10. SEC-bench: Automated Benchmarking of LLM Agents on ... \- arXiv, accessed January 6, 2026, [https://arxiv.org/pdf/2506.11791](https://arxiv.org/pdf/2506.11791)  
11. Purple Llama CyberSecEval: A Secure Coding Benchmark for Language Models, accessed January 6, 2026, [https://www.semanticscholar.org/paper/Purple-Llama-CyberSecEval%3A-A-Secure-Coding-for-Bhatt-Chennabasappa/ea07b2e1a910defdcac01a47a152b6ad87e02ed4](https://www.semanticscholar.org/paper/Purple-Llama-CyberSecEval%3A-A-Secure-Coding-for-Bhatt-Chennabasappa/ea07b2e1a910defdcac01a47a152b6ad87e02ed4)  
12. CYBERSECEVAL 2 : Benchmark to quantify LLM security risks and capabilities \- Medium, accessed January 6, 2026, [https://medium.com/@techsachin/cyberseceval-2-benchmark-to-quantify-llm-security-risks-and-capabilities-29a128351e1f](https://medium.com/@techsachin/cyberseceval-2-benchmark-to-quantify-llm-security-risks-and-capabilities-29a128351e1f)  
13. The LLM Landscape: A Look at GPT-4, Gemini, Claude 3, and Meta Llama 3, accessed January 6, 2026, [https://complereinfosystem.com/the-llm-landscape-gpt-4-gemini-claude-3-meta-llama-3](https://complereinfosystem.com/the-llm-landscape-gpt-4-gemini-claude-3-meta-llama-3)  
14. Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2311.16169v2](https://arxiv.org/html/2311.16169v2)  
15. Llama 405B Benchmarks (Comparison to GPT-4o and Claude 3.5 Sonnet) \- Reddit, accessed January 6, 2026, [https://www.reddit.com/r/singularity/comments/1eaa8ht/llama\_405b\_benchmarks\_comparison\_to\_gpt4o\_and/](https://www.reddit.com/r/singularity/comments/1eaa8ht/llama_405b_benchmarks_comparison_to_gpt4o_and/)  
16. Llama 3 70B wins against GPT-4 Turbo in test code generation eval (and other \+130 LLMs), accessed January 6, 2026, [https://www.reddit.com/r/LocalLLaMA/comments/1cihrdt/llama\_3\_70b\_wins\_against\_gpt4\_turbo\_in\_test\_code/](https://www.reddit.com/r/LocalLLaMA/comments/1cihrdt/llama_3_70b_wins_against_gpt4_turbo_in_test_code/)  
17. Llama 3.3 just dropped — is it better than GPT-4 or Claude-Sonnet-3.5? \- Helicone, accessed January 6, 2026, [https://www.helicone.ai/blog/meta-llama-3-3-70-b-instruct](https://www.helicone.ai/blog/meta-llama-3-3-70-b-instruct)  
18. Llama 3 70B vs GPT-4: Comparison Analysis \- Vellum AI, accessed January 6, 2026, [https://www.vellum.ai/blog/llama-3-70b-vs-gpt-4-comparison-analysis](https://www.vellum.ai/blog/llama-3-70b-vs-gpt-4-comparison-analysis)  
19. Prompt Engineering for AI Guide | Google Cloud, accessed January 6, 2026, [https://cloud.google.com/discover/what-is-prompt-engineering](https://cloud.google.com/discover/what-is-prompt-engineering)  
20. Prompt Engineering Techniques | IBM, accessed January 6, 2026, [https://www.ibm.com/think/topics/prompt-engineering-techniques](https://www.ibm.com/think/topics/prompt-engineering-techniques)  
21. Few-Shot Prompting \- Prompt Engineering Guide, accessed January 6, 2026, [https://www.promptingguide.ai/techniques/fewshot](https://www.promptingguide.ai/techniques/fewshot)  
22. AI-Powered Memory Safety with the Pointer Ownership Model, accessed January 6, 2026, [https://www.sei.cmu.edu/blog/ai-powered-memory-safety-with-the-pointer-ownership-model/](https://www.sei.cmu.edu/blog/ai-powered-memory-safety-with-the-pointer-ownership-model/)  
23. Accepted to IEEE Symposium on Security and Privacy 2026 deepSURF: Detecting Memory Safety Vulnerabilities in Rust Through Fuzzing LLM-Augmented Harnesses \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2506.15648v2](https://arxiv.org/html/2506.15648v2)  
24. Agent READMEs: An Empirical Study of Context Files for Agentic Coding \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2511.12884v1](https://arxiv.org/html/2511.12884v1)  
25. The Engineer's Guide to the Cargo MCP Server \- Skywork.ai, accessed January 6, 2026, [https://skywork.ai/skypage/en/engineers-guide-cargo-mcp-server/1980815203829092352](https://skywork.ai/skypage/en/engineers-guide-cargo-mcp-server/1980815203829092352)  
26. AMP Code System Prompt 2025-10-25.md \- GitHub Gist, accessed January 6, 2026, [https://gist.github.com/gregce/9ae20efc085d45b36f1ce7a6a2b48845](https://gist.github.com/gregce/9ae20efc085d45b36f1ce7a6a2b48845)  
27. Affected Functions: A Key to Understanding Open-Source Vulnerabilities \- CramHacks, accessed January 6, 2026, [https://www.cramhacks.com/p/public-affected-functions](https://www.cramhacks.com/p/public-affected-functions)  
28. rustsec/advisory-db: Security advisory database for Rust crates published through crates.io \- GitHub, accessed January 6, 2026, [https://github.com/rustsec/advisory-db](https://github.com/rustsec/advisory-db)  
29. HALURust: Exploiting Hallucinations of Large Language Models to Detect Vulnerabilities in Rust \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2503.10793v1](https://arxiv.org/html/2503.10793v1)  
30. Fine-Tuning LLaMA 3: A Practical Guide | by Hey Amit | Medium, accessed January 6, 2026, [https://medium.com/@heyamit10/fine-tuning-llama-3-a-practical-guide-0989df65dbfc](https://medium.com/@heyamit10/fine-tuning-llama-3-a-practical-guide-0989df65dbfc)  
31. Evaluating LLaMA 3.2 for Software Vulnerability Detection \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2503.07770v1](https://arxiv.org/html/2503.07770v1)  
32. Veracode pricing 2025: Is it worth it? \- Beagle Security, accessed January 6, 2026, [https://beaglesecurity.com/blog/article/veracode-pricing.html](https://beaglesecurity.com/blog/article/veracode-pricing.html)  
33. A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2509.18101v3](https://arxiv.org/html/2509.18101v3)  
34. Llama 3 \- A cost analysis \- Isaac's Tech Blog, accessed January 6, 2026, [https://isaacstechblog.com/blog/llama3-cost-analysis/](https://isaacstechblog.com/blog/llama3-cost-analysis/)  
35. Large Language Models Versus Static Code Analysis Tools: A Systematic Benchmark for Vulnerability Detection \- IEEE Xplore, accessed January 6, 2026, [https://ieeexplore.ieee.org/iel8/6287639/10820123/11261665.pdf](https://ieeexplore.ieee.org/iel8/6287639/10820123/11261665.pdf)  
36. Enhancing Code Static Analysis with LLMs: An Experiment with SpotBugs and Gemini \- DiVA portal, accessed January 6, 2026, [http://www.diva-portal.org/smash/get/diva2:1995525/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:1995525/FULLTEXT01.pdf)  
37. Investigating Large Language Models for Code Vulnerability Detection: An Experimental Study \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2412.18260v2](https://arxiv.org/html/2412.18260v2)