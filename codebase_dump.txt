################################################################################
# SEC_AUDITOR CODEBASE DUMP
# Generated: 2026-02-01 02:58:05 UTC
# Project: sec_auditor - High-Performance Rust Security Analysis Engine
# Language: Rust
# Total files: 32
################################################################################


================================================================================
FILE: src\main.rs
================================================================================
//! sec_auditor - GitHub Security Analysis CLI
//!
//! A high-performance security analysis tool for GitHub repositories.

use clap::{Parser, Subcommand, ValueEnum};
use sec_auditor::{
    config::{Config, OutputFormat},
    reporter::create_reporter,
    Scanner, Severity,
};
use std::path::PathBuf;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;
use tracing::{error, info, warn, Level};
use tracing_subscriber::{fmt, prelude::*, EnvFilter};

/// Parse a severity string into a Severity enum.
fn parse_severity(s: &str) -> Severity {
    match s.to_lowercase().as_str() {
        "critical" => Severity::Critical,
        "high" => Severity::High,
        "medium" => Severity::Medium,
        "low" => Severity::Low,
        "none" => Severity::None,
        _ => Severity::Low, // Default to low
    }
}

/// High-performance security analysis for GitHub repositories
#[derive(Parser)]
#[command(name = "sec_auditor")]
#[command(author, version, about, long_about = None)]
struct Cli {
    /// Verbosity level (-v, -vv, -vvv)
    #[arg(short, long, action = clap::ArgAction::Count)]
    verbose: u8,

    /// Suppress all output except errors
    #[arg(short, long)]
    quiet: bool,

    /// Output format
    #[arg(short = 'f', long, default_value = "text")]
    format: OutputFormatArg,

    /// Output file (stdout if not specified)
    #[arg(short, long)]
    output: Option<PathBuf>,

    /// GitHub token (or set GITHUB_TOKEN env var)
    #[arg(long)]
    token: Option<String>,

    /// Configuration file
    #[arg(short, long)]
    config: Option<PathBuf>,

    #[command(subcommand)]
    command: Commands,
}

#[derive(ValueEnum, Clone, Copy)]
enum OutputFormatArg {
    Text,
    Json,
    Sarif,
}

impl From<OutputFormatArg> for OutputFormat {
    fn from(arg: OutputFormatArg) -> Self {
        match arg {
            OutputFormatArg::Text => OutputFormat::Text,
            OutputFormatArg::Json => OutputFormat::Json,
            OutputFormatArg::Sarif => OutputFormat::Sarif,
        }
    }
}

#[derive(Subcommand)]
enum Commands {
    /// Scan a repository or local path
    Scan {
        /// Repository (owner/repo), URL, or local path
        target: String,

        /// Enable SAST analysis
        #[arg(long, default_value = "true")]
        sast: bool,

        /// Enable SCA (dependency) analysis
        #[arg(long, default_value = "true")]
        sca: bool,

        /// Enable secret detection
        #[arg(long, default_value = "true")]
        secrets: bool,

        /// Enable AI-driven analysis
        #[arg(long)]
        ai: bool,

        /// Enable provenance verification
        #[arg(long)]
        provenance: bool,

        /// Languages to analyze (comma-separated)
        #[arg(long, default_value = "rust,python,javascript,go")]
        languages: String,

        /// Maximum file size to analyze (bytes)
        #[arg(long, default_value = "1048576")]
        max_file_size: usize,

        /// Minimum severity to report
        #[arg(long, default_value = "low")]
        min_severity: String,
    },

    /// Scan all repositories in an organization
    Org {
        /// Organization name
        name: String,

        /// Maximum repositories to scan
        #[arg(long, default_value = "100")]
        max_repos: usize,
    },

    /// Scan all repositories for a user
    User {
        /// Username
        name: String,

        /// Maximum repositories to scan
        #[arg(long, default_value = "100")]
        max_repos: usize,
    },

    /// Search and scan repositories
    Search {
        /// Search query (GitHub search syntax)
        query: String,

        /// Maximum repositories to scan
        #[arg(long, default_value = "10")]
        max_repos: usize,
    },

    /// Verify supply chain provenance
    Verify {
        /// Path to Cargo.lock or package-lock.json
        path: PathBuf,
    },

    /// Check rate limit status
    RateLimit,
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let cli = Cli::parse();

    // Set up logging
    let log_level = match cli.verbose {
        0 if cli.quiet => Level::ERROR,
        0 => Level::WARN,
        1 => Level::INFO,
        2 => Level::DEBUG,
        _ => Level::TRACE,
    };

    tracing_subscriber::registry()
        .with(fmt::layer().with_target(false))
        .with(
            EnvFilter::try_from_default_env()
                .unwrap_or_else(|_| EnvFilter::new(log_level.to_string())),
        )
        .init();

    // Set up graceful shutdown handling
    let shutdown_flag = Arc::new(AtomicBool::new(false));
    let shutdown_flag_clone = shutdown_flag.clone();

    // Spawn a task to handle shutdown signals
    tokio::spawn(async move {
        if let Err(e) = tokio::signal::ctrl_c().await {
            error!("Failed to listen for shutdown signal: {}", e);
            return;
        }

        warn!("Received interrupt signal, initiating graceful shutdown...");
        shutdown_flag_clone.store(true, Ordering::SeqCst);

        // If we get a second signal, force exit
        if let Ok(()) = tokio::signal::ctrl_c().await {
            error!("Received second interrupt, forcing shutdown");
            std::process::exit(130); // Standard exit code for SIGINT
        }
    });

    // Load configuration
    let mut config = if let Some(ref config_path) = cli.config {
        Config::from_file(config_path)?
    } else {
        Config::default()
    };

    // Override with CLI options
    if let Some(ref token) = cli.token {
        config.github.token = Some(token.clone());
    }
    config.output.format = cli.format.into();
    config.output.output_path = cli.output.clone();

    // Create scanner
    let scanner = Scanner::new(config.clone())?;

    // Check for early shutdown
    if shutdown_flag.load(Ordering::SeqCst) {
        warn!("Shutdown requested before scan started");
        return Ok(());
    }

    // Execute command
    let result = match cli.command {
        Commands::Scan {
            target,
            sast,
            sca,
            secrets,
            ai,
            provenance,
            languages,
            max_file_size,
            min_severity,
        } => {
            // Update config based on flags
            let mut scan_config = config.clone();
            scan_config.analysis.enable_sast = sast;
            scan_config.analysis.enable_sca = sca;
            scan_config.analysis.enable_secrets = secrets;
            scan_config.analysis.enable_ai = ai;
            scan_config.analysis.enable_provenance = provenance;
            scan_config.analysis.languages = languages
                .split(',')
                .map(|s| s.trim().to_lowercase())
                .collect();
            scan_config.analysis.max_file_size = max_file_size;
            let min_sev = parse_severity(&min_severity);
            scan_config.analysis.min_severity = min_sev;

            let scanner = Scanner::new(scan_config)?;
            let mut result = scanner.scan_repository(&target).await?;

            // Filter findings by minimum severity
            result.findings.retain(|f| f.severity >= min_sev);
            result
        }

        Commands::Org { name, max_repos } => {
            let mut org_config = config.clone();
            org_config.github.max_repos = max_repos;
            let scanner = Scanner::new(org_config)?;
            scanner.scan_repository(&format!("org:{}", name)).await?
        }

        Commands::User { name, max_repos } => {
            let mut user_config = config.clone();
            user_config.github.max_repos = max_repos;
            let scanner = Scanner::new(user_config)?;
            scanner.scan_repository(&format!("user:{}", name)).await?
        }

        Commands::Search { query, max_repos } => {
            let mut search_config = config.clone();
            search_config.github.max_repos = max_repos;
            let scanner = Scanner::new(search_config)?;
            scanner.scan_repository(&query).await?
        }

        Commands::Verify { path } => {
            let findings = scanner.verify_provenance(&path).await?;
            let mut result = sec_auditor::ScanResult::new(path.display().to_string());
            for finding in findings {
                result.add_finding(finding);
            }
            result
        }

        Commands::RateLimit => {
            if config.github.token.is_none() {
                error!("GitHub token required for rate limit check");
                std::process::exit(1);
            }

            let github = sec_auditor::crawler::GitHubClient::new(config.github)?;
            let status = github.check_rate_limit().await?;

            println!("GitHub API Rate Limit Status:");
            println!("  Limit: {}", status.limit);
            println!("  Remaining: {}", status.remaining);
            println!("  Reset in: {}s", status.seconds_until_reset());

            if status.is_limited() {
                println!("\nWarning: You are currently rate limited!");
            }

            return Ok(());
        }
    };

    // Generate and output report
    let reporter = create_reporter(config.output.format);
    let report = reporter.generate(&result);

    if let Some(ref output_path) = config.output.output_path {
        std::fs::write(output_path, &report)?;
        info!("Report written to: {}", output_path.display());
    } else {
        println!("{}", report);
    }

    // Exit with non-zero code if critical/high findings
    let critical = result
        .findings
        .iter()
        .filter(|f| matches!(f.severity, sec_auditor::Severity::Critical | sec_auditor::Severity::High))
        .count();

    if critical > 0 {
        std::process::exit(1);
    }

    Ok(())
}

================================================================================
END: src\main.rs
================================================================================

================================================================================
FILE: src\lib.rs
================================================================================
#![recursion_limit = "1024"]
//! sec_auditor - High-Performance Rust Security Analysis Engine
//!
//! A comprehensive security analysis application for GitHub repositories,
//! combining SAST, SCA, and AI-driven vulnerability detection.
//!
//! # Features
//!
//! - **SAST (Static Application Security Testing)**: Tree-sitter based code analysis
//! - **SCA (Software Composition Analysis)**: OSV database integration for dependency vulnerabilities
//! - **Secret Detection**: High-entropy and pattern-based secret detection
//! - **Provenance Verification**: SLSA/Sigstore supply chain verification
//! - **AI Analysis**: Optional LLM-powered vulnerability detection
//! - **SARIF Output**: Industry-standard reporting format
//!
//! # Architecture
//!
//! The application uses a hybrid concurrency model:
//! - **Tokio** for async I/O operations (GitHub API, network requests)
//! - **Rayon** for parallel CPU-bound analysis (parsing, pattern matching)
//!
//! # Example Usage
//!
//! ```no_run
//! use sec_auditor::{Config, Scanner};
//!
//! #[tokio::main]
//! async fn main() -> Result<(), Box<dyn std::error::Error>> {
//!     let config = Config::default();
//!     let scanner = Scanner::new(config)?;
//!
//!     let result = scanner.scan_repository("owner/repo").await?;
//!     println!("Found {} findings", result.findings.len());
//!
//!     Ok(())
//! }
//! ```

pub mod ai;
pub mod analyzer;
pub mod concurrency;
pub mod config;
pub mod crawler;
pub mod crosslang;
pub mod error;
pub mod models;
pub mod privacy;
pub mod provenance;
pub mod reporter;

// Re-export commonly used types
pub use config::{Config, OutputFormat};
pub use error::{AuditorError, Result};
pub use models::{Finding, Repository, ScanResult, Severity, Vulnerability};

use analyzer::{SastEngine, ScaEngine, SecretDetector};
use crawler::{GitHubClient, GitOperations, RepoTraverser};
use models::ScanTarget;
use provenance::SlsaVerifier;
use std::path::Path;
use std::sync::Arc;
use std::time::Instant;
use tracing::{debug, error, info, warn};

/// Main scanner orchestrating all analysis components.
pub struct Scanner {
    /// Configuration
    config: Config,

    /// GitHub client
    github: Option<GitHubClient>,

    /// Git operations handler
    git: GitOperations,

    /// SAST engine
    sast: SastEngine,

    /// SCA engine
    sca: ScaEngine,

    /// Secret detector
    secrets: SecretDetector,

    /// SLSA verifier
    slsa: SlsaVerifier,

    /// AI analyzer
    ai: ai::AiAnalyzer,
}

impl Scanner {
    /// Create a new scanner with the given configuration.
    pub fn new(config: Config) -> Result<Self> {
        // Initialize GitHub client if token is available
        let github = if config.github.token.is_some() {
            Some(GitHubClient::new(config.github.clone())?)
        } else {
            warn!("No GitHub token provided. Some features may be limited.");
            None
        };

        // Initialize Git operations
        let temp_dir = config
            .analysis
            .temp_dir
            .clone()
            .unwrap_or_else(|| std::env::temp_dir().join("sec_auditor"));
        let git = GitOperations::new(temp_dir);

        // Initialize SAST engine
        let sast = SastEngine::new(config.analysis.clone())?;

        // Initialize other components
        let sca = ScaEngine::new();
        let secrets = SecretDetector::new(config.analysis.entropy_threshold);
        let slsa = SlsaVerifier::new();
        let ai = ai::AiAnalyzer::new();

        Ok(Self {
            config,
            github,
            git,
            sast,
            sca,
            secrets,
            slsa,
            ai,
        })
    }

    /// Scan a repository by owner/name.
    pub async fn scan_repository(&self, repo_spec: &str) -> Result<ScanResult> {
        let target = ScanTarget::parse(repo_spec);

        match target {
            ScanTarget::Repository(repo) => self.scan_github_repo(repo).await,
            ScanTarget::LocalPath(path) => self.scan_local_path(&path).await,
            ScanTarget::Organization(org) => self.scan_organization(&org).await,
            ScanTarget::User(user) => self.scan_user(&user).await,
            ScanTarget::Search(query) => self.scan_search(&query).await,
        }
    }

    /// Scan a GitHub repository.
    async fn scan_github_repo(&self, mut repo: Repository) -> Result<ScanResult> {
        let start_time = Instant::now();
        info!("Starting scan of repository: {}", repo.full_name);

        let mut result = ScanResult::new(&repo.full_name);

        // Get repository metadata if we have a GitHub client
        if let Some(ref github) = self.github {
            match github.get_repository(&repo.owner, &repo.name).await {
                Ok(metadata) => {
                    repo = metadata;
                    result.commit_sha = repo.commit_sha.clone();
                }
                Err(e) => {
                    warn!("Could not fetch repository metadata: {}", e);
                }
            }
        }

        // Clone the repository
        let local_path = self.git.clone_repository(&mut repo)?;

        // Perform the scan
        let scan_result = self.scan_local_path(&local_path).await?;

        // Merge results
        result.findings = scan_result.findings;
        result.stats = scan_result.stats;
        result.completed_at = chrono::Utc::now();
        result.stats.duration_ms = start_time.elapsed().as_millis() as u64;

        // Clean up if configured
        if self.config.analysis.temp_dir.is_none() {
            // Only clean up if using system temp dir
            if let Err(e) = self.git.cleanup(&repo) {
                warn!("Failed to clean up cloned repository: {}", e);
            }
        }

        Ok(result)
    }

    /// Scan a local directory.
    pub async fn scan_local_path(&self, path: &Path) -> Result<ScanResult> {
        let start_time = Instant::now();
        info!("Starting scan of local path: {}", path.display());

        let mut result = ScanResult::new(path.display().to_string());

        // Get commit SHA if it's a git repository
        if let Ok(sha) = GitOperations::get_head_sha(path) {
            result.commit_sha = Some(sha);
        }

        // Get source files
        let traverser = RepoTraverser::new(path);
        let mut files = traverser.get_source_files(path)?;

        result.stats.files_scanned = files.len();
        result.stats.lines_analyzed = files.iter().map(|f| f.size as usize / 40).sum(); // Rough estimate

        // Run SAST analysis
        if self.config.analysis.enable_sast {
            info!("Running SAST analysis on {} files", files.len());
            match self.sast.analyze_files(&mut files) {
                Ok(findings) => {
                    result.stats.sast_findings = findings.len();
                    for finding in findings {
                        result.add_finding(finding);
                    }
                }
                Err(e) => {
                    error!("SAST analysis failed: {}", e);
                }
            }
        }

        // Run secret detection
        if self.config.analysis.enable_secrets {
            info!("Running secret detection");
            for file in &mut files {
                let file_path = file.path.clone();
                let language = file.language;

                if let Ok(content) = file.load_content() {
                    let content = content.to_string();
                    let secret_findings = self.secrets.detect(&content, &file_path, language);
                    for finding in secret_findings {
                        result.add_finding(finding);
                    }
                }
            }
        }

        // Run SCA analysis
        if self.config.analysis.enable_sca {
            info!("Running SCA analysis");
            match self.sca.analyze_repository(path).await {
                Ok(findings) => {
                    result.stats.sca_findings = findings.len();
                    for finding in findings {
                        result.add_finding(finding);
                    }
                }
                Err(e) => {
                    warn!("SCA analysis failed: {}", e);
                }
            }
        }

        // Run AI analysis if enabled
        if self.config.analysis.enable_ai && self.ai.is_available() {
            info!("Running AI-driven analysis");
            for file in &mut files {
                let file_path = file.path.clone();
                let language = file.language;

                if let Ok(content) = file.load_content() {
                    let content = content.to_string();
                    let context = ai::AnalysisContext {
                        file_path: file_path.clone(),
                        ..Default::default()
                    };

                    match self.ai.analyze_snippet(&content, language, &context).await {
                        Ok(findings) => {
                            for finding in findings {
                                result.add_finding(finding);
                            }
                        }
                        Err(e) => {
                            debug!("AI analysis failed for {}: {}", file_path.display(), e);
                        }
                    }
                }
            }
        }

        // Run provenance verification if enabled
        if self.config.analysis.enable_provenance {
            info!("Running provenance verification");
            match self.verify_provenance(path).await {
                Ok(findings) => {
                    for finding in findings {
                        result.add_finding(finding);
                    }
                }
                Err(e) => {
                    warn!("Provenance verification failed: {}", e);
                }
            }
        }

        result.completed_at = chrono::Utc::now();
        result.stats.duration_ms = start_time.elapsed().as_millis() as u64;

        info!(
            "Scan complete. Found {} findings in {}ms",
            result.findings.len(),
            result.stats.duration_ms
        );

        Ok(result)
    }

    /// Scan all repositories in an organization.
    async fn scan_organization(&self, org: &str) -> Result<ScanResult> {
        let github = self
            .github
            .as_ref()
            .ok_or_else(|| AuditorError::AuthRequired)?;

        let repos = github
            .list_org_repos(org, self.config.github.max_repos)
            .await?;

        self.scan_multiple_repos(repos).await
    }

    /// Scan all repositories for a user.
    async fn scan_user(&self, user: &str) -> Result<ScanResult> {
        let github = self
            .github
            .as_ref()
            .ok_or_else(|| AuditorError::AuthRequired)?;

        let repos = github
            .list_user_repos(user, self.config.github.max_repos)
            .await?;

        self.scan_multiple_repos(repos).await
    }

    /// Scan repositories matching a search query.
    async fn scan_search(&self, query: &str) -> Result<ScanResult> {
        let github = self
            .github
            .as_ref()
            .ok_or_else(|| AuditorError::AuthRequired)?;

        let repos = github
            .search_repos(query, self.config.github.max_repos)
            .await?;

        self.scan_multiple_repos(repos).await
    }

    /// Scan multiple repositories and aggregate results.
    async fn scan_multiple_repos(&self, repos: Vec<Repository>) -> Result<ScanResult> {
        let start_time = Instant::now();
        let mut combined_result = ScanResult::new(format!("{} repositories", repos.len()));

        for repo in repos {
            if repo.archived {
                debug!("Skipping archived repository: {}", repo.full_name);
                continue;
            }

            match self.scan_github_repo(repo.clone()).await {
                Ok(result) => {
                    combined_result.stats.files_scanned += result.stats.files_scanned;
                    combined_result.stats.lines_analyzed += result.stats.lines_analyzed;
                    combined_result.stats.sast_findings += result.stats.sast_findings;
                    combined_result.stats.sca_findings += result.stats.sca_findings;
                    combined_result.stats.secrets_found += result.stats.secrets_found;

                    for mut finding in result.findings {
                        // Prefix finding location with repo name
                        finding.location.file = std::path::PathBuf::from(&repo.full_name)
                            .join(&finding.location.file);
                        combined_result.add_finding(finding);
                    }
                }
                Err(e) => {
                    warn!("Failed to scan {}: {}", repo.full_name, e);
                }
            }

            // Rate limiting
            if let Some(ref github) = self.github {
                github.rate_limit_delay().await;
            }
        }

        combined_result.completed_at = chrono::Utc::now();
        combined_result.stats.duration_ms = start_time.elapsed().as_millis() as u64;

        Ok(combined_result)
    }

    /// Verify provenance for dependencies.
    pub async fn verify_provenance(&self, path: &Path) -> Result<Vec<Finding>> {
        let mut findings = Vec::new();

        let cargo_lock = path.join("Cargo.lock");
        if cargo_lock.exists() {
            let lockfile = cargo_lock::Lockfile::load(&cargo_lock)?;

            for package in &lockfile.packages {
                let provenance = self
                    .slsa
                    .verify_crate(package.name.as_str(), &package.version.to_string())
                    .await?;

                findings.extend(
                    self.slsa
                        .generate_findings(package.name.as_str(), &provenance),
                );
            }
        }

        Ok(findings)
    }
}

/// Create a default scanner with environment-based configuration.
pub fn create_scanner() -> Result<Scanner> {
    let config = Config::default();
    Scanner::new(config)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_scanner_creation() {
        let config = Config::builder()
            .enable_sast(true)
            .enable_sca(true)
            .build();

        let scanner = Scanner::new(config);
        assert!(scanner.is_ok());
    }

    #[test]
    fn test_scan_target_parsing() {
        assert!(matches!(
            ScanTarget::parse("owner/repo"),
            ScanTarget::Repository(_)
        ));

        assert!(matches!(
            ScanTarget::parse("https://github.com/owner/repo"),
            ScanTarget::Repository(_)
        ));

        assert!(matches!(
            ScanTarget::parse("org:myorg"),
            ScanTarget::Organization(_)
        ));

        assert!(matches!(
            ScanTarget::parse("user:myuser"),
            ScanTarget::User(_)
        ));
    }
}

================================================================================
END: src\lib.rs
================================================================================

================================================================================
FILE: src\config.rs
================================================================================
//! Configuration management for the security auditor.

use crate::models::Severity;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;

/// Main configuration for the security auditor.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Config {
    /// GitHub configuration
    pub github: GitHubConfig,

    /// Analysis configuration
    pub analysis: AnalysisConfig,

    /// Output configuration
    pub output: OutputConfig,

    /// Concurrency settings
    pub concurrency: ConcurrencyConfig,
}

/// GitHub API configuration.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GitHubConfig {
    /// GitHub personal access token or app token
    pub token: Option<String>,

    /// GitHub API base URL (for GitHub Enterprise)
    #[serde(default = "default_github_api_url")]
    pub api_url: String,

    /// Rate limit handling: delay in milliseconds between requests
    #[serde(default = "default_rate_limit_delay")]
    pub rate_limit_delay_ms: u64,

    /// Maximum repositories to scan in a single run
    #[serde(default = "default_max_repos")]
    pub max_repos: usize,
}

/// Analysis configuration.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AnalysisConfig {
    /// Enable SAST (Static Application Security Testing)
    #[serde(default = "default_true")]
    pub enable_sast: bool,

    /// Enable SCA (Software Composition Analysis)
    #[serde(default = "default_true")]
    pub enable_sca: bool,

    /// Enable secret detection
    #[serde(default = "default_true")]
    pub enable_secrets: bool,

    /// Enable provenance verification (SLSA/Sigstore)
    #[serde(default)]
    pub enable_provenance: bool,

    /// Enable AI-driven analysis
    #[serde(default)]
    pub enable_ai: bool,

    /// Languages to analyze
    #[serde(default = "default_languages")]
    pub languages: Vec<String>,

    /// File patterns to ignore
    #[serde(default = "default_ignore_patterns")]
    pub ignore_patterns: Vec<String>,

    /// Maximum file size to analyze (in bytes)
    #[serde(default = "default_max_file_size")]
    pub max_file_size: usize,

    /// Minimum entropy threshold for secret detection
    #[serde(default = "default_entropy_threshold")]
    pub entropy_threshold: f64,

    /// Minimum severity level to report
    #[serde(default = "default_min_severity")]
    pub min_severity: Severity,

    /// Temporary directory for cloning repositories
    pub temp_dir: Option<PathBuf>,
}

/// Output configuration.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OutputConfig {
    /// Output format
    #[serde(default)]
    pub format: OutputFormat,

    /// Output file path (stdout if not specified)
    pub output_path: Option<PathBuf>,

    /// Include source code snippets in findings
    #[serde(default = "default_true")]
    pub include_snippets: bool,

    /// Maximum snippet lines
    #[serde(default = "default_snippet_lines")]
    pub snippet_lines: usize,
}

/// Concurrency configuration.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ConcurrencyConfig {
    /// Number of tokio worker threads (0 = auto)
    #[serde(default)]
    pub tokio_workers: usize,

    /// Number of rayon threads for CPU-bound work (0 = auto)
    #[serde(default)]
    pub rayon_threads: usize,

    /// Maximum concurrent repository clones
    #[serde(default = "default_concurrent_clones")]
    pub concurrent_clones: usize,

    /// Channel buffer size for pipeline
    #[serde(default = "default_channel_buffer")]
    pub channel_buffer: usize,

    /// Maximum files to process in a single parallel batch
    /// (helps control memory usage for large repositories)
    #[serde(default = "default_batch_size")]
    pub batch_size: usize,
}

/// Output format enumeration.
#[derive(Debug, Clone, Default, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "lowercase")]
pub enum OutputFormat {
    /// SARIF (Static Analysis Results Interchange Format)
    #[default]
    Sarif,
    /// JSON format
    Json,
    /// Human-readable text
    Text,
}

// Default value functions
fn default_github_api_url() -> String {
    "https://api.github.com".to_string()
}

fn default_rate_limit_delay() -> u64 {
    100
}

fn default_max_repos() -> usize {
    100
}

fn default_true() -> bool {
    true
}

fn default_languages() -> Vec<String> {
    vec![
        "rust".to_string(),
        "python".to_string(),
        "javascript".to_string(),
        "go".to_string(),
    ]
}

fn default_ignore_patterns() -> Vec<String> {
    vec![
        "**/target/**".to_string(),
        "**/node_modules/**".to_string(),
        "**/vendor/**".to_string(),
        "**/.git/**".to_string(),
        "**/dist/**".to_string(),
        "**/build/**".to_string(),
    ]
}

fn default_max_file_size() -> usize {
    1024 * 1024 // 1 MB
}

fn default_entropy_threshold() -> f64 {
    4.5
}

fn default_min_severity() -> Severity {
    Severity::Low
}

fn default_snippet_lines() -> usize {
    5
}

fn default_concurrent_clones() -> usize {
    4
}

fn default_channel_buffer() -> usize {
    100
}

fn default_batch_size() -> usize {
    500 // Process files in batches to control memory
}

impl Default for Config {
    fn default() -> Self {
        Self {
            github: GitHubConfig::default(),
            analysis: AnalysisConfig::default(),
            output: OutputConfig::default(),
            concurrency: ConcurrencyConfig::default(),
        }
    }
}

impl Default for GitHubConfig {
    fn default() -> Self {
        Self {
            token: std::env::var("GITHUB_TOKEN").ok(),
            api_url: default_github_api_url(),
            rate_limit_delay_ms: default_rate_limit_delay(),
            max_repos: default_max_repos(),
        }
    }
}

impl Default for AnalysisConfig {
    fn default() -> Self {
        Self {
            enable_sast: true,
            enable_sca: true,
            enable_secrets: true,
            enable_provenance: false,
            enable_ai: false,
            languages: default_languages(),
            ignore_patterns: default_ignore_patterns(),
            max_file_size: default_max_file_size(),
            entropy_threshold: default_entropy_threshold(),
            min_severity: default_min_severity(),
            temp_dir: None,
        }
    }
}

impl Default for OutputConfig {
    fn default() -> Self {
        Self {
            format: OutputFormat::default(),
            output_path: None,
            include_snippets: true,
            snippet_lines: default_snippet_lines(),
        }
    }
}

impl Default for ConcurrencyConfig {
    fn default() -> Self {
        Self {
            tokio_workers: 0,
            rayon_threads: 0,
            concurrent_clones: default_concurrent_clones(),
            channel_buffer: default_channel_buffer(),
            batch_size: default_batch_size(),
        }
    }
}

impl Config {
    /// Load configuration from a TOML file.
    pub fn from_file(path: &std::path::Path) -> crate::error::Result<Self> {
        let content = std::fs::read_to_string(path)?;
        let config: Config = toml::from_str(&content)?;
        Ok(config)
    }

    /// Create a configuration builder.
    pub fn builder() -> ConfigBuilder {
        ConfigBuilder::default()
    }
}

/// Builder for creating configurations programmatically.
#[derive(Debug, Default)]
pub struct ConfigBuilder {
    config: Config,
}

impl ConfigBuilder {
    pub fn github_token(mut self, token: impl Into<String>) -> Self {
        self.config.github.token = Some(token.into());
        self
    }

    pub fn enable_sast(mut self, enable: bool) -> Self {
        self.config.analysis.enable_sast = enable;
        self
    }

    pub fn enable_sca(mut self, enable: bool) -> Self {
        self.config.analysis.enable_sca = enable;
        self
    }

    pub fn enable_secrets(mut self, enable: bool) -> Self {
        self.config.analysis.enable_secrets = enable;
        self
    }

    pub fn enable_provenance(mut self, enable: bool) -> Self {
        self.config.analysis.enable_provenance = enable;
        self
    }

    pub fn enable_ai(mut self, enable: bool) -> Self {
        self.config.analysis.enable_ai = enable;
        self
    }

    pub fn min_severity(mut self, severity: Severity) -> Self {
        self.config.analysis.min_severity = severity;
        self
    }

    pub fn output_format(mut self, format: OutputFormat) -> Self {
        self.config.output.format = format;
        self
    }

    pub fn output_path(mut self, path: PathBuf) -> Self {
        self.config.output.output_path = Some(path);
        self
    }

    pub fn temp_dir(mut self, path: PathBuf) -> Self {
        self.config.analysis.temp_dir = Some(path);
        self
    }

    pub fn languages(mut self, languages: Vec<String>) -> Self {
        self.config.analysis.languages = languages;
        self
    }

    pub fn max_repos(mut self, max: usize) -> Self {
        self.config.github.max_repos = max;
        self
    }

    pub fn build(self) -> Config {
        self.config
    }
}

================================================================================
END: src\config.rs
================================================================================

================================================================================
FILE: src\error.rs
================================================================================
//! Error types for the security auditor application.

use thiserror::Error;

/// Main error type for the security auditor.
#[derive(Error, Debug)]
pub enum AuditorError {
    #[error("GitHub API error: {0}")]
    GitHub(String),

    #[error("Git operation failed: {0}")]
    Git(#[from] git2::Error),

    #[error("Failed to clone repository: {0}")]
    Clone(String),

    #[error("Analysis error: {0}")]
    Analysis(String),

    #[error("Tree-sitter parsing error: {0}")]
    Parse(String),

    #[error("OSV query failed: {0}")]
    Osv(String),

    #[error("Sigstore verification failed: {0}")]
    Sigstore(String),

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    #[error("Serialization error: {0}")]
    Serialization(#[from] serde_json::Error),

    #[error("TOML parsing error: {0}")]
    Toml(#[from] toml::de::Error),

    #[error("HTTP request failed: {0}")]
    Http(#[from] reqwest::Error),

    #[error("URL parse error: {0}")]
    Url(#[from] url::ParseError),

    #[error("Configuration error: {0}")]
    Config(String),

    #[error("Rate limited: retry after {0} seconds")]
    RateLimited(u64),

    #[error("Repository not found: {0}")]
    NotFound(String),

    #[error("Authentication required")]
    AuthRequired,

    #[error("Invalid Cargo.lock format: {0}")]
    CargoLock(String),

    #[error("Channel send error")]
    ChannelSend,

    #[error("Channel receive error")]
    ChannelRecv,
}

/// Result type alias for auditor operations.
pub type Result<T> = std::result::Result<T, AuditorError>;

impl From<octocrab::Error> for AuditorError {
    fn from(err: octocrab::Error) -> Self {
        AuditorError::GitHub(err.to_string())
    }
}

impl From<cargo_lock::Error> for AuditorError {
    fn from(err: cargo_lock::Error) -> Self {
        AuditorError::CargoLock(err.to_string())
    }
}

impl<T> From<async_channel::SendError<T>> for AuditorError {
    fn from(_: async_channel::SendError<T>) -> Self {
        AuditorError::ChannelSend
    }
}

impl From<async_channel::RecvError> for AuditorError {
    fn from(_: async_channel::RecvError) -> Self {
        AuditorError::ChannelRecv
    }
}

================================================================================
END: src\error.rs
================================================================================

================================================================================
FILE: src\concurrency.rs
================================================================================
//! Advanced concurrency utilities for optimal resource utilization.
//!
//! This module provides:
//! - **Core Partitioning**: Dedicates CPU cores between Tokio (async I/O) and Rayon (parallel compute)
//! - **Batched Channels**: Amortizes channel overhead by sending batches of items
//! - **AIMD Controller**: Adaptive concurrency using Additive Increase Multiplicative Decrease
//!
//! # Research Background
//!
//! The optimal split between I/O and compute threads depends on workload characteristics:
//! - For I/O-heavy workloads (GitHub API, network): More Tokio threads
//! - For CPU-heavy workloads (parsing, analysis): More Rayon threads
//!
//! The default 20% Tokio / 80% Rayon split is based on profiling security analysis
//! workloads where parsing and pattern matching dominate runtime.

use std::sync::atomic::{AtomicU64, AtomicUsize, Ordering};
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::mpsc;
use tracing::{debug, info, warn};

// ============================================================================
// Core Partitioning
// ============================================================================

/// Configuration for CPU core partitioning between async I/O and parallel compute.
#[derive(Debug, Clone)]
pub struct CorePartitionConfig {
    /// Fraction of cores to dedicate to Tokio (async I/O). Default: 0.20 (20%)
    pub tokio_fraction: f64,
    /// Minimum number of Tokio worker threads.
    pub tokio_min_threads: usize,
    /// Maximum number of Tokio worker threads.
    pub tokio_max_threads: usize,
    /// Minimum number of Rayon worker threads.
    pub rayon_min_threads: usize,
    /// Whether to enable work stealing between pools.
    pub enable_work_stealing: bool,
}

impl Default for CorePartitionConfig {
    fn default() -> Self {
        Self {
            tokio_fraction: 0.20,
            tokio_min_threads: 1,
            tokio_max_threads: 8,
            rayon_min_threads: 2,
            enable_work_stealing: true,
        }
    }
}

/// Computed core partition for the system.
#[derive(Debug, Clone)]
pub struct CorePartition {
    /// Number of cores available on the system.
    pub total_cores: usize,
    /// Number of threads for Tokio runtime.
    pub tokio_threads: usize,
    /// Number of threads for Rayon pool.
    pub rayon_threads: usize,
    /// Configuration used.
    pub config: CorePartitionConfig,
}

impl CorePartition {
    /// Compute an optimal core partition for the current system.
    pub fn compute(config: CorePartitionConfig) -> Self {
        let total_cores = num_cpus::get();

        // Calculate Tokio threads (for async I/O)
        let tokio_ideal = (total_cores as f64 * config.tokio_fraction).ceil() as usize;
        let tokio_threads = tokio_ideal
            .max(config.tokio_min_threads)
            .min(config.tokio_max_threads)
            .min(total_cores);

        // Remaining cores go to Rayon (for parallel compute)
        let rayon_threads = (total_cores - tokio_threads).max(config.rayon_min_threads);

        info!(
            "Core partition: {} total cores -> {} Tokio + {} Rayon",
            total_cores, tokio_threads, rayon_threads
        );

        Self {
            total_cores,
            tokio_threads,
            rayon_threads,
            config,
        }
    }

    /// Compute partition with default configuration.
    pub fn default_partition() -> Self {
        Self::compute(CorePartitionConfig::default())
    }

    /// Initialize Rayon global thread pool with the computed partition.
    ///
    /// This should be called once at application startup before using Rayon.
    pub fn init_rayon_pool(&self) -> Result<(), rayon::ThreadPoolBuildError> {
        rayon::ThreadPoolBuilder::new()
            .num_threads(self.rayon_threads)
            .thread_name(|idx| format!("rayon-worker-{}", idx))
            .build_global()
    }

    /// Create a Tokio runtime with the computed partition.
    pub fn build_tokio_runtime(&self) -> std::io::Result<tokio::runtime::Runtime> {
        tokio::runtime::Builder::new_multi_thread()
            .worker_threads(self.tokio_threads)
            .thread_name("tokio-worker")
            .enable_all()
            .build()
    }
}

// ============================================================================
// Batched Channel Transport
// ============================================================================

/// Configuration for batched channel transport.
#[derive(Debug, Clone)]
pub struct BatchConfig {
    /// Maximum number of items per batch.
    pub max_batch_size: usize,
    /// Maximum time to wait before flushing a partial batch.
    pub max_batch_delay: Duration,
    /// Channel buffer size (in batches).
    pub channel_buffer: usize,
}

impl Default for BatchConfig {
    fn default() -> Self {
        Self {
            max_batch_size: 64,
            max_batch_delay: Duration::from_millis(10),
            channel_buffer: 16,
        }
    }
}

/// A sender that batches items before sending.
pub struct BatchSender<T> {
    /// Inner channel sender.
    inner: mpsc::Sender<Vec<T>>,
    /// Current batch being accumulated.
    current_batch: Vec<T>,
    /// Configuration.
    config: BatchConfig,
    /// Last flush time.
    last_flush: Instant,
    /// Statistics.
    stats: Arc<BatchStats>,
}

/// A receiver that receives batches of items.
pub struct BatchReceiver<T> {
    /// Inner channel receiver.
    inner: mpsc::Receiver<Vec<T>>,
    /// Current batch being consumed.
    current_batch: Vec<T>,
    /// Position in current batch.
    batch_pos: usize,
    /// Statistics.
    stats: Arc<BatchStats>,
}

/// Statistics for batched channel.
#[derive(Debug, Default)]
pub struct BatchStats {
    /// Total items sent.
    pub items_sent: AtomicU64,
    /// Total batches sent.
    pub batches_sent: AtomicU64,
    /// Total items received.
    pub items_received: AtomicU64,
    /// Total batches received.
    pub batches_received: AtomicU64,
}

impl BatchStats {
    /// Calculate average batch size.
    pub fn avg_batch_size(&self) -> f64 {
        let items = self.items_sent.load(Ordering::Relaxed) as f64;
        let batches = self.batches_sent.load(Ordering::Relaxed) as f64;
        if batches > 0.0 {
            items / batches
        } else {
            0.0
        }
    }
}

/// Create a batched channel pair.
pub fn batched_channel<T>(config: BatchConfig) -> (BatchSender<T>, BatchReceiver<T>) {
    let (tx, rx) = mpsc::channel(config.channel_buffer);
    let stats = Arc::new(BatchStats::default());

    let sender = BatchSender {
        inner: tx,
        current_batch: Vec::with_capacity(config.max_batch_size),
        config: config.clone(),
        last_flush: Instant::now(),
        stats: Arc::clone(&stats),
    };

    let receiver = BatchReceiver {
        inner: rx,
        current_batch: Vec::new(),
        batch_pos: 0,
        stats,
    };

    (sender, receiver)
}

impl<T: Send> BatchSender<T> {
    /// Send an item, batching it for efficiency.
    pub async fn send(&mut self, item: T) -> Result<(), mpsc::error::SendError<Vec<T>>> {
        self.current_batch.push(item);

        // Flush if batch is full or timeout elapsed
        let should_flush = self.current_batch.len() >= self.config.max_batch_size
            || self.last_flush.elapsed() >= self.config.max_batch_delay;

        if should_flush {
            self.flush().await?;
        }

        Ok(())
    }

    /// Flush the current batch.
    pub async fn flush(&mut self) -> Result<(), mpsc::error::SendError<Vec<T>>> {
        if !self.current_batch.is_empty() {
            let batch = std::mem::replace(
                &mut self.current_batch,
                Vec::with_capacity(self.config.max_batch_size),
            );

            self.stats
                .items_sent
                .fetch_add(batch.len() as u64, Ordering::Relaxed);
            self.stats.batches_sent.fetch_add(1, Ordering::Relaxed);

            self.inner.send(batch).await?;
            self.last_flush = Instant::now();
        }
        Ok(())
    }

    /// Get channel statistics.
    pub fn stats(&self) -> &BatchStats {
        &self.stats
    }
}

impl<T> BatchReceiver<T> {
    /// Receive the next item.
    pub async fn recv(&mut self) -> Option<T> {
        // Return from current batch if available
        if self.batch_pos < self.current_batch.len() {
            let item = self.current_batch.swap_remove(self.batch_pos);
            self.stats.items_received.fetch_add(1, Ordering::Relaxed);
            return Some(item);
        }

        // Get next batch
        self.current_batch = self.inner.recv().await?;
        self.batch_pos = 0;
        self.stats.batches_received.fetch_add(1, Ordering::Relaxed);

        if !self.current_batch.is_empty() {
            let item = self.current_batch.swap_remove(0);
            self.stats.items_received.fetch_add(1, Ordering::Relaxed);
            Some(item)
        } else {
            None
        }
    }

    /// Try to receive the next item without blocking.
    pub fn try_recv(&mut self) -> Option<T> {
        // Return from current batch if available
        if self.batch_pos < self.current_batch.len() {
            let item = self.current_batch.swap_remove(self.batch_pos);
            self.stats.items_received.fetch_add(1, Ordering::Relaxed);
            return Some(item);
        }

        // Try to get next batch
        match self.inner.try_recv() {
            Ok(batch) => {
                self.current_batch = batch;
                self.batch_pos = 0;
                self.stats.batches_received.fetch_add(1, Ordering::Relaxed);

                if !self.current_batch.is_empty() {
                    let item = self.current_batch.swap_remove(0);
                    self.stats.items_received.fetch_add(1, Ordering::Relaxed);
                    Some(item)
                } else {
                    None
                }
            }
            Err(_) => None,
        }
    }

    /// Get channel statistics.
    pub fn stats(&self) -> &BatchStats {
        &self.stats
    }
}

// ============================================================================
// AIMD Adaptive Concurrency Controller
// ============================================================================

/// Configuration for AIMD (Additive Increase Multiplicative Decrease) controller.
#[derive(Debug, Clone)]
pub struct AimdConfig {
    /// Initial concurrency limit.
    pub initial_limit: usize,
    /// Minimum concurrency limit.
    pub min_limit: usize,
    /// Maximum concurrency limit.
    pub max_limit: usize,
    /// Additive increase step (on success).
    pub additive_increase: usize,
    /// Multiplicative decrease factor (on failure). E.g., 0.5 = halve.
    pub multiplicative_decrease: f64,
    /// Latency threshold for proactive decrease (in ms).
    pub latency_threshold_ms: u64,
    /// Sample window for latency averaging.
    pub sample_window: usize,
}

impl Default for AimdConfig {
    fn default() -> Self {
        Self {
            initial_limit: 8,
            min_limit: 1,
            max_limit: 128,
            additive_increase: 1,
            multiplicative_decrease: 0.5,
            latency_threshold_ms: 500,
            sample_window: 20,
        }
    }
}

/// AIMD-based adaptive concurrency controller.
///
/// This controller dynamically adjusts the concurrency limit based on feedback:
/// - **Success**: Increase limit additively (slow growth)
/// - **Failure/Timeout**: Decrease limit multiplicatively (fast reduction)
/// - **High Latency**: Proactively decrease before failures occur
///
/// This is inspired by TCP congestion control and is widely used in
/// load balancing (e.g., Netflix Concurrency Limits library).
pub struct AimdController {
    /// Current concurrency limit.
    limit: AtomicUsize,
    /// Current in-flight requests.
    in_flight: AtomicUsize,
    /// Configuration.
    config: AimdConfig,
    /// Recent latency samples (ring buffer).
    latency_samples: parking_lot::Mutex<Vec<u64>>,
    /// Sample position.
    sample_pos: AtomicUsize,
    /// Statistics.
    stats: AimdStats,
}

/// Statistics for AIMD controller.
#[derive(Debug, Default)]
pub struct AimdStats {
    /// Total successful requests.
    pub successes: AtomicU64,
    /// Total failed requests.
    pub failures: AtomicU64,
    /// Total limit increases.
    pub increases: AtomicU64,
    /// Total limit decreases.
    pub decreases: AtomicU64,
}

impl AimdController {
    /// Create a new AIMD controller.
    pub fn new(config: AimdConfig) -> Self {
        let limit = config.initial_limit;
        let sample_window = config.sample_window;

        Self {
            limit: AtomicUsize::new(limit),
            in_flight: AtomicUsize::new(0),
            config,
            latency_samples: parking_lot::Mutex::new(vec![0; sample_window]),
            sample_pos: AtomicUsize::new(0),
            stats: AimdStats::default(),
        }
    }

    /// Create with default configuration.
    pub fn default_controller() -> Self {
        Self::new(AimdConfig::default())
    }

    /// Get the current concurrency limit.
    pub fn limit(&self) -> usize {
        self.limit.load(Ordering::Relaxed)
    }

    /// Get the current in-flight count.
    pub fn in_flight(&self) -> usize {
        self.in_flight.load(Ordering::Relaxed)
    }

    /// Check if we can acquire a slot (non-blocking).
    pub fn try_acquire(&self) -> Option<AimdPermit<'_>> {
        let current = self.in_flight.load(Ordering::Relaxed);
        let limit = self.limit.load(Ordering::Relaxed);

        if current < limit {
            // Try to increment in-flight counter
            let result = self.in_flight.compare_exchange(
                current,
                current + 1,
                Ordering::Acquire,
                Ordering::Relaxed,
            );

            if result.is_ok() {
                return Some(AimdPermit {
                    controller: self,
                    start_time: Instant::now(),
                });
            }
        }

        None
    }

    /// Wait to acquire a slot.
    pub async fn acquire(&self) -> AimdPermit<'_> {
        loop {
            if let Some(permit) = self.try_acquire() {
                return permit;
            }
            // Backoff before retry
            tokio::time::sleep(Duration::from_micros(100)).await;
        }
    }

    /// Record a successful completion.
    fn on_success(&self, latency_ms: u64) {
        self.stats.successes.fetch_add(1, Ordering::Relaxed);

        // Record latency sample
        let pos = self.sample_pos.fetch_add(1, Ordering::Relaxed) % self.config.sample_window;
        {
            let mut samples = self.latency_samples.lock();
            samples[pos] = latency_ms;
        }

        // Check if latency is acceptable
        let avg_latency = self.avg_latency();
        if avg_latency > self.config.latency_threshold_ms {
            // High latency - proactively decrease
            self.decrease_limit();
            debug!(
                "AIMD: Proactive decrease due to latency {}ms > {}ms threshold",
                avg_latency, self.config.latency_threshold_ms
            );
        } else {
            // Good latency - increase limit
            self.increase_limit();
        }
    }

    /// Record a failure.
    fn on_failure(&self) {
        self.stats.failures.fetch_add(1, Ordering::Relaxed);
        self.decrease_limit();
        debug!("AIMD: Decrease due to failure");
    }

    /// Increase the limit additively.
    fn increase_limit(&self) {
        let current = self.limit.load(Ordering::Relaxed);
        let new_limit = (current + self.config.additive_increase).min(self.config.max_limit);

        if new_limit > current {
            self.limit.store(new_limit, Ordering::Relaxed);
            self.stats.increases.fetch_add(1, Ordering::Relaxed);
        }
    }

    /// Decrease the limit multiplicatively.
    fn decrease_limit(&self) {
        let current = self.limit.load(Ordering::Relaxed);
        let new_limit = ((current as f64 * self.config.multiplicative_decrease) as usize)
            .max(self.config.min_limit);

        if new_limit < current {
            self.limit.store(new_limit, Ordering::Relaxed);
            self.stats.decreases.fetch_add(1, Ordering::Relaxed);
        }
    }

    /// Calculate average latency from samples.
    fn avg_latency(&self) -> u64 {
        let samples = self.latency_samples.lock();
        let sum: u64 = samples.iter().sum();
        sum / samples.len() as u64
    }

    /// Get controller statistics.
    pub fn stats(&self) -> &AimdStats {
        &self.stats
    }
}

/// A permit to perform work under AIMD control.
pub struct AimdPermit<'a> {
    controller: &'a AimdController,
    start_time: Instant,
}

impl<'a> AimdPermit<'a> {
    /// Complete successfully.
    pub fn success(self) {
        let latency = self.start_time.elapsed().as_millis() as u64;
        self.controller.in_flight.fetch_sub(1, Ordering::Release);
        self.controller.on_success(latency);
    }

    /// Complete with failure.
    pub fn failure(self) {
        self.controller.in_flight.fetch_sub(1, Ordering::Release);
        self.controller.on_failure();
    }
}

impl Drop for AimdPermit<'_> {
    fn drop(&mut self) {
        // If not explicitly completed, treat as failure
        // Note: This is overly conservative; in practice you should call success() or failure()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_core_partition_default() {
        let partition = CorePartition::default_partition();

        assert!(partition.total_cores > 0);
        assert!(partition.tokio_threads >= 1);
        assert!(partition.rayon_threads >= 2);
        assert_eq!(
            partition.tokio_threads + partition.rayon_threads,
            partition.total_cores.max(3)
        );
    }

    #[test]
    fn test_core_partition_custom() {
        let config = CorePartitionConfig {
            tokio_fraction: 0.50,
            tokio_min_threads: 2,
            tokio_max_threads: 16,
            rayon_min_threads: 4,
            enable_work_stealing: true,
        };

        let partition = CorePartition::compute(config);

        assert!(partition.tokio_threads >= 2);
        assert!(partition.rayon_threads >= 4);
    }

    #[tokio::test]
    async fn test_batched_channel() {
        let config = BatchConfig {
            max_batch_size: 3,
            max_batch_delay: Duration::from_millis(100),
            channel_buffer: 4,
        };

        let (mut tx, mut rx) = batched_channel::<i32>(config);

        // Send items
        tx.send(1).await.unwrap();
        tx.send(2).await.unwrap();
        tx.send(3).await.unwrap(); // This should trigger a flush
        tx.flush().await.unwrap();

        // Receive items - collect all and check we got all 3
        let mut received = Vec::new();
        while let Some(item) = rx.recv().await {
            received.push(item);
            if received.len() == 3 {
                break;
            }
        }

        received.sort();
        assert_eq!(received, vec![1, 2, 3]);
    }

    #[test]
    fn test_aimd_controller() {
        let controller = AimdController::default_controller();

        assert_eq!(controller.limit(), 8); // Default initial limit
        assert_eq!(controller.in_flight(), 0);

        // Acquire permits
        let permit1 = controller.try_acquire().expect("Should acquire");
        assert_eq!(controller.in_flight(), 1);

        let permit2 = controller.try_acquire().expect("Should acquire");
        assert_eq!(controller.in_flight(), 2);

        // Complete with success
        permit1.success();
        assert_eq!(controller.in_flight(), 1);

        // Limit should increase after success
        assert!(controller.limit() >= 8);

        permit2.failure();
        assert_eq!(controller.in_flight(), 0);

        // Limit should decrease after failure
        assert!(controller.limit() <= 8);
    }
}

================================================================================
END: src\concurrency.rs
================================================================================

================================================================================
FILE: src\models\mod.rs
================================================================================
//! Data models for the security auditor.

mod finding;
mod repository;
mod vulnerability;

pub use finding::*;
pub use repository::*;
pub use vulnerability::*;

================================================================================
END: src\models\mod.rs
================================================================================

================================================================================
FILE: src\models\finding.rs
================================================================================
//! Security finding data models.

use super::{Language, Severity, Vulnerability};
use serde::{Deserialize, Serialize};
use std::path::PathBuf;

/// Represents a security finding discovered during analysis.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Finding {
    /// Unique finding ID
    pub id: String,

    /// Finding category
    pub category: FindingCategory,

    /// Severity level
    pub severity: Severity,

    /// Short title
    pub title: String,

    /// Detailed description
    pub description: String,

    /// Location of the finding
    pub location: Location,

    /// Source code snippet (if available)
    pub snippet: Option<CodeSnippet>,

    /// Related vulnerability (for SCA findings)
    pub vulnerability: Option<Vulnerability>,

    /// Confidence level
    pub confidence: Confidence,

    /// Rule or query that triggered this finding
    pub rule_id: String,

    /// Suggested fix or remediation
    pub remediation: Option<String>,

    /// Additional metadata
    #[serde(default)]
    pub metadata: std::collections::HashMap<String, serde_json::Value>,

    /// Timestamp when finding was discovered
    pub discovered_at: chrono::DateTime<chrono::Utc>,
}

impl Finding {
    /// Create a new SAST finding.
    pub fn sast(
        rule_id: impl Into<String>,
        title: impl Into<String>,
        description: impl Into<String>,
        location: Location,
        severity: Severity,
    ) -> Self {
        Self {
            id: uuid_v4(),
            category: FindingCategory::Sast(SastCategory::Other),
            severity,
            title: title.into(),
            description: description.into(),
            location,
            snippet: None,
            vulnerability: None,
            confidence: Confidence::Medium,
            rule_id: rule_id.into(),
            remediation: None,
            metadata: std::collections::HashMap::new(),
            discovered_at: chrono::Utc::now(),
        }
    }

    /// Create a new SCA finding.
    pub fn sca(vulnerability: Vulnerability, dependency: &str, version: &str) -> Self {
        Self {
            id: uuid_v4(),
            category: FindingCategory::Sca,
            severity: vulnerability.severity,
            title: format!(
                "Vulnerable dependency: {} {} ({})",
                dependency, version, vulnerability.id
            ),
            description: vulnerability.summary.clone(),
            location: Location {
                file: PathBuf::from("Cargo.lock"),
                start_line: 0,
                end_line: 0,
                start_column: 0,
                end_column: 0,
                language: None,
            },
            snippet: None,
            vulnerability: Some(vulnerability),
            confidence: Confidence::High,
            rule_id: "sca/vulnerable-dependency".to_string(),
            remediation: None,
            metadata: std::collections::HashMap::new(),
            discovered_at: chrono::Utc::now(),
        }
    }

    /// Set the SAST category.
    pub fn with_sast_category(mut self, category: SastCategory) -> Self {
        self.category = FindingCategory::Sast(category);
        self
    }

    /// Set the code snippet.
    pub fn with_snippet(mut self, snippet: CodeSnippet) -> Self {
        self.snippet = Some(snippet);
        self
    }

    /// Set the confidence level.
    pub fn with_confidence(mut self, confidence: Confidence) -> Self {
        self.confidence = confidence;
        self
    }

    /// Set remediation advice.
    pub fn with_remediation(mut self, remediation: impl Into<String>) -> Self {
        self.remediation = Some(remediation.into());
        self
    }

    /// Add metadata.
    pub fn with_metadata(mut self, key: impl Into<String>, value: serde_json::Value) -> Self {
        self.metadata.insert(key.into(), value);
        self
    }
}

/// Categories of findings.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "lowercase", tag = "type")]
pub enum FindingCategory {
    /// Static Application Security Testing finding
    Sast(SastCategory),

    /// Software Composition Analysis finding
    Sca,

    /// Supply chain / provenance finding
    Provenance,

    /// AI-detected finding
    Ai,

    /// Configuration issue
    Config,

    /// Secret detection
    Secret,
}

/// SAST finding subcategories.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "kebab-case")]
pub enum SastCategory {
    /// Unsafe code usage
    UnsafeCode,

    /// Memory safety issue
    MemorySafety,

    /// Injection vulnerability (SQL, command, etc.)
    Injection,

    /// Cross-site scripting
    Xss,

    /// Path traversal
    PathTraversal,

    /// Insecure cryptography
    Crypto,

    /// Authentication/authorization issue
    Auth,

    /// Information disclosure
    InfoDisclosure,

    /// Race condition
    RaceCondition,

    /// Denial of service
    Dos,

    /// Unbounded resource consumption
    UnboundedResource,

    /// Prototype pollution (JavaScript-specific)
    PrototypePollution,

    /// Other/uncategorized
    Other,
}

/// Location of a finding in source code.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Location {
    /// File path (relative to repository root)
    pub file: PathBuf,

    /// Start line (1-indexed)
    pub start_line: usize,

    /// End line (1-indexed)
    pub end_line: usize,

    /// Start column (1-indexed)
    pub start_column: usize,

    /// End column (1-indexed)
    pub end_column: usize,

    /// Language of the file
    pub language: Option<Language>,
}

impl Location {
    /// Create a new location.
    pub fn new(file: PathBuf, start_line: usize, start_column: usize) -> Self {
        Self {
            file,
            start_line,
            end_line: start_line,
            start_column,
            end_column: start_column,
            language: None,
        }
    }

    /// Set the end position.
    pub fn with_end(mut self, end_line: usize, end_column: usize) -> Self {
        self.end_line = end_line;
        self.end_column = end_column;
        self
    }

    /// Set the language.
    pub fn with_language(mut self, language: Language) -> Self {
        self.language = Some(language);
        self
    }
}

/// Code snippet for context.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CodeSnippet {
    /// Lines of code with line numbers
    pub lines: Vec<(usize, String)>,

    /// Highlighted line (the problematic line)
    pub highlight_line: usize,
}

impl CodeSnippet {
    /// Create a snippet from source content.
    pub fn from_content(content: &str, line: usize, context_lines: usize) -> Self {
        let lines: Vec<&str> = content.lines().collect();
        let start = line.saturating_sub(context_lines + 1);
        let end = (line + context_lines).min(lines.len());

        let snippet_lines = lines[start..end]
            .iter()
            .enumerate()
            .map(|(i, l)| (start + i + 1, l.to_string()))
            .collect();

        Self {
            lines: snippet_lines,
            highlight_line: line,
        }
    }
}

/// Confidence level for findings.
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum Confidence {
    Low,
    Medium,
    High,
}

impl Default for Confidence {
    fn default() -> Self {
        Confidence::Medium
    }
}

/// Scan result for a repository.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScanResult {
    /// Repository that was scanned
    pub repository: String,

    /// Commit SHA that was scanned
    pub commit_sha: Option<String>,

    /// All findings
    pub findings: Vec<Finding>,

    /// Scan statistics
    pub stats: ScanStats,

    /// Scan start time
    pub started_at: chrono::DateTime<chrono::Utc>,

    /// Scan end time
    pub completed_at: chrono::DateTime<chrono::Utc>,

    /// Whether the scan completed successfully
    pub success: bool,

    /// Error message if scan failed
    pub error: Option<String>,
}

impl ScanResult {
    /// Create a new scan result.
    pub fn new(repository: impl Into<String>) -> Self {
        let now = chrono::Utc::now();
        Self {
            repository: repository.into(),
            commit_sha: None,
            findings: Vec::new(),
            stats: ScanStats::default(),
            started_at: now,
            completed_at: now,
            success: true,
            error: None,
        }
    }

    /// Add a finding.
    pub fn add_finding(&mut self, finding: Finding) {
        match finding.category {
            FindingCategory::Sast(_) => self.stats.sast_findings += 1,
            FindingCategory::Sca => self.stats.sca_findings += 1,
            FindingCategory::Secret => self.stats.secrets_found += 1,
            _ => {}
        }
        self.findings.push(finding);
    }

    /// Get findings by severity.
    pub fn findings_by_severity(&self, severity: Severity) -> Vec<&Finding> {
        self.findings.iter().filter(|f| f.severity == severity).collect()
    }

    /// Get the highest severity finding.
    pub fn max_severity(&self) -> Severity {
        self.findings
            .iter()
            .map(|f| f.severity)
            .max()
            .unwrap_or(Severity::None)
    }
}

/// Statistics about a scan.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct ScanStats {
    /// Number of files scanned
    pub files_scanned: usize,

    /// Total lines of code analyzed
    pub lines_analyzed: usize,

    /// Number of SAST findings
    pub sast_findings: usize,

    /// Number of SCA findings
    pub sca_findings: usize,

    /// Number of dependencies checked
    pub dependencies_checked: usize,

    /// Number of secrets found
    pub secrets_found: usize,

    /// Scan duration in milliseconds
    pub duration_ms: u64,
}

/// Generate a proper UUID v4 using cryptographic randomness.
fn uuid_v4() -> String {
    uuid::Uuid::new_v4().to_string()
}

================================================================================
END: src\models\finding.rs
================================================================================

================================================================================
FILE: src\models\repository.rs
================================================================================
//! Repository data models.

use serde::{Deserialize, Serialize};
use std::path::PathBuf;

/// Represents a GitHub repository to be analyzed.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Repository {
    /// Repository owner (user or organization)
    pub owner: String,

    /// Repository name
    pub name: String,

    /// Full name (owner/name)
    pub full_name: String,

    /// Default branch
    pub default_branch: String,

    /// Latest commit SHA
    pub commit_sha: Option<String>,

    /// Repository URL
    pub url: String,

    /// Clone URL
    pub clone_url: String,

    /// Primary language
    pub language: Option<String>,

    /// All detected languages
    pub languages: Vec<String>,

    /// Star count
    pub stars: u64,

    /// Fork count
    pub forks: u64,

    /// Whether the repository is archived
    pub archived: bool,

    /// Whether the repository is a fork
    pub is_fork: bool,

    /// Repository description
    pub description: Option<String>,

    /// Topics/tags
    pub topics: Vec<String>,

    /// Last updated timestamp
    pub updated_at: Option<chrono::DateTime<chrono::Utc>>,

    /// Local path after cloning
    #[serde(skip)]
    pub local_path: Option<PathBuf>,
}

impl Repository {
    /// Create a new repository from owner and name.
    pub fn new(owner: impl Into<String>, name: impl Into<String>) -> Self {
        let owner = owner.into();
        let name = name.into();
        let full_name = format!("{}/{}", owner, name);
        let url = format!("https://github.com/{}", full_name);
        let clone_url = format!("https://github.com/{}.git", full_name);

        Self {
            owner,
            name,
            full_name,
            default_branch: "main".to_string(),
            commit_sha: None,
            url,
            clone_url,
            language: None,
            languages: Vec::new(),
            stars: 0,
            forks: 0,
            archived: false,
            is_fork: false,
            description: None,
            topics: Vec::new(),
            updated_at: None,
            local_path: None,
        }
    }

    /// Parse a repository from a GitHub URL or owner/name string.
    pub fn from_url(input: &str) -> Option<Self> {
        // Handle full URLs
        let normalized = input
            .trim()
            .trim_end_matches('/')
            .trim_end_matches(".git");

        // Extract owner/name from URL or direct input
        let parts: Vec<&str> = if normalized.contains("github.com") {
            normalized
                .split("github.com")
                .last()?
                .trim_start_matches('/')
                .trim_start_matches(':')
                .split('/')
                .collect()
        } else {
            normalized.split('/').collect()
        };

        if parts.len() >= 2 {
            let owner = parts[parts.len() - 2].to_string();
            let name = parts[parts.len() - 1].to_string();
            Some(Self::new(owner, name))
        } else {
            None
        }
    }

    /// Check if this repository has been cloned locally.
    pub fn is_cloned(&self) -> bool {
        self.local_path
            .as_ref()
            .map(|p| p.exists())
            .unwrap_or(false)
    }
}

/// Source file within a repository.
#[derive(Debug, Clone)]
pub struct SourceFile {
    /// Relative path from repository root
    pub path: PathBuf,

    /// Absolute path on disk
    pub absolute_path: PathBuf,

    /// Detected language
    pub language: Language,

    /// File size in bytes
    pub size: u64,

    /// File content (loaded on demand)
    pub content: Option<String>,
}

impl SourceFile {
    /// Load the file content if not already loaded.
    pub fn load_content(&mut self) -> std::io::Result<&str> {
        if self.content.is_none() {
            self.content = Some(std::fs::read_to_string(&self.absolute_path)?);
        }
        Ok(self.content.as_ref().unwrap())
    }
}

/// Supported programming languages.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum Language {
    Rust,
    Python,
    JavaScript,
    TypeScript,
    Go,
    Java,
    C,
    Cpp,
    Ruby,
    Unknown,
}

impl Language {
    /// Detect language from file extension.
    pub fn from_extension(ext: &str) -> Self {
        match ext.to_lowercase().as_str() {
            "rs" => Language::Rust,
            "py" => Language::Python,
            "js" | "mjs" | "cjs" => Language::JavaScript,
            "ts" | "tsx" => Language::TypeScript,
            "go" => Language::Go,
            "java" => Language::Java,
            "c" | "h" => Language::C,
            "cpp" | "cc" | "cxx" | "hpp" | "hh" => Language::Cpp,
            "rb" => Language::Ruby,
            _ => Language::Unknown,
        }
    }

    /// Get the tree-sitter language name.
    pub fn tree_sitter_name(&self) -> Option<&'static str> {
        match self {
            Language::Rust => Some("rust"),
            Language::Python => Some("python"),
            Language::JavaScript | Language::TypeScript => Some("javascript"),
            Language::Go => Some("go"),
            _ => None,
        }
    }

    /// Check if this language is supported for SAST.
    pub fn is_supported(&self) -> bool {
        self.tree_sitter_name().is_some()
    }
}

impl std::fmt::Display for Language {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Language::Rust => write!(f, "Rust"),
            Language::Python => write!(f, "Python"),
            Language::JavaScript => write!(f, "JavaScript"),
            Language::TypeScript => write!(f, "TypeScript"),
            Language::Go => write!(f, "Go"),
            Language::Java => write!(f, "Java"),
            Language::C => write!(f, "C"),
            Language::Cpp => write!(f, "C++"),
            Language::Ruby => write!(f, "Ruby"),
            Language::Unknown => write!(f, "Unknown"),
        }
    }
}

/// Scan target representing what to analyze.
#[derive(Debug, Clone)]
pub enum ScanTarget {
    /// Single repository by owner/name
    Repository(Repository),

    /// Local directory path
    LocalPath(PathBuf),

    /// GitHub organization (all repos)
    Organization(String),

    /// GitHub user (all repos)
    User(String),

    /// Search query
    Search(String),
}

impl ScanTarget {
    /// Parse a scan target from CLI input.
    pub fn parse(input: &str) -> Self {
        let input = input.trim();

        // Check if it's a local path
        let path = PathBuf::from(input);
        if path.exists() {
            return ScanTarget::LocalPath(path);
        }

        // Check if it's a URL or owner/name
        if let Some(repo) = Repository::from_url(input) {
            return ScanTarget::Repository(repo);
        }

        // Check for org: or user: prefix
        if let Some(org) = input.strip_prefix("org:") {
            return ScanTarget::Organization(org.to_string());
        }
        if let Some(user) = input.strip_prefix("user:") {
            return ScanTarget::User(user.to_string());
        }

        // Treat as search query
        ScanTarget::Search(input.to_string())
    }
}

================================================================================
END: src\models\repository.rs
================================================================================

================================================================================
FILE: src\models\vulnerability.rs
================================================================================
//! Vulnerability data models.

use serde::{Deserialize, Serialize};

/// Represents a known vulnerability from a database (OSV, CVE, etc.).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Vulnerability {
    /// Unique vulnerability ID (e.g., CVE-2024-1234, RUSTSEC-2024-0001)
    pub id: String,

    /// Alternative IDs (aliases)
    #[serde(default)]
    pub aliases: Vec<String>,

    /// Human-readable summary
    pub summary: String,

    /// Detailed description
    pub details: Option<String>,

    /// Severity rating
    pub severity: Severity,

    /// CVSS score (if available)
    pub cvss_score: Option<f64>,

    /// Affected package name
    pub package: String,

    /// Affected version ranges
    pub affected_versions: Vec<VersionRange>,

    /// Fixed version (if known)
    pub fixed_version: Option<String>,

    /// References (links to advisories, patches, etc.)
    #[serde(default)]
    pub references: Vec<Reference>,

    /// CWE IDs
    #[serde(default)]
    pub cwes: Vec<String>,

    /// Publication date
    pub published: Option<chrono::DateTime<chrono::Utc>>,

    /// Last modified date
    pub modified: Option<chrono::DateTime<chrono::Utc>>,

    /// Source database
    pub source: VulnerabilitySource,
}

/// Severity levels for vulnerabilities.
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum Severity {
    Unknown,
    None,
    Low,
    Medium,
    High,
    Critical,
}

impl Default for Severity {
    fn default() -> Self {
        Severity::Unknown
    }
}

impl Severity {
    /// Create severity from CVSS v3 score.
    pub fn from_cvss(score: f64) -> Self {
        match score {
            s if s >= 9.0 => Severity::Critical,
            s if s >= 7.0 => Severity::High,
            s if s >= 4.0 => Severity::Medium,
            s if s > 0.0 => Severity::Low,
            s if s == 0.0 => Severity::None,
            _ => Severity::Unknown,
        }
    }

    /// Get display color for terminal output.
    pub fn color(&self) -> &'static str {
        match self {
            Severity::Critical => "\x1b[91m", // Bright red
            Severity::High => "\x1b[31m",     // Red
            Severity::Medium => "\x1b[33m",   // Yellow
            Severity::Low => "\x1b[36m",      // Cyan
            Severity::None => "\x1b[32m",     // Green
            Severity::Unknown => "\x1b[37m",  // White
        }
    }
}

impl std::fmt::Display for Severity {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Severity::Unknown => write!(f, "UNKNOWN"),
            Severity::None => write!(f, "NONE"),
            Severity::Low => write!(f, "LOW"),
            Severity::Medium => write!(f, "MEDIUM"),
            Severity::High => write!(f, "HIGH"),
            Severity::Critical => write!(f, "CRITICAL"),
        }
    }
}

/// Version range specification.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VersionRange {
    /// Starting version (inclusive)
    pub introduced: Option<String>,

    /// Ending version (exclusive) - the fix version
    pub fixed: Option<String>,

    /// Last affected version
    pub last_affected: Option<String>,
}

impl VersionRange {
    /// Check if a version is within this range.
    pub fn contains(&self, version: &semver::Version) -> bool {
        let in_range = if let Some(ref introduced) = self.introduced {
            if let Ok(intro) = semver::Version::parse(introduced) {
                version >= &intro
            } else {
                true // Assume in range if we can't parse
            }
        } else {
            true // No lower bound
        };

        if !in_range {
            return false;
        }

        if let Some(ref fixed) = self.fixed {
            if let Ok(fix) = semver::Version::parse(fixed) {
                return version < &fix;
            }
        }

        if let Some(ref last) = self.last_affected {
            if let Ok(last_ver) = semver::Version::parse(last) {
                return version <= &last_ver;
            }
        }

        true
    }
}

/// Reference link for vulnerability.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Reference {
    /// Reference type
    pub ref_type: ReferenceType,

    /// URL
    pub url: String,
}

/// Types of vulnerability references.
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "SCREAMING_SNAKE_CASE")]
pub enum ReferenceType {
    Advisory,
    Article,
    Detection,
    Discussion,
    Report,
    Fix,
    Introduced,
    Package,
    Evidence,
    Web,
}

impl Default for ReferenceType {
    fn default() -> Self {
        ReferenceType::Web
    }
}

/// Source database for vulnerability data.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
#[serde(rename_all = "lowercase")]
pub enum VulnerabilitySource {
    Osv,
    RustSec,
    Nvd,
    GitHub,
    Snyk,
    DepsDevAdvisory,
    Other(String),
}

impl Default for VulnerabilitySource {
    fn default() -> Self {
        VulnerabilitySource::Other("unknown".to_string())
    }
}

/// Dependency information from Cargo.lock or similar.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Dependency {
    /// Package name
    pub name: String,

    /// Resolved version
    pub version: String,

    /// Package source (registry, git, path)
    pub source: Option<String>,

    /// Checksum/hash
    pub checksum: Option<String>,

    /// Direct or transitive dependency
    pub is_direct: bool,

    /// Dependencies of this package
    #[serde(default)]
    pub dependencies: Vec<String>,
}

impl Dependency {
    /// Parse the version as semver.
    pub fn semver_version(&self) -> Option<semver::Version> {
        semver::Version::parse(&self.version).ok()
    }
}

/// Provenance information for supply chain verification.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Provenance {
    /// SLSA level (0-4)
    pub slsa_level: u8,

    /// Whether the artifact has a valid signature
    pub signature_valid: bool,

    /// Builder identity (e.g., GitHub Actions workflow)
    pub builder: Option<String>,

    /// Source repository
    pub source_repo: Option<String>,

    /// Source commit
    pub source_commit: Option<String>,

    /// Build timestamp
    pub build_time: Option<chrono::DateTime<chrono::Utc>>,

    /// Verification errors
    #[serde(default)]
    pub errors: Vec<String>,
}

impl Default for Provenance {
    fn default() -> Self {
        Self {
            slsa_level: 0,
            signature_valid: false,
            builder: None,
            source_repo: None,
            source_commit: None,
            build_time: None,
            errors: Vec::new(),
        }
    }
}

================================================================================
END: src\models\vulnerability.rs
================================================================================

================================================================================
FILE: src\analyzer\mod.rs
================================================================================
//! Security analysis modules.

mod benchmark;
mod name_resolution;
mod queries;
mod sast;
mod sca;
mod secrets;
mod taint;

pub use benchmark::*;
pub use name_resolution::*;
pub use queries::*;
pub use sast::*;
pub use sca::*;
pub use secrets::*;
pub use taint::*;

================================================================================
END: src\analyzer\mod.rs
================================================================================

================================================================================
FILE: src\analyzer\queries.rs
================================================================================
//! Tree-sitter query definitions for security analysis.

use crate::models::{Language, Severity};

/// A security query pattern.
#[derive(Debug, Clone)]
pub struct SecurityQuery {
    /// Unique identifier for this query
    pub id: &'static str,

    /// Human-readable name
    pub name: &'static str,

    /// Description of what this query detects
    pub description: &'static str,

    /// Tree-sitter query string
    pub query: &'static str,

    /// Target language
    pub language: Language,

    /// Severity of findings from this query
    pub severity: Severity,

    /// CWE IDs related to this query
    pub cwes: &'static [&'static str],

    /// Remediation advice
    pub remediation: &'static str,
}

/// Get all security queries for Rust.
pub fn rust_queries() -> Vec<SecurityQuery> {
    vec![
        // Unsafe code detection
        SecurityQuery {
            id: "rust/unsafe-block",
            name: "Unsafe Block Usage",
            description: "Detects usage of unsafe blocks which bypass Rust's safety guarantees",
            query: r#"(unsafe_block) @unsafe_usage"#,
            language: Language::Rust,
            severity: Severity::Medium,
            cwes: &["CWE-119", "CWE-416"],
            remediation: "Review unsafe block for memory safety. Add SAFETY comment documenting invariants.",
        },
        SecurityQuery {
            id: "rust/unsafe-fn",
            name: "Unsafe Function Definition",
            description: "Detects unsafe function definitions which expose unsafe operations to callers",
            query: r#"
            (function_item
              (function_modifiers
                "unsafe" @unsafe_kw
              )
              name: (identifier) @fn_name
            ) @unsafe_fn
            "#,
            language: Language::Rust,
            severity: Severity::Medium,
            cwes: &["CWE-119"],
            remediation: "Ensure safety invariants are documented and callers are aware of requirements.",
        },
        SecurityQuery {
            id: "rust/unsafe-impl",
            name: "Unsafe Trait Implementation",
            description: "Detects unsafe trait implementations",
            query: r#"
            (impl_item
              "unsafe" @unsafe_kw
            ) @unsafe_impl
            "#,
            language: Language::Rust,
            severity: Severity::Medium,
            cwes: &["CWE-119"],
            remediation: "Verify that all trait safety requirements are upheld.",
        },
        SecurityQuery {
            id: "rust/raw-pointer-deref",
            name: "Raw Pointer Dereference",
            description: "Detects raw pointer dereferences which can cause undefined behavior",
            query: r#"
            (unary_expression
              "*" @op
              (identifier) @ptr
              (#match? @ptr "^(ptr|raw|p_)")
            ) @deref
            "#,
            language: Language::Rust,
            severity: Severity::High,
            cwes: &["CWE-119", "CWE-476"],
            remediation: "Ensure pointer validity before dereferencing. Consider using safe abstractions.",
        },
        SecurityQuery {
            id: "rust/transmute",
            name: "Transmute Usage",
            description: "Detects std::mem::transmute which can cause undefined behavior",
            query: r#"
            (call_expression
              function: (scoped_identifier
                path: (scoped_identifier
                  path: (identifier) @std (#eq? @std "std")
                  name: (identifier) @mem (#eq? @mem "mem")
                )
                name: (identifier) @transmute (#eq? @transmute "transmute")
              )
            ) @transmute_call
            "#,
            language: Language::Rust,
            severity: Severity::High,
            cwes: &["CWE-843"],
            remediation: "Avoid transmute when possible. Use safer alternatives like from_ne_bytes.",
        },
        SecurityQuery {
            id: "rust/unwrap-panic",
            name: "Unwrap/Expect in Library Code",
            description: "Detects .unwrap() and .expect() calls which can panic",
            query: r#"
            (call_expression
              function: (field_expression
                field: (field_identifier) @method
                (#match? @method "^(unwrap|expect)$")
              )
            ) @unwrap_call
            "#,
            language: Language::Rust,
            severity: Severity::Low,
            cwes: &["CWE-755"],
            remediation: "Use proper error handling with ? operator or match instead of unwrap.",
        },
        // Note: Format string vulnerabilities in Rust are rare because format! macros
        // require compile-time format strings. True format string injection would require
        // using format_args! with runtime strings, which is uncommon. The taint analyzer
        // handles data flow tracking for such cases. A simple pattern match here would
        // produce too many false positives on safe code like format!("Bearer {}", token).
        SecurityQuery {
            id: "rust/command-injection",
            name: "Potential Command Injection",
            description: "Detects Command::new with potential string interpolation",
            query: r#"
            (call_expression
              function: (scoped_identifier
                name: (identifier) @new (#eq? @new "new")
              )
              arguments: (arguments
                (call_expression
                  function: (field_expression
                    field: (field_identifier) @method
                    (#match? @method "^(format|to_string)$")
                  )
                )
              )
            ) @cmd_injection
            "#,
            language: Language::Rust,
            severity: Severity::High,
            cwes: &["CWE-78"],
            remediation: "Validate and sanitize command arguments. Use allowlists for permitted commands.",
        },
        SecurityQuery {
            id: "rust/sql-string",
            name: "SQL String Construction",
            description: "Detects potential SQL injection through string concatenation",
            query: r#"
            (call_expression
              function: (scoped_identifier
                name: (identifier) @method
                (#match? @method "^(query|execute)$")
              )
              arguments: (arguments
                (binary_expression
                  operator: "+"
                )
              )
            ) @sql_injection
            "#,
            language: Language::Rust,
            severity: Severity::Critical,
            cwes: &["CWE-89"],
            remediation: "Use parameterized queries instead of string concatenation.",
        },
        SecurityQuery {
            id: "rust/path-traversal",
            name: "Potential Path Traversal",
            description: "Detects file operations with potential path traversal",
            query: r#"
            (call_expression
              function: (scoped_identifier
                path: (scoped_identifier
                  name: (identifier) @fs (#eq? @fs "fs")
                )
                name: (identifier) @method
                (#match? @method "^(read|write|open|create|remove).*$")
              )
            ) @path_op
            "#,
            language: Language::Rust,
            severity: Severity::Medium,
            cwes: &["CWE-22"],
            remediation: "Validate paths and use canonicalize() to resolve symlinks.",
        },
        SecurityQuery {
            id: "rust/hardcoded-secret",
            name: "Hardcoded Secret Pattern",
            description: "Detects variables named like secrets with string literal values",
            query: r#"
            (let_declaration
              pattern: (identifier) @var_name
              value: (string_literal) @value
              (#match? @var_name "(?i)(api_key|apikey|secret|token|password|passwd|auth|credential)")
            ) @hardcoded_secret
            "#,
            language: Language::Rust,
            severity: Severity::High,
            cwes: &["CWE-798"],
            remediation: "Store secrets in environment variables or a secrets manager.",
        },
        SecurityQuery {
            id: "rust/weak-crypto",
            name: "Weak Cryptography",
            description: "Detects usage of weak cryptographic algorithms",
            query: r#"
            (use_declaration
              argument: (scoped_identifier) @import
              (#match? @import "(md5|sha1|rc4|des|md4)")
            ) @weak_crypto
            "#,
            language: Language::Rust,
            severity: Severity::High,
            cwes: &["CWE-327", "CWE-328"],
            remediation: "Use strong cryptography: SHA-256+, AES-GCM, ChaCha20-Poly1305.",
        },
        // NEW: Detect panic! in library code (availability DoS)
        SecurityQuery {
            id: "rust/panic-in-library",
            name: "Panic in Library Code",
            description: "Detects panic! macro calls in library code which can cause denial of service",
            query: r#"
            (macro_invocation
              macro: (identifier) @macro_name
              (#eq? @macro_name "panic")
            ) @panic_call
            "#,
            language: Language::Rust,
            severity: Severity::Medium,
            cwes: &["CWE-400", "CWE-755"],
            remediation: "Return Result::Err instead of panicking in library code. Panic should be reserved for unrecoverable errors.",
        },
        // NEW: Detect lazy_static usage (potential initialization races)
        SecurityQuery {
            id: "rust/lazy-static-race",
            name: "Lazy Static Initialization Race",
            description: "Detects lazy_static usage which may have initialization race conditions in multi-threaded contexts",
            query: r#"
            (macro_invocation
              macro: (identifier) @macro_name
              (#eq? @macro_name "lazy_static")
            ) @lazy_static_usage
            "#,
            language: Language::Rust,
            severity: Severity::Low,
            cwes: &["CWE-362", "CWE-362"],
            remediation: "Consider using std::sync::LazyLock (Rust 1.80+) or once_cell for safer lazy initialization.",
        },
        // NEW: Detect RefCell usage (runtime borrow violations)
        SecurityQuery {
            id: "rust/refcell-usage",
            name: "RefCell Dynamic Borrow",
            description: "Detects RefCell usage which can cause runtime panic on borrow violations",
            query: r#"
            (call_expression
              function: (scoped_identifier
                path: (identifier) @cell (#eq? @cell "RefCell")
                name: (identifier) @new (#eq? @new "new")
              )
            ) @refcell_usage
            "#,
            language: Language::Rust,
            severity: Severity::Low,
            cwes: &["CWE-400"],
            remediation: "Use RefCell carefully. Consider alternatives like Mutex or RwLock for thread-safe interior mutability.",
        },
        // NEW: Detect unbounded Vec::push in loops (memory DoS)
        SecurityQuery {
            id: "rust/unbounded-vec-push",
            name: "Unbounded Vec Growth in Loop",
            description: "Detects unbounded Vec::push operations in loops that can lead to memory exhaustion",
            query: r#"
            (for_expression
              body: (block
                (call_expression
                  function: (field_expression
                    field: (field_identifier) @push (#eq? @push "push")
                  ) @vec_push
                )
              )
            ) @unbounded_push
            "#,
            language: Language::Rust,
            severity: Severity::Medium,
            cwes: &["CWE-400", "CWE-770"],
            remediation: "Add bounds checking or capacity limits before pushing to vectors in loops.",
        },
    ]
}

/// Get all security queries for Python.
pub fn python_queries() -> Vec<SecurityQuery> {
    vec![
        SecurityQuery {
            id: "python/exec-eval",
            name: "Dangerous Exec/Eval",
            description: "Detects exec() and eval() which can execute arbitrary code",
            query: r#"
            (call
              function: (identifier) @fn_name
              (#match? @fn_name "^(exec|eval|compile)$")
            ) @dangerous_call
            "#,
            language: Language::Python,
            severity: Severity::Critical,
            cwes: &["CWE-94", "CWE-95"],
            remediation: "Avoid exec/eval. Use ast.literal_eval for safe literal parsing.",
        },
        SecurityQuery {
            id: "python/sql-injection",
            name: "SQL Injection",
            description: "Detects SQL queries built with string formatting",
            query: r#"
            (call
              function: (attribute
                attribute: (identifier) @method
                (#match? @method "^(execute|executemany)$")
              )
              arguments: (argument_list
                (binary_operator
                  operator: "%"
                )
              )
            ) @sql_injection
            "#,
            language: Language::Python,
            severity: Severity::Critical,
            cwes: &["CWE-89"],
            remediation: "Use parameterized queries with placeholders.",
        },
        SecurityQuery {
            id: "python/command-injection",
            name: "Command Injection",
            description: "Detects os.system and subprocess with shell=True",
            query: r#"
            (call
              function: (attribute
                object: (identifier) @module (#eq? @module "os")
                attribute: (identifier) @method (#eq? @method "system")
              )
            ) @cmd_injection
            "#,
            language: Language::Python,
            severity: Severity::Critical,
            cwes: &["CWE-78"],
            remediation: "Use subprocess with shell=False and list arguments.",
        },
        SecurityQuery {
            id: "python/pickle-load",
            name: "Insecure Deserialization",
            description: "Detects pickle.load which can execute arbitrary code",
            query: r#"
            (call
              function: (attribute
                object: (identifier) @module (#eq? @module "pickle")
                attribute: (identifier) @method (#match? @method "^(load|loads)$")
              )
            ) @pickle_load
            "#,
            language: Language::Python,
            severity: Severity::Critical,
            cwes: &["CWE-502"],
            remediation: "Use JSON or other safe serialization formats for untrusted data.",
        },
        SecurityQuery {
            id: "python/hardcoded-password",
            name: "Hardcoded Password",
            description: "Detects hardcoded passwords in assignments",
            query: r#"
            (assignment
              left: (identifier) @var
              right: (string) @value
              (#match? @var "(?i)(password|passwd|secret|token|api_key)")
            ) @hardcoded_pw
            "#,
            language: Language::Python,
            severity: Severity::High,
            cwes: &["CWE-798"],
            remediation: "Use environment variables or a secrets manager.",
        },
        SecurityQuery {
            id: "python/assert-security",
            name: "Assert Used for Security",
            description: "Detects assert statements that may be stripped in production",
            query: r#"
            (assert_statement
              (comparison_operator) @condition
            ) @assert
            "#,
            language: Language::Python,
            severity: Severity::Medium,
            cwes: &["CWE-617"],
            remediation: "Use proper if/raise for security checks, not assert.",
        },
        SecurityQuery {
            id: "python/debug-true",
            name: "Debug Mode Enabled",
            description: "Detects DEBUG=True which may leak sensitive information",
            query: r#"
            (assignment
              left: (identifier) @var (#eq? @var "DEBUG")
              right: (true) @value
            ) @debug_true
            "#,
            language: Language::Python,
            severity: Severity::Medium,
            cwes: &["CWE-489"],
            remediation: "Ensure DEBUG is False in production.",
        },
        SecurityQuery {
            id: "python/weak-crypto",
            name: "Weak Cryptography",
            description: "Detects usage of weak cryptographic algorithms",
            query: r#"
            (import_from_statement
              module_name: (dotted_name) @module
              (#match? @module "(md5|sha1|DES|RC4)")
            ) @weak_crypto
            "#,
            language: Language::Python,
            severity: Severity::High,
            cwes: &["CWE-327"],
            remediation: "Use strong algorithms: SHA-256+, AES-GCM.",
        },
        // NEW: Detect requests without timeout (connection DoS)
        SecurityQuery {
            id: "python/requests-no-timeout",
            name: "HTTP Request Without Timeout",
            description: "Detects requests library calls without timeout parameter which can cause connection DoS",
            query: r#"
            (call
              function: (attribute
                object: (identifier) @requests (#eq? @requests "requests")
                attribute: (identifier) @method
                (#match? @method "^(get|post|put|delete|patch|head|options|request)$")
              )
              arguments: (argument_list) @args
              (#not-match? @args "timeout")
            ) @no_timeout_request
            "#,
            language: Language::Python,
            severity: Severity::Medium,
            cwes: &["CWE-400"],
            remediation: "Always specify a timeout parameter for HTTP requests to prevent indefinite hanging.",
        },
        // NEW: Detect tempfile.mktemp race condition
        SecurityQuery {
            id: "python/tempfile-mktemp",
            name: "Insecure Temporary File Creation",
            description: "Detects tempfile.mktemp usage which is vulnerable to race conditions",
            query: r#"
            (call
              function: (attribute
                object: (identifier) @tempfile (#eq? @tempfile "tempfile")
                attribute: (identifier) @mktemp (#eq? @mktemp "mktemp")
              )
            ) @mktemp_race
            "#,
            language: Language::Python,
            severity: Severity::High,
            cwes: &["CWE-377", "CWE-362"],
            remediation: "Use tempfile.mkstemp() or NamedTemporaryFile instead of mktemp to avoid race conditions.",
        },
        // NEW: Detect logging format string injection
        SecurityQuery {
            id: "python/logging-format-injection",
            name: "Logging Format String Injection",
            description: "Detects logging calls that may be vulnerable to format string injection",
            query: r#"
            (call
              function: (attribute
                object: (identifier) @logger (#match? @logger "^(logging|logger|log)$")
                attribute: (identifier) @method
                (#match? @method "^(debug|info|warning|error|critical|exception)$")
              )
              arguments: (argument_list
                (string) @msg
                (#match? @msg "%")
              )
            ) @format_injection
            "#,
            language: Language::Python,
            severity: Severity::Medium,
            cwes: &["CWE-134"],
            remediation: "Use %-style formatting with a tuple or use f-strings (Python 3.6+) instead of passing user data to log messages.",
        },
        // NEW: Detect __import__ with user input
        SecurityQuery {
            id: "python/dynamic-import",
            name: "Dynamic Import with User Input",
            description: "Detects __import__ or importlib usage which can execute arbitrary code",
            query: r#"
            (call
              function: (identifier) @import
              (#eq? @import "__import__")
            ) @dynamic_import
            "#,
            language: Language::Python,
            severity: Severity::High,
            cwes: &["CWE-94", "CWE-95"],
            remediation: "Avoid dynamic imports with user-controlled input. Use a whitelist of allowed modules if necessary.",
        },
    ]
}

/// Get all security queries for JavaScript.
pub fn javascript_queries() -> Vec<SecurityQuery> {
    vec![
        SecurityQuery {
            id: "js/eval",
            name: "Eval Usage",
            description: "Detects eval() which can execute arbitrary code",
            query: r#"
            (call_expression
              function: (identifier) @fn_name (#eq? @fn_name "eval")
            ) @eval_call
            "#,
            language: Language::JavaScript,
            severity: Severity::Critical,
            cwes: &["CWE-94", "CWE-95"],
            remediation: "Avoid eval(). Use JSON.parse for data, proper functions for logic.",
        },
        SecurityQuery {
            id: "js/innerhtml",
            name: "innerHTML Assignment",
            description: "Detects innerHTML which can lead to XSS",
            query: r#"
            (assignment_expression
              left: (member_expression
                property: (property_identifier) @prop (#eq? @prop "innerHTML")
              )
            ) @innerhtml
            "#,
            language: Language::JavaScript,
            severity: Severity::High,
            cwes: &["CWE-79"],
            remediation: "Use textContent or DOM methods instead of innerHTML.",
        },
        SecurityQuery {
            id: "js/document-write",
            name: "document.write Usage",
            description: "Detects document.write which can lead to XSS",
            query: r#"
            (call_expression
              function: (member_expression
                object: (identifier) @obj (#eq? @obj "document")
                property: (property_identifier) @prop (#eq? @prop "write")
              )
            ) @doc_write
            "#,
            language: Language::JavaScript,
            severity: Severity::High,
            cwes: &["CWE-79"],
            remediation: "Use DOM manipulation methods instead of document.write.",
        },
        SecurityQuery {
            id: "js/hardcoded-secret",
            name: "Hardcoded Secret",
            description: "Detects hardcoded secrets in variable declarations",
            query: r#"
            (variable_declarator
              name: (identifier) @var
              value: (string) @value
              (#match? @var "(?i)(api_key|apikey|secret|token|password|auth)")
            ) @hardcoded
            "#,
            language: Language::JavaScript,
            severity: Severity::High,
            cwes: &["CWE-798"],
            remediation: "Use environment variables for secrets.",
        },
        SecurityQuery {
            id: "js/sql-concat",
            name: "SQL String Concatenation",
            description: "Detects SQL queries built with string concatenation",
            query: r#"
            (call_expression
              function: (member_expression
                property: (property_identifier) @method (#match? @method "^(query|execute)$")
              )
              arguments: (arguments
                (binary_expression
                  operator: "+"
                  left: (string) @sql (#match? @sql "(?i)(SELECT|INSERT|UPDATE|DELETE)")
                )
              )
            ) @sql_injection
            "#,
            language: Language::JavaScript,
            severity: Severity::Critical,
            cwes: &["CWE-89"],
            remediation: "Use parameterized queries.",
        },
        SecurityQuery {
            id: "js/shell-exec",
            name: "Shell Command Execution",
            description: "Detects child_process exec with potential command injection",
            query: r#"
            (call_expression
              function: (identifier) @fn (#match? @fn "^(exec|execSync)$")
            ) @shell_exec
            "#,
            language: Language::JavaScript,
            severity: Severity::High,
            cwes: &["CWE-78"],
            remediation: "Use execFile with array arguments instead of exec.",
        },
        SecurityQuery {
            id: "js/regex-dos",
            name: "ReDoS Vulnerable Regex",
            description: "Detects regexes with nested quantifiers that may cause ReDoS",
            query: r#"
            (regex
              pattern: (regex_pattern) @pattern
              (#match? @pattern "\\(.+\\+\\).+\\+|\\(.+\\*\\).+\\*")
            ) @redos
            "#,
            language: Language::JavaScript,
            severity: Severity::Medium,
            cwes: &["CWE-1333"],
            remediation: "Simplify regex or use a timeout. Avoid nested quantifiers.",
        },
        SecurityQuery {
            id: "js/no-csrf",
            name: "Missing CSRF Protection",
            description: "Detects form submissions without CSRF tokens",
            query: r#"
            (call_expression
              function: (member_expression
                property: (property_identifier) @method (#match? @method "^(post|put|delete)$")
              )
            ) @http_call
            "#,
            language: Language::JavaScript,
            severity: Severity::Medium,
            cwes: &["CWE-352"],
            remediation: "Include CSRF tokens in state-changing requests.",
        },
        // NEW: Detect prototype pollution via Object.assign
        SecurityQuery {
            id: "js/prototype-pollution",
            name: "Prototype Pollution Risk",
            description: "Detects Object.assign or spread operator that may allow prototype pollution",
            query: r#"
            (call_expression
              function: (member_expression
                object: (identifier) @obj (#eq? @obj "Object")
                property: (property_identifier) @method
                (#eq? @method "assign")
              )
              arguments: (arguments
                (identifier) @target (#match? @target "^(target|dst|dest|destination)$")
              )
            ) @prototype_pollution
            "#,
            language: Language::JavaScript,
            severity: Severity::High,
            cwes: &["CWE-915", "CWE-1321"],
            remediation: "Use Object.assign with an empty object as target, or use structuredClone. Validate property keys to block __proto__ and constructor.",
        },
        // NEW: Detect dynamic require with user-controlled paths
        SecurityQuery {
            id: "js/dynamic-require",
            name: "Dynamic Module Loading",
            description: "Detects require() with dynamic or potentially user-controlled paths",
            query: r#"
            (call_expression
              function: (identifier) @require (#eq? @require "require")
              arguments: (arguments
                [
                  (identifier) @dynamic
                  (template_string) @template
                  (binary_expression) @concat
                ]
              )
            ) @dynamic_require
            "#,
            language: Language::JavaScript,
            severity: Severity::Medium,
            cwes: &["CWE-94", "CWE-95"],
            remediation: "Avoid dynamic require paths. Use a whitelist of allowed modules if dynamic loading is necessary.",
        },
        // NEW: Detect missing Object.freeze on exported constants
        SecurityQuery {
            id: "js/mutable-exports",
            name: "Mutable Exported Constants",
            description: "Detects exported objects/arrays that are not frozen, allowing external mutation",
            query: r#"
            (export_statement
              (lexical_declaration
                (variable_declarator
                  name: (identifier) @const_name
                  value: [
                    (object) @obj_value
                    (array) @arr_value
                  ]
                )
              )
            ) @mutable_export
            "#,
            language: Language::JavaScript,
            severity: Severity::Low,
            cwes: &["CWE-665"],
            remediation: "Use Object.freeze() on exported constants to prevent external mutation: export const CONFIG = Object.freeze({...}).",
        },
        // NEW: Detect JSON.parse without try-catch
        SecurityQuery {
            id: "js/unhandled-json-parse",
            name: "Unhandled JSON Parsing",
            description: "Detects JSON.parse calls that may throw on invalid input",
            query: r#"
            (call_expression
              function: (member_expression
                object: (identifier) @json (#eq? @json "JSON")
                property: (property_identifier) @parse (#eq? @parse "parse")
              )
            ) @json_parse
            "#,
            language: Language::JavaScript,
            severity: Severity::Low,
            cwes: &["CWE-755", "CWE-248"],
            remediation: "Wrap JSON.parse in a try-catch block to handle malformed JSON gracefully.",
        },
    ]
}

/// Get all security queries for Go.
pub fn go_queries() -> Vec<SecurityQuery> {
    vec![
        SecurityQuery {
            id: "go/sql-injection",
            name: "SQL Injection",
            description: "Detects SQL queries built with string concatenation",
            query: r#"
            (call_expression
              function: (selector_expression
                field: (field_identifier) @method
                (#match? @method "^(Query|Exec|QueryRow)$")
              )
              arguments: (argument_list
                (binary_expression
                  operator: "+"
                )
              )
            ) @sql_injection
            "#,
            language: Language::Go,
            severity: Severity::Critical,
            cwes: &["CWE-89"],
            remediation: "Use prepared statements with placeholders.",
        },
        SecurityQuery {
            id: "go/command-injection",
            name: "Command Injection",
            description: "Detects os/exec with potential command injection",
            query: r#"
            (call_expression
              function: (selector_expression
                operand: (identifier) @pkg (#eq? @pkg "exec")
                field: (field_identifier) @method (#eq? @method "Command")
              )
            ) @cmd_exec
            "#,
            language: Language::Go,
            severity: Severity::High,
            cwes: &["CWE-78"],
            remediation: "Validate command arguments. Use allowlists for commands.",
        },
        SecurityQuery {
            id: "go/hardcoded-cred",
            name: "Hardcoded Credential",
            description: "Detects hardcoded credentials in variable declarations",
            query: r#"
            (short_var_declaration
              left: (expression_list
                (identifier) @var
                (#match? @var "(?i)(password|secret|token|apikey|api_key)")
              )
              right: (expression_list
                (interpreted_string_literal) @value
              )
            ) @hardcoded
            "#,
            language: Language::Go,
            severity: Severity::High,
            cwes: &["CWE-798"],
            remediation: "Use environment variables or a secrets manager.",
        },
        SecurityQuery {
            id: "go/tls-insecure",
            name: "Insecure TLS Configuration",
            description: "Detects InsecureSkipVerify which disables TLS certificate validation",
            query: r#"
            (keyed_element
              (field_identifier) @field (#eq? @field "InsecureSkipVerify")
              (true) @value
            ) @insecure_tls
            "#,
            language: Language::Go,
            severity: Severity::High,
            cwes: &["CWE-295"],
            remediation: "Enable proper TLS certificate validation.",
        },
        SecurityQuery {
            id: "go/weak-random",
            name: "Weak Random Number Generator",
            description: "Detects math/rand usage which is not cryptographically secure",
            query: r#"
            (call_expression
              function: (selector_expression
                operand: (identifier) @pkg (#eq? @pkg "rand")
                field: (field_identifier) @method
                (#match? @method "^(Int|Intn|Float|Read)$")
              )
            ) @weak_rand
            "#,
            language: Language::Go,
            severity: Severity::Medium,
            cwes: &["CWE-338"],
            remediation: "Use crypto/rand for security-sensitive random numbers.",
        },
        SecurityQuery {
            id: "go/path-traversal",
            name: "Path Traversal",
            description: "Detects file operations with potential path traversal",
            query: r#"
            (call_expression
              function: (selector_expression
                operand: (identifier) @pkg (#match? @pkg "^(os|ioutil)$")
                field: (field_identifier) @method
                (#match? @method "^(Open|Create|ReadFile|WriteFile)$")
              )
            ) @path_op
            "#,
            language: Language::Go,
            severity: Severity::Medium,
            cwes: &["CWE-22"],
            remediation: "Validate and sanitize file paths. Use filepath.Clean.",
        },
        SecurityQuery {
            id: "go/unhandled-error",
            name: "Unhandled Error",
            description: "Detects ignored error return values",
            query: r#"
            (assignment_statement
              left: (expression_list
                (identifier) @blank (#eq? @blank "_")
              )
              right: (expression_list
                (call_expression)
              )
            ) @ignored_error
            "#,
            language: Language::Go,
            severity: Severity::Low,
            cwes: &["CWE-755"],
            remediation: "Handle errors appropriately instead of ignoring them.",
        },
        // NEW: Detect defer in loops (resource exhaustion)
        SecurityQuery {
            id: "go/defer-in-loop",
            name: "Defer Inside Loop",
            description: "Detects defer statements inside loops that can cause resource exhaustion",
            query: r#"
            (for_statement
              body: (block
                (defer_statement) @defer
              )
            ) @defer_in_loop
            "#,
            language: Language::Go,
            severity: Severity::Medium,
            cwes: &["CWE-400", "CWE-770"],
            remediation: "Move defer outside the loop or use an anonymous function to execute and release resources immediately.",
        },
        // NEW: Detect sync.Map type assertions
        SecurityQuery {
            id: "go/sync-map-assertion",
            name: "Sync.Map Type Assertion Panic",
            description: "Detects type assertions on sync.Map values that can panic at runtime",
            query: r#"
            (call_expression
              function: (selector_expression
                operand: (call_expression
                  function: (selector_expression
                    operand: (identifier) @sm (#eq? @sm "sync")
                    field: (field_identifier) @map (#eq? @map "Map")
                  )
                )
                field: (field_identifier) @load (#eq? @load "Load")
              )
            ) @sync_map_load
            "#,
            language: Language::Go,
            severity: Severity::Low,
            cwes: &["CWE-400", "CWE-754"],
            remediation: "Use the two-value form of type assertion (value, ok := ...) to handle type mismatches gracefully.",
        },
        // NEW: Detect unbuffered channel operations that may deadlock
        SecurityQuery {
            id: "go/unbuffered-channel",
            name: "Potential Channel Deadlock",
            description: "Detects unbuffered channel operations within the same function that may cause deadlock",
            query: r#"
            (call_expression
              function: (identifier) @make (#eq? @make "make")
              arguments: (argument_list
                (channel_type)
              )
            ) @unbuffered_channel
            "#,
            language: Language::Go,
            severity: Severity::Low,
            cwes: &["CWE-833", "CWE-662"],
            remediation: "Use buffered channels or ensure proper goroutine coordination to prevent deadlocks.",
        },
        // NEW: Detect context.Background without timeout
        SecurityQuery {
            id: "go/context-no-timeout",
            name: "Context Without Timeout",
            description: "Detects context.Background() usage which lacks cancellation/timeout",
            query: r#"
            (call_expression
              function: (selector_expression
                operand: (identifier) @ctx (#eq? @ctx "context")
                field: (field_identifier) @bg (#eq? @bg "Background")
              )
            ) @context_background
            "#,
            language: Language::Go,
            severity: Severity::Low,
            cwes: &["CWE-400"],
            remediation: "Use context.WithTimeout() or context.WithDeadline() instead of context.Background() for operations that should not run indefinitely.",
        },
    ]
}

/// Get all queries for a specific language.
pub fn get_queries_for_language(language: Language) -> Vec<SecurityQuery> {
    match language {
        Language::Rust => rust_queries(),
        Language::Python => python_queries(),
        Language::JavaScript | Language::TypeScript => javascript_queries(),
        Language::Go => go_queries(),
        _ => Vec::new(),
    }
}

/// Get all security queries across all languages.
pub fn all_queries() -> Vec<SecurityQuery> {
    let mut queries = Vec::new();
    queries.extend(rust_queries());
    queries.extend(python_queries());
    queries.extend(javascript_queries());
    queries.extend(go_queries());
    queries
}

================================================================================
END: src\analyzer\queries.rs
================================================================================

================================================================================
FILE: src\analyzer\sast.rs
================================================================================
//! Static Application Security Testing (SAST) engine using tree-sitter.

use crate::analyzer::queries::{get_queries_for_language, SecurityQuery};
use crate::analyzer::taint::{analyze_taint, SinkCategory, TaintAnalysisResult};
use crate::config::AnalysisConfig;
use crate::error::{AuditorError, Result};
use crate::models::{
    CodeSnippet, Confidence, Finding, Language, Location, SastCategory, Severity, SourceFile,
};
use rayon::prelude::*;
use std::cell::RefCell;
use std::collections::HashMap;
use std::path::Path;
use tracing::{debug, info, warn};
use streaming_iterator::StreamingIterator;
use tree_sitter::{Parser, Query, QueryCursor};

// Thread-local parser cache to avoid recreating parsers for each file
// while maintaining thread safety for Rayon parallel iteration.
thread_local! {
    static PARSER_CACHE: RefCell<HashMap<Language, Parser>> = RefCell::new(HashMap::new());
}

/// SAST engine for analyzing source code.
pub struct SastEngine {
    /// Analysis configuration
    config: AnalysisConfig,

    /// Parsers for each language
    parsers: HashMap<Language, Parser>,

    /// Whether to use taint analysis for FP reduction
    enable_taint_analysis: bool,
}

impl SastEngine {
    /// Create a new SAST engine.
    pub fn new(config: AnalysisConfig) -> Result<Self> {
        let mut parsers = HashMap::new();

        // Initialize parsers for supported languages
        if config.languages.iter().any(|l| l == "rust") {
            if let Ok(parser) = Self::create_parser(Language::Rust) {
                parsers.insert(Language::Rust, parser);
            }
        }

        if config.languages.iter().any(|l| l == "python") {
            if let Ok(parser) = Self::create_parser(Language::Python) {
                parsers.insert(Language::Python, parser);
            }
        }

        if config.languages.iter().any(|l| l == "javascript" || l == "typescript") {
            if let Ok(parser) = Self::create_parser(Language::JavaScript) {
                parsers.insert(Language::JavaScript, parser);
            }
        }

        if config.languages.iter().any(|l| l == "go") {
            if let Ok(parser) = Self::create_parser(Language::Go) {
                parsers.insert(Language::Go, parser);
            }
        }

        info!("SAST engine initialized with {} language parsers", parsers.len());

        Ok(Self {
            config,
            parsers,
            enable_taint_analysis: true, // Enable by default for FP reduction
        })
    }

    /// Enable or disable taint analysis for false positive reduction.
    pub fn with_taint_analysis(mut self, enable: bool) -> Self {
        self.enable_taint_analysis = enable;
        self
    }

    /// Create a parser for a specific language.
    fn create_parser(language: Language) -> Result<Parser> {
        let mut parser = Parser::new();

        let ts_language = match language {
            Language::Rust => tree_sitter_rust::LANGUAGE,
            Language::Python => tree_sitter_python::LANGUAGE,
            Language::JavaScript | Language::TypeScript => tree_sitter_javascript::LANGUAGE,
            Language::Go => tree_sitter_go::LANGUAGE,
            _ => {
                return Err(AuditorError::Parse(format!(
                    "Unsupported language: {:?}",
                    language
                )))
            }
        };

        parser
            .set_language(&ts_language.into())
            .map_err(|e| AuditorError::Parse(format!("Failed to set language: {}", e)))?;

        Ok(parser)
    }

    /// Get a parser from thread-local cache, creating one if needed.
    ///
    /// This allows parser reuse within a thread while maintaining
    /// thread safety for parallel analysis.
    fn get_cached_parser(language: Language) -> Result<()> {
        PARSER_CACHE.with(|cache| {
            let mut cache = cache.borrow_mut();
            if !cache.contains_key(&language) {
                let parser = Self::create_parser(language)?;
                cache.insert(language, parser);
            }
            Ok(())
        })
    }

    /// Parse content using a thread-local cached parser.
    fn parse_with_cache(language: Language, content: &str) -> Result<tree_sitter::Tree> {
        Self::get_cached_parser(language)?;

        PARSER_CACHE.with(|cache| {
            let mut cache = cache.borrow_mut();
            let parser = cache.get_mut(&language).ok_or_else(|| {
                AuditorError::Parse(format!("Parser not in cache for {:?}", language))
            })?;

            parser.parse(content, None).ok_or_else(|| {
                AuditorError::Parse("Failed to parse file".to_string())
            })
        })
    }

    /// Analyze a single file.
    pub fn analyze_file(&self, file: &mut SourceFile) -> Result<Vec<Finding>> {
        // Get language and path before borrowing for content
        let language = file.language;
        let file_path = file.path.clone();

        if !language.is_supported() {
            debug!("Skipping unsupported language: {:?}", language);
            return Ok(Vec::new());
        }

        // Load content if not already loaded
        let content = file.load_content()?.to_string();

        // Parse the file using thread-local cached parser
        let tree = Self::parse_with_cache(language, &content)?;

        // Run taint analysis if enabled (for FP reduction)
        let taint_result = if self.enable_taint_analysis {
            match analyze_taint(language, &tree, &content) {
                Ok(result) => {
                    debug!(
                        "Taint analysis for {}: {} sources, {} sinks, {} flows",
                        file_path.display(),
                        result.source_count,
                        result.sink_count,
                        result.flows.len()
                    );
                    Some(result)
                }
                Err(e) => {
                    warn!("Taint analysis failed for {}: {}", file_path.display(), e);
                    None
                }
            }
        } else {
            None
        };

        // Get queries for this language
        let queries = get_queries_for_language(language);
        let mut findings = Vec::new();

        // Run each security query
        for security_query in queries {
            match self.run_query(&security_query, &tree, &content, &file_path) {
                Ok(query_findings) => {
                    // Filter findings using taint analysis results
                    let filtered = if let Some(ref taint) = taint_result {
                        self.filter_with_taint(query_findings, taint, &security_query)
                    } else {
                        query_findings
                    };
                    findings.extend(filtered);
                }
                Err(e) => {
                    warn!(
                        "Query {} failed on {}: {}",
                        security_query.id,
                        file_path.display(),
                        e
                    );
                }
            }
        }

        // Add taint flow findings for confirmed data flow vulnerabilities
        if let Some(ref taint) = taint_result {
            for flow in &taint.flows {
                let location = Location::new(
                    file_path.clone(),
                    flow.sink_line,
                    1,
                )
                .with_language(language);

                let severity = match flow.sink.category {
                    SinkCategory::SqlQuery | SinkCategory::CommandExec | SinkCategory::CodeEval => {
                        Severity::Critical
                    }
                    SinkCategory::HtmlOutput | SinkCategory::FilePath => Severity::High,
                    SinkCategory::Deserialization | SinkCategory::LdapQuery | SinkCategory::XPathQuery => {
                        Severity::High
                    }
                    SinkCategory::LogOutput => Severity::Medium,
                };

                let category = match flow.sink.category {
                    SinkCategory::SqlQuery => SastCategory::Injection,
                    SinkCategory::CommandExec => SastCategory::Injection,
                    SinkCategory::CodeEval => SastCategory::Injection,
                    SinkCategory::HtmlOutput => SastCategory::Xss,
                    SinkCategory::FilePath => SastCategory::PathTraversal,
                    _ => SastCategory::Other,
                };

                let mut finding = Finding::sast(
                    &format!("taint-flow-{:?}", flow.sink.category),
                    &format!("Taint flow to {:?} sink", flow.sink.category),
                    &format!(
                        "Untrusted data flows from {} (line {}) to {} (line {}) without sanitization",
                        flow.source.pattern, flow.source_line, flow.sink.pattern, flow.sink_line
                    ),
                    location,
                    severity,
                )
                .with_sast_category(category)
                .with_confidence(Confidence::High) // Taint-confirmed findings have high confidence
                .with_metadata("taint_source", serde_json::json!(flow.source.pattern))
                .with_metadata("taint_sink", serde_json::json!(flow.sink.pattern))
                .with_metadata("source_line", serde_json::json!(flow.source_line))
                .with_metadata("sink_line", serde_json::json!(flow.sink_line));

                findings.push(finding);
            }
        }

        debug!(
            "Found {} findings in {}",
            findings.len(),
            file_path.display()
        );
        Ok(findings)
    }

    /// Filter findings using taint analysis to reduce false positives.
    ///
    /// If a finding is about a potential injection but taint analysis
    /// shows no data flow from untrusted sources, reduce its confidence.
    fn filter_with_taint(
        &self,
        findings: Vec<Finding>,
        taint: &TaintAnalysisResult,
        query: &SecurityQuery,
    ) -> Vec<Finding> {
        findings
            .into_iter()
            .filter_map(|mut finding| {
                // Check if this is an injection-related finding
                let is_injection = query.id.contains("injection")
                    || query.id.contains("sql")
                    || query.id.contains("cmd")
                    || query.id.contains("eval")
                    || query.id.contains("xss");

                if is_injection {
                    // Check if taint analysis found any flows to this location
                    let line = finding.location.start_line;
                    let has_taint_flow = taint.flows.iter().any(|f| {
                        f.sink_line == line
                            || (f.sink_line.saturating_sub(2) <= line && line <= f.sink_line + 2)
                    });

                    if !has_taint_flow && taint.source_count > 0 {
                        // No taint flow detected but sources exist - lower confidence
                        finding = finding.with_confidence(Confidence::Low);
                        debug!(
                            "Lowered confidence for {} at line {} (no taint flow)",
                            query.id, line
                        );
                    } else if has_taint_flow {
                        // Taint flow confirmed - high confidence
                        finding = finding.with_confidence(Confidence::High);
                        debug!(
                            "Confirmed taint flow for {} at line {}",
                            query.id, line
                        );
                    }
                }

                Some(finding)
            })
            .collect()
    }

    /// Run a security query on a parsed tree.
    fn run_query(
        &self,
        security_query: &SecurityQuery,
        tree: &tree_sitter::Tree,
        source: &str,
        file_path: &Path,
    ) -> Result<Vec<Finding>> {
        let ts_language = match security_query.language {
            Language::Rust => tree_sitter_rust::LANGUAGE.into(),
            Language::Python => tree_sitter_python::LANGUAGE.into(),
            Language::JavaScript | Language::TypeScript => tree_sitter_javascript::LANGUAGE.into(),
            Language::Go => tree_sitter_go::LANGUAGE.into(),
            _ => return Ok(Vec::new()),
        };

        let query = Query::new(&ts_language, security_query.query)
            .map_err(|e| AuditorError::Parse(format!("Invalid query {}: {}", security_query.id, e)))?;

        let mut cursor = QueryCursor::new();
        let mut matches = cursor.matches(&query, tree.root_node(), source.as_bytes());

        let mut findings = Vec::new();

        while let Some(match_) = matches.next() {
            // Get the primary capture (usually the main node of interest)
            if let Some(capture) = match_.captures.first() {
                let node = capture.node;
                let start = node.start_position();
                let end = node.end_position();

                let location = Location::new(
                    file_path.to_path_buf(),
                    start.row + 1, // Convert to 1-indexed
                    start.column + 1,
                )
                .with_end(end.row + 1, end.column + 1)
                .with_language(security_query.language);

                let snippet = Some(CodeSnippet::from_content(
                    source,
                    start.row + 1,
                    3, // Default context lines
                ));

                let sast_category = categorize_finding(security_query.id);

                let mut finding = Finding::sast(
                    security_query.id,
                    security_query.name,
                    security_query.description,
                    location,
                    security_query.severity,
                )
                .with_sast_category(sast_category)
                .with_confidence(determine_confidence(security_query, &node, source))
                .with_remediation(security_query.remediation);

                if let Some(snippet) = snippet {
                    finding = finding.with_snippet(snippet);
                }

                // Add CWE metadata
                for cwe in security_query.cwes {
                    finding = finding.with_metadata("cwe", serde_json::json!(cwe));
                }

                findings.push(finding);
            }
        }

        Ok(findings)
    }

    /// Analyze multiple files in parallel with bounded parallelism.
    ///
    /// Files are processed in batches to control memory usage, which is
    /// important for large repositories with thousands of files.
    pub fn analyze_files(&self, files: &mut [SourceFile]) -> Result<Vec<Finding>> {
        info!("Analyzing {} files with SAST engine", files.len());

        let mut all_findings = Vec::new();

        // Process files in batches to control memory usage
        let batch_size = self.config.max_file_size.min(500); // Default to 500 if not configured
        let total_batches = (files.len() + batch_size - 1) / batch_size;

        for (batch_idx, batch) in files.chunks_mut(batch_size).enumerate() {
            debug!("Processing batch {}/{} ({} files)", batch_idx + 1, total_batches, batch.len());

            // Use rayon for parallel analysis within each batch
            let results: Vec<Result<Vec<Finding>>> = batch
                .par_iter_mut()
                .map(|file| self.analyze_file(file))
                .collect();

            // Collect findings from this batch
            for result in results {
                match result {
                    Ok(findings) => all_findings.extend(findings),
                    Err(e) => warn!("Analysis error: {}", e),
                }
            }
        }

        info!("SAST analysis complete. Found {} total findings", all_findings.len());
        Ok(all_findings)
    }

    /// Check if the SAST engine supports a language.
    pub fn supports_language(&self, language: Language) -> bool {
        self.parsers.contains_key(&language)
    }

    /// Get supported languages.
    pub fn supported_languages(&self) -> Vec<Language> {
        self.parsers.keys().copied().collect()
    }
}

/// Categorize a finding based on the query ID.
fn categorize_finding(query_id: &str) -> SastCategory {
    if query_id.contains("unsafe") {
        SastCategory::UnsafeCode
    } else if query_id.contains("injection") || query_id.contains("sql") || query_id.contains("cmd") {
        SastCategory::Injection
    } else if query_id.contains("xss") || query_id.contains("innerhtml") || query_id.contains("document-write") {
        SastCategory::Xss
    } else if query_id.contains("path") || query_id.contains("traversal") {
        SastCategory::PathTraversal
    } else if query_id.contains("crypto") || query_id.contains("weak") {
        SastCategory::Crypto
    } else if query_id.contains("auth") || query_id.contains("password") || query_id.contains("secret") {
        SastCategory::Auth
    } else if query_id.contains("race") || query_id.contains("toctou") || query_id.contains("mktemp") {
        SastCategory::RaceCondition
    } else if query_id.contains("dos") || query_id.contains("redos") || query_id.contains("timeout") {
        SastCategory::Dos
    } else if query_id.contains("memory") || query_id.contains("transmute") || query_id.contains("pointer") {
        SastCategory::MemorySafety
    } else if query_id.contains("panic") || query_id.contains("refcell") {
        SastCategory::MemorySafety
    } else if query_id.contains("unbounded") || query_id.contains("defer-in-loop") {
        SastCategory::UnboundedResource
    } else if query_id.contains("prototype") || query_id.contains("pollution") {
        SastCategory::PrototypePollution
    } else {
        SastCategory::Other
    }
}

/// Determine confidence level based on context.
fn determine_confidence(
    query: &SecurityQuery,
    node: &tree_sitter::Node,
    source: &str,
) -> Confidence {
    // Start with medium confidence
    let mut confidence = Confidence::Medium;

    // High severity patterns typically have high confidence
    if matches!(query.severity, Severity::Critical | Severity::High) {
        confidence = Confidence::High;
    }

    // Check for mitigating factors in surrounding context
    let node_text = node
        .utf8_text(source.as_bytes())
        .unwrap_or("");

    // Look for test-related patterns (lower confidence)
    if node_text.contains("test") || node_text.contains("mock") || node_text.contains("example") {
        confidence = Confidence::Low;
    }

    // Look for safety comments in Rust (higher confidence of false positive)
    if query.language == Language::Rust {
        // Check if there's a SAFETY comment before the node
        let start_byte = node.start_byte().saturating_sub(200);
        let end_byte = node.start_byte();
        // Find valid UTF-8 char boundaries to avoid panicking on multi-byte chars
        let start = source
            .char_indices()
            .map(|(i, _)| i)
            .take_while(|&i| i <= start_byte)
            .last()
            .unwrap_or(0);
        let end = source
            .char_indices()
            .map(|(i, _)| i)
            .take_while(|&i| i <= end_byte)
            .last()
            .unwrap_or(0);
        let before = &source[start..end];
        if before.contains("// SAFETY:") || before.contains("// safety:") {
            confidence = Confidence::Low;
        }
    }

    confidence
}

/// Helper extension trait for AnalysisConfig to avoid Option handling.
trait AnalysisConfigExt {
    fn include_snippets(&self) -> Option<bool>;
    fn snippet_lines(&self) -> Option<usize>;
}

impl AnalysisConfigExt for AnalysisConfig {
    fn include_snippets(&self) -> Option<bool> {
        Some(true) // Default to true
    }

    fn snippet_lines(&self) -> Option<usize> {
        Some(3) // Default context lines
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_rust_unsafe_detection() {
        let config = AnalysisConfig::default();
        let engine = SastEngine::new(config).unwrap();

        let code = r#"
fn main() {
    unsafe {
        let ptr = 0x1234 as *mut i32;
        *ptr = 42;
    }
}
"#;

        let mut file = SourceFile {
            path: "test.rs".into(),
            absolute_path: "test.rs".into(),
            language: Language::Rust,
            size: code.len() as u64,
            content: Some(code.to_string()),
        };

        let findings = engine.analyze_file(&mut file).unwrap();
        assert!(!findings.is_empty(), "Should detect unsafe block");
    }

    #[test]
    fn test_python_eval_detection() {
        let config = AnalysisConfig::default();
        let engine = SastEngine::new(config).unwrap();

        let code = r#"
user_input = input()
result = eval(user_input)
"#;

        let mut file = SourceFile {
            path: "test.py".into(),
            absolute_path: "test.py".into(),
            language: Language::Python,
            size: code.len() as u64,
            content: Some(code.to_string()),
        };

        let findings = engine.analyze_file(&mut file).unwrap();
        assert!(!findings.is_empty(), "Should detect eval usage");
    }
}

================================================================================
END: src\analyzer\sast.rs
================================================================================

================================================================================
FILE: src\analyzer\sca.rs
================================================================================
//! Software Composition Analysis (SCA) module with OSV integration.
//!
//! This module provides:
//! - PURL-based package identification (Package URL standard)
//! - OSV vulnerability database integration
//! - Support for multiple ecosystems (npm, PyPI, crates.io, Go)
//! - Ecosystem-specific version parsing (semver for npm/cargo, PEP 440 for PyPI)

use crate::error::{AuditorError, Result};
use crate::models::{Dependency, Finding, Reference, ReferenceType, Severity, VersionRange, Vulnerability, VulnerabilitySource};
use cargo_lock::Lockfile;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::Path;
use std::time::Duration;
use tracing::{debug, info, warn};

// ============================================================================
// Ecosystem-Specific Version Parsing
// ============================================================================

/// Supported package ecosystems for version parsing.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum Ecosystem {
    /// Rust crates (crates.io) - uses strict semver
    Cargo,
    /// Node.js packages (npm) - uses node-semver (more permissive)
    Npm,
    /// Python packages (PyPI) - uses PEP 440
    PyPI,
    /// Go modules - uses semantic versioning with v prefix
    Go,
    /// Maven packages - uses Maven version ordering
    Maven,
    /// Generic/unknown ecosystem - falls back to string comparison
    Unknown,
}

impl Ecosystem {
    /// Detect ecosystem from PURL type string.
    pub fn from_purl_type(purl_type: &str) -> Self {
        match purl_type.to_lowercase().as_str() {
            "cargo" => Ecosystem::Cargo,
            "npm" => Ecosystem::Npm,
            "pypi" => Ecosystem::PyPI,
            "golang" | "go" => Ecosystem::Go,
            "maven" => Ecosystem::Maven,
            _ => Ecosystem::Unknown,
        }
    }
}

/// Ecosystem-aware version parser that handles npm, PyPI, Cargo, and Go versioning schemes.
pub struct EcosystemVersionParser;

impl EcosystemVersionParser {
    /// Check if a version string satisfies a version range constraint.
    ///
    /// This method uses ecosystem-specific parsing:
    /// - npm: node-semver ranges (e.g., ">=1.0.0 <2.0.0", "^1.2.3", "~1.2.3")
    /// - PyPI: PEP 440 specifiers (e.g., ">=1.0,<2.0", "~=1.4.2", "==1.0.*")
    /// - Cargo: strict semver ranges
    /// - Go: semver with optional 'v' prefix
    pub fn version_in_range(
        ecosystem: Ecosystem,
        version: &str,
        introduced: Option<&str>,
        fixed: Option<&str>,
        last_affected: Option<&str>,
    ) -> bool {
        match ecosystem {
            Ecosystem::Npm => Self::npm_version_in_range(version, introduced, fixed, last_affected),
            Ecosystem::PyPI => Self::pypi_version_in_range(version, introduced, fixed, last_affected),
            Ecosystem::Cargo | Ecosystem::Go => {
                Self::semver_version_in_range(version, introduced, fixed, last_affected)
            }
            _ => Self::fallback_version_in_range(version, introduced, fixed, last_affected),
        }
    }

    /// npm version range check using node-semver.
    fn npm_version_in_range(
        version: &str,
        introduced: Option<&str>,
        fixed: Option<&str>,
        last_affected: Option<&str>,
    ) -> bool {
        // Parse the target version
        let target = match node_semver::Version::parse(version) {
            Ok(v) => v,
            Err(_) => return Self::fallback_version_in_range(version, introduced, fixed, last_affected),
        };

        // Check lower bound (introduced)
        if let Some(intro) = introduced {
            if let Ok(intro_ver) = node_semver::Version::parse(intro) {
                if target < intro_ver {
                    return false;
                }
            }
        }

        // Check upper bound (fixed - exclusive)
        if let Some(fix) = fixed {
            if let Ok(fix_ver) = node_semver::Version::parse(fix) {
                if target >= fix_ver {
                    return false;
                }
            }
            return true;
        }

        // Check last_affected (inclusive upper bound)
        if let Some(last) = last_affected {
            if let Ok(last_ver) = node_semver::Version::parse(last) {
                return target <= last_ver;
            }
        }

        true
    }

    /// npm range string matching (e.g., ">=1.0.0 <2.0.0").
    pub fn npm_satisfies(version: &str, range: &str) -> bool {
        let ver = match node_semver::Version::parse(version) {
            Ok(v) => v,
            Err(_) => return false,
        };

        match node_semver::Range::parse(range) {
            Ok(r) => r.satisfies(&ver),
            Err(_) => false,
        }
    }

    /// PyPI version range check using PEP 440.
    fn pypi_version_in_range(
        version: &str,
        introduced: Option<&str>,
        fixed: Option<&str>,
        last_affected: Option<&str>,
    ) -> bool {
        use pep440_rs::{Version as Pep440Version, VersionSpecifier};
        use std::str::FromStr;

        // Parse the target version
        let target = match Pep440Version::from_str(version) {
            Ok(v) => v,
            Err(_) => return Self::fallback_version_in_range(version, introduced, fixed, last_affected),
        };

        // Check lower bound (introduced)
        if let Some(intro) = introduced {
            if let Ok(intro_ver) = Pep440Version::from_str(intro) {
                if target < intro_ver {
                    return false;
                }
            }
        }

        // Check upper bound (fixed - exclusive)
        if let Some(fix) = fixed {
            if let Ok(fix_ver) = Pep440Version::from_str(fix) {
                if target >= fix_ver {
                    return false;
                }
            }
            return true;
        }

        // Check last_affected (inclusive upper bound)
        if let Some(last) = last_affected {
            if let Ok(last_ver) = Pep440Version::from_str(last) {
                return target <= last_ver;
            }
        }

        true
    }

    /// PyPI specifier matching (e.g., ">=1.0,<2.0", "~=1.4.2").
    pub fn pypi_satisfies(version: &str, specifier: &str) -> bool {
        use pep440_rs::{Version as Pep440Version, VersionSpecifiers};
        use std::str::FromStr;

        let ver = match Pep440Version::from_str(version) {
            Ok(v) => v,
            Err(_) => return false,
        };

        match VersionSpecifiers::from_str(specifier) {
            Ok(specs) => specs.contains(&ver),
            Err(_) => false,
        }
    }

    /// Cargo/Go version range check using strict semver.
    fn semver_version_in_range(
        version: &str,
        introduced: Option<&str>,
        fixed: Option<&str>,
        last_affected: Option<&str>,
    ) -> bool {
        // Strip 'v' prefix for Go versions
        let version = version.strip_prefix('v').unwrap_or(version);

        let target = match semver::Version::parse(version) {
            Ok(v) => v,
            Err(_) => return Self::fallback_version_in_range(version, introduced, fixed, last_affected),
        };

        // Check lower bound
        if let Some(intro) = introduced {
            let intro = intro.strip_prefix('v').unwrap_or(intro);
            if let Ok(intro_ver) = semver::Version::parse(intro) {
                if target < intro_ver {
                    return false;
                }
            }
        }

        // Check upper bound (fixed - exclusive)
        if let Some(fix) = fixed {
            let fix = fix.strip_prefix('v').unwrap_or(fix);
            if let Ok(fix_ver) = semver::Version::parse(fix) {
                if target >= fix_ver {
                    return false;
                }
            }
            return true;
        }

        // Check last_affected
        if let Some(last) = last_affected {
            let last = last.strip_prefix('v').unwrap_or(last);
            if let Ok(last_ver) = semver::Version::parse(last) {
                return target <= last_ver;
            }
        }

        true
    }

    /// Cargo version requirement matching (e.g., "^1.0", ">=1.0, <2.0").
    pub fn cargo_satisfies(version: &str, requirement: &str) -> bool {
        let ver = match semver::Version::parse(version) {
            Ok(v) => v,
            Err(_) => return false,
        };

        match semver::VersionReq::parse(requirement) {
            Ok(req) => req.matches(&ver),
            Err(_) => false,
        }
    }

    /// Fallback version comparison using lexicographic ordering with numeric awareness.
    fn fallback_version_in_range(
        version: &str,
        introduced: Option<&str>,
        fixed: Option<&str>,
        last_affected: Option<&str>,
    ) -> bool {
        let target_parts = Self::parse_version_parts(version);

        if let Some(intro) = introduced {
            let intro_parts = Self::parse_version_parts(intro);
            if Self::compare_version_parts(&target_parts, &intro_parts) == std::cmp::Ordering::Less {
                return false;
            }
        }

        if let Some(fix) = fixed {
            let fix_parts = Self::parse_version_parts(fix);
            if Self::compare_version_parts(&target_parts, &fix_parts) != std::cmp::Ordering::Less {
                return false;
            }
            return true;
        }

        if let Some(last) = last_affected {
            let last_parts = Self::parse_version_parts(last);
            return Self::compare_version_parts(&target_parts, &last_parts) != std::cmp::Ordering::Greater;
        }

        true
    }

    /// Parse version string into numeric and string parts for comparison.
    fn parse_version_parts(version: &str) -> Vec<VersionPart> {
        let mut parts = Vec::new();
        let mut current_num = String::new();
        let mut current_str = String::new();

        for c in version.chars() {
            if c.is_ascii_digit() {
                if !current_str.is_empty() {
                    parts.push(VersionPart::String(std::mem::take(&mut current_str)));
                }
                current_num.push(c);
            } else if c == '.' || c == '-' || c == '_' || c == '+' {
                if !current_num.is_empty() {
                    if let Ok(n) = current_num.parse::<u64>() {
                        parts.push(VersionPart::Numeric(n));
                    }
                    current_num.clear();
                }
                if !current_str.is_empty() {
                    parts.push(VersionPart::String(std::mem::take(&mut current_str)));
                }
            } else {
                if !current_num.is_empty() {
                    if let Ok(n) = current_num.parse::<u64>() {
                        parts.push(VersionPart::Numeric(n));
                    }
                    current_num.clear();
                }
                current_str.push(c);
            }
        }

        if !current_num.is_empty() {
            if let Ok(n) = current_num.parse::<u64>() {
                parts.push(VersionPart::Numeric(n));
            }
        }
        if !current_str.is_empty() {
            parts.push(VersionPart::String(current_str));
        }

        parts
    }

    /// Compare two version part lists.
    fn compare_version_parts(a: &[VersionPart], b: &[VersionPart]) -> std::cmp::Ordering {
        use std::cmp::Ordering;

        for (pa, pb) in a.iter().zip(b.iter()) {
            match (pa, pb) {
                (VersionPart::Numeric(na), VersionPart::Numeric(nb)) => {
                    match na.cmp(nb) {
                        Ordering::Equal => continue,
                        other => return other,
                    }
                }
                (VersionPart::String(sa), VersionPart::String(sb)) => {
                    match sa.cmp(sb) {
                        Ordering::Equal => continue,
                        other => return other,
                    }
                }
                (VersionPart::Numeric(_), VersionPart::String(_)) => return Ordering::Greater,
                (VersionPart::String(_), VersionPart::Numeric(_)) => return Ordering::Less,
            }
        }

        a.len().cmp(&b.len())
    }
}

/// A part of a version string (either numeric or string).
#[derive(Debug, Clone)]
enum VersionPart {
    Numeric(u64),
    String(String),
}

/// Extension trait for VersionRange to support ecosystem-specific parsing.
pub trait VersionRangeExt {
    /// Check if a version is in range using ecosystem-specific parsing.
    fn contains_version(&self, version: &str, ecosystem: Ecosystem) -> bool;
}

impl VersionRangeExt for VersionRange {
    fn contains_version(&self, version: &str, ecosystem: Ecosystem) -> bool {
        EcosystemVersionParser::version_in_range(
            ecosystem,
            version,
            self.introduced.as_deref(),
            self.fixed.as_deref(),
            self.last_affected.as_deref(),
        )
    }
}

/// Package URL (PURL) - A standardized format for identifying software packages.
///
/// Format: `pkg:type/namespace/name@version?qualifiers#subpath`
///
/// Examples:
/// - `pkg:cargo/serde@1.0.193`
/// - `pkg:npm/@types/node@20.10.0`
/// - `pkg:pypi/requests@2.31.0`
/// - `pkg:golang/github.com/gin-gonic/gin@1.9.1`
#[derive(Debug, Clone, PartialEq, Eq)]
pub struct PackageUrl {
    /// Package type (ecosystem): cargo, npm, pypi, golang, maven, etc.
    pub pkg_type: String,
    /// Namespace (optional): npm scope, Go module path prefix, Maven groupId
    pub namespace: Option<String>,
    /// Package name
    pub name: String,
    /// Package version (optional)
    pub version: Option<String>,
    /// Qualifiers (optional): key=value pairs
    pub qualifiers: HashMap<String, String>,
    /// Subpath (optional): path within the package
    pub subpath: Option<String>,
}

impl PackageUrl {
    /// Create a new PURL with required fields.
    pub fn new(pkg_type: impl Into<String>, name: impl Into<String>) -> Self {
        Self {
            pkg_type: pkg_type.into(),
            namespace: None,
            name: name.into(),
            version: None,
            qualifiers: HashMap::new(),
            subpath: None,
        }
    }

    /// Set the namespace.
    pub fn with_namespace(mut self, namespace: impl Into<String>) -> Self {
        self.namespace = Some(namespace.into());
        self
    }

    /// Set the version.
    pub fn with_version(mut self, version: impl Into<String>) -> Self {
        self.version = Some(version.into());
        self
    }

    /// Add a qualifier.
    pub fn with_qualifier(mut self, key: impl Into<String>, value: impl Into<String>) -> Self {
        self.qualifiers.insert(key.into(), value.into());
        self
    }

    /// Set the subpath.
    pub fn with_subpath(mut self, subpath: impl Into<String>) -> Self {
        self.subpath = Some(subpath.into());
        self
    }

    /// Parse a PURL string.
    pub fn parse(purl: &str) -> Result<Self> {
        // Must start with "pkg:"
        if !purl.starts_with("pkg:") {
            return Err(AuditorError::Parse("PURL must start with 'pkg:'".to_string()));
        }

        let rest = &purl[4..];

        // Split off subpath first (after #)
        let (rest, subpath) = if let Some(hash_pos) = rest.find('#') {
            let subpath = Self::decode(&rest[hash_pos + 1..]);
            (&rest[..hash_pos], Some(subpath))
        } else {
            (rest, None)
        };

        // Split off qualifiers (after ?)
        let (rest, qualifiers) = if let Some(q_pos) = rest.find('?') {
            let quals = Self::parse_qualifiers(&rest[q_pos + 1..]);
            (&rest[..q_pos], quals)
        } else {
            (rest, HashMap::new())
        };

        // Split off version (after @)
        let (rest, version) = if let Some(at_pos) = rest.rfind('@') {
            let version = Self::decode(&rest[at_pos + 1..]);
            (&rest[..at_pos], Some(version))
        } else {
            (rest, None)
        };

        // Parse type/namespace/name
        let parts: Vec<&str> = rest.splitn(2, '/').collect();
        if parts.is_empty() {
            return Err(AuditorError::Parse("PURL missing type".to_string()));
        }

        let pkg_type = parts[0].to_lowercase();
        let name_part = parts.get(1).unwrap_or(&"");

        // Parse namespace and name based on type
        let (namespace, name) = Self::parse_name_with_namespace(&pkg_type, name_part)?;

        Ok(Self {
            pkg_type,
            namespace,
            name,
            version,
            qualifiers,
            subpath,
        })
    }

    /// Parse namespace and name based on package type conventions.
    fn parse_name_with_namespace(pkg_type: &str, name_part: &str) -> Result<(Option<String>, String)> {
        if name_part.is_empty() {
            return Err(AuditorError::Parse("PURL missing name".to_string()));
        }

        match pkg_type {
            // npm uses @scope/name format
            "npm" => {
                if name_part.starts_with('@') {
                    // @scope/name format
                    let parts: Vec<&str> = name_part.splitn(2, '/').collect();
                    if parts.len() == 2 {
                        Ok((Some(Self::decode(parts[0])), Self::decode(parts[1])))
                    } else {
                        Err(AuditorError::Parse("Invalid npm scoped package".to_string()))
                    }
                } else {
                    Ok((None, Self::decode(name_part)))
                }
            }
            // Go modules use the full import path as name
            "golang" => {
                // For Go, the full path is the name; namespace could be the host
                if let Some(slash_pos) = name_part.find('/') {
                    let host = &name_part[..slash_pos];
                    Ok((Some(Self::decode(host)), Self::decode(name_part)))
                } else {
                    Ok((None, Self::decode(name_part)))
                }
            }
            // Maven uses groupId/artifactId
            "maven" => {
                let parts: Vec<&str> = name_part.splitn(2, '/').collect();
                if parts.len() == 2 {
                    Ok((Some(Self::decode(parts[0])), Self::decode(parts[1])))
                } else {
                    Err(AuditorError::Parse("Maven PURL requires groupId/artifactId".to_string()))
                }
            }
            // Default: no namespace
            _ => {
                if let Some(slash_pos) = name_part.rfind('/') {
                    let namespace = &name_part[..slash_pos];
                    let name = &name_part[slash_pos + 1..];
                    Ok((Some(Self::decode(namespace)), Self::decode(name)))
                } else {
                    Ok((None, Self::decode(name_part)))
                }
            }
        }
    }

    /// Parse qualifiers from a query string.
    fn parse_qualifiers(query: &str) -> HashMap<String, String> {
        query
            .split('&')
            .filter_map(|pair| {
                let mut parts = pair.splitn(2, '=');
                let key = parts.next()?;
                let value = parts.next()?;
                Some((Self::decode(key), Self::decode(value)))
            })
            .collect()
    }

    /// URL-decode a string.
    fn decode(s: &str) -> String {
        // Basic percent decoding
        let mut result = String::new();
        let mut chars = s.chars().peekable();

        while let Some(c) = chars.next() {
            if c == '%' {
                let hex: String = chars.by_ref().take(2).collect();
                if let Ok(byte) = u8::from_str_radix(&hex, 16) {
                    result.push(byte as char);
                } else {
                    result.push('%');
                    result.push_str(&hex);
                }
            } else {
                result.push(c);
            }
        }

        result
    }

    /// URL-encode a string for PURL.
    fn encode(s: &str) -> String {
        s.chars()
            .map(|c| match c {
                'A'..='Z' | 'a'..='z' | '0'..='9' | '-' | '.' | '_' | '~' => c.to_string(),
                _ => format!("%{:02X}", c as u8),
            })
            .collect()
    }

    /// Convert to a canonical PURL string.
    pub fn to_string(&self) -> String {
        let mut result = format!("pkg:{}/", self.pkg_type);

        // Add namespace if present
        if let Some(ref ns) = self.namespace {
            result.push_str(&Self::encode(ns));
            result.push('/');
        }

        // Add name
        result.push_str(&Self::encode(&self.name));

        // Add version
        if let Some(ref ver) = self.version {
            result.push('@');
            result.push_str(&Self::encode(ver));
        }

        // Add qualifiers
        if !self.qualifiers.is_empty() {
            result.push('?');
            let quals: Vec<String> = self
                .qualifiers
                .iter()
                .map(|(k, v)| format!("{}={}", Self::encode(k), Self::encode(v)))
                .collect();
            result.push_str(&quals.join("&"));
        }

        // Add subpath
        if let Some(ref subpath) = self.subpath {
            result.push('#');
            result.push_str(&Self::encode(subpath));
        }

        result
    }

    /// Get the OSV ecosystem name for this PURL type.
    pub fn osv_ecosystem(&self) -> &str {
        match self.pkg_type.as_str() {
            "cargo" => "crates.io",
            "npm" => "npm",
            "pypi" => "PyPI",
            "golang" => "Go",
            "maven" => "Maven",
            "nuget" => "NuGet",
            "gem" => "RubyGems",
            "composer" => "Packagist",
            "pub" => "Pub",
            "hex" => "Hex",
            "hackage" => "Hackage",
            "conan" => "ConanCenter",
            "swift" => "SwiftURL",
            _ => &self.pkg_type,
        }
    }

    /// Create a PURL from a Cargo.lock package.
    pub fn from_cargo(name: &str, version: &str) -> Self {
        Self::new("cargo", name).with_version(version)
    }

    /// Create a PURL from an npm package.
    pub fn from_npm(name: &str, version: &str) -> Self {
        if name.starts_with('@') {
            // Scoped package
            let parts: Vec<&str> = name.splitn(2, '/').collect();
            if parts.len() == 2 {
                return Self::new("npm", parts[1])
                    .with_namespace(parts[0])
                    .with_version(version);
            }
        }
        Self::new("npm", name).with_version(version)
    }

    /// Create a PURL from a PyPI package.
    pub fn from_pypi(name: &str, version: &str) -> Self {
        // PyPI normalizes names to lowercase with hyphens
        let normalized = name.to_lowercase().replace('_', "-");
        Self::new("pypi", normalized).with_version(version)
    }

    /// Create a PURL from a Go module.
    pub fn from_golang(module: &str, version: &str) -> Self {
        Self::new("golang", module).with_version(version)
    }
}

/// Default rate limit delay between OSV API calls (in milliseconds).
const DEFAULT_RATE_LIMIT_MS: u64 = 50;

/// SCA engine for analyzing dependencies.
pub struct ScaEngine {
    /// HTTP client for OSV API
    client: Client,

    /// OSV API base URL
    osv_url: String,

    /// Rate limit delay between API calls
    rate_limit_delay: Duration,
}

impl ScaEngine {
    /// Create a new SCA engine.
    pub fn new() -> Self {
        Self {
            client: Client::new(),
            osv_url: "https://api.osv.dev/v1".to_string(),
            rate_limit_delay: Duration::from_millis(DEFAULT_RATE_LIMIT_MS),
        }
    }

    /// Set the rate limit delay between API calls.
    pub fn with_rate_limit(mut self, delay_ms: u64) -> Self {
        self.rate_limit_delay = Duration::from_millis(delay_ms);
        self
    }

    /// Analyze dependencies from a Cargo.lock file.
    pub async fn analyze_cargo_lock(&self, lock_path: &Path) -> Result<Vec<Finding>> {
        info!("Analyzing Cargo.lock: {}", lock_path.display());

        let lockfile = Lockfile::load(lock_path)?;
        let dependencies = Self::parse_lockfile(&lockfile);

        info!("Found {} dependencies in lockfile", dependencies.len());

        let mut findings = Vec::new();

        for dep in &dependencies {
            // Apply rate limiting between API calls
            self.apply_rate_limit().await;

            match self.query_osv(&dep.name, &dep.version, "crates.io").await {
                Ok(vulns) => {
                    for vuln in vulns {
                        let finding = Finding::sca(vuln.clone(), &dep.name, &dep.version);
                        findings.push(finding);
                    }
                }
                Err(e) => {
                    warn!("Failed to query OSV for {}: {}", dep.name, e);
                }
            }
        }

        info!("SCA analysis complete. Found {} vulnerabilities", findings.len());
        Ok(findings)
    }

    /// Parse a Cargo lockfile into dependencies.
    fn parse_lockfile(lockfile: &Lockfile) -> Vec<Dependency> {
        lockfile
            .packages
            .iter()
            .map(|pkg| Dependency {
                name: pkg.name.as_str().to_string(),
                version: pkg.version.to_string(),
                source: pkg.source.as_ref().map(|s| s.to_string()),
                checksum: pkg.checksum.as_ref().map(|c| c.to_string()),
                is_direct: false, // Would need Cargo.toml to determine
                dependencies: pkg
                    .dependencies
                    .iter()
                    .map(|d| d.name.as_str().to_string())
                    .collect(),
            })
            .collect()
    }

    /// Apply rate limiting delay between API calls.
    async fn apply_rate_limit(&self) {
        if !self.rate_limit_delay.is_zero() {
            tokio::time::sleep(self.rate_limit_delay).await;
        }
    }

    /// Query OSV API for vulnerabilities using a PURL.
    pub async fn query_osv_purl(&self, purl: &PackageUrl) -> Result<Vec<Vulnerability>> {
        let ecosystem = purl.osv_ecosystem();
        let version = purl.version.as_deref().unwrap_or("");

        // For Go modules, use the full name; for others use just the name
        let name: String = if purl.pkg_type == "golang" {
            purl.name.clone()
        } else if let Some(ref ns) = purl.namespace {
            // For npm scoped packages, reconstruct the full name
            if purl.pkg_type == "npm" {
                format!("{}/{}", ns, purl.name)
            } else {
                purl.name.clone()
            }
        } else {
            purl.name.clone()
        };

        debug!("Querying OSV via PURL: {} -> {}@{} ({})", purl.to_string(), name, version, ecosystem);
        self.query_osv(&name, version, ecosystem).await
    }

    /// Query OSV API for vulnerabilities.
    pub async fn query_osv(
        &self,
        package: &str,
        version: &str,
        ecosystem: &str,
    ) -> Result<Vec<Vulnerability>> {
        debug!("Querying OSV for {}@{} ({})", package, version, ecosystem);

        let request = OsvQueryRequest {
            package: OsvPackage {
                name: package.to_string(),
                ecosystem: ecosystem.to_string(),
            },
            version: version.to_string(),
        };

        let response = self
            .client
            .post(format!("{}/query", self.osv_url))
            .json(&request)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(AuditorError::Osv(format!(
                "OSV API returned {}",
                response.status()
            )));
        }

        let osv_response: OsvQueryResponse = response.json().await?;

        let vulnerabilities = osv_response
            .vulns
            .into_iter()
            .filter_map(|v| self.convert_osv_vuln(v))
            .collect();

        Ok(vulnerabilities)
    }

    /// Query OSV for a specific vulnerability ID.
    pub async fn get_vulnerability(&self, vuln_id: &str) -> Result<Option<Vulnerability>> {
        debug!("Fetching vulnerability: {}", vuln_id);

        let response = self
            .client
            .get(format!("{}/vulns/{}", self.osv_url, vuln_id))
            .send()
            .await?;

        if response.status() == reqwest::StatusCode::NOT_FOUND {
            return Ok(None);
        }

        if !response.status().is_success() {
            return Err(AuditorError::Osv(format!(
                "OSV API returned {}",
                response.status()
            )));
        }

        let osv_vuln: OsvVulnerability = response.json().await?;
        Ok(self.convert_osv_vuln(osv_vuln))
    }

    /// Convert OSV vulnerability to our model.
    fn convert_osv_vuln(&self, osv: OsvVulnerability) -> Option<Vulnerability> {
        // Get severity from database_specific or severity field
        let severity = osv
            .severity
            .first()
            .and_then(|s| s.score.as_ref())
            .and_then(|score| score.parse::<f64>().ok())
            .map(Severity::from_cvss)
            .unwrap_or(Severity::Unknown);

        let cvss_score = osv
            .severity
            .first()
            .and_then(|s| s.score.as_ref())
            .and_then(|score| score.parse::<f64>().ok());

        // Get affected versions
        let affected_versions: Vec<VersionRange> = osv
            .affected
            .iter()
            .flat_map(|a| {
                a.ranges.iter().flat_map(|r| {
                    r.events.windows(2).filter_map(|w| {
                        let introduced = w.get(0).and_then(|e| e.introduced.clone());
                        let fixed = w.get(1).and_then(|e| e.fixed.clone());
                        if introduced.is_some() || fixed.is_some() {
                            Some(VersionRange {
                                introduced,
                                fixed,
                                last_affected: None,
                            })
                        } else {
                            None
                        }
                    })
                })
            })
            .collect();

        // Get fixed version
        let fixed_version = osv.affected.first().and_then(|a| {
            a.ranges.first().and_then(|r| {
                r.events.iter().find_map(|e| e.fixed.clone())
            })
        });

        // Get package name
        let package = osv
            .affected
            .first()
            .map(|a| a.package.name.clone())
            .unwrap_or_else(|| "unknown".to_string());

        // Convert references
        let references: Vec<Reference> = osv
            .references
            .iter()
            .map(|r| Reference {
                ref_type: match r.ref_type.as_str() {
                    "ADVISORY" => ReferenceType::Advisory,
                    "ARTICLE" => ReferenceType::Article,
                    "DETECTION" => ReferenceType::Detection,
                    "DISCUSSION" => ReferenceType::Discussion,
                    "REPORT" => ReferenceType::Report,
                    "FIX" => ReferenceType::Fix,
                    "INTRODUCED" => ReferenceType::Introduced,
                    "PACKAGE" => ReferenceType::Package,
                    "EVIDENCE" => ReferenceType::Evidence,
                    _ => ReferenceType::Web,
                },
                url: r.url.clone(),
            })
            .collect();

        // Get CWE IDs
        let cwes: Vec<String> = osv
            .database_specific
            .as_ref()
            .and_then(|d| d.get("cwes"))
            .and_then(|v| v.as_array())
            .map(|arr| {
                arr.iter()
                    .filter_map(|v| v.as_str().map(|s| s.to_string()))
                    .collect()
            })
            .unwrap_or_default();

        Some(Vulnerability {
            id: osv.id,
            aliases: osv.aliases,
            summary: osv.summary.unwrap_or_else(|| "No summary available".to_string()),
            details: osv.details,
            severity,
            cvss_score,
            package,
            affected_versions,
            fixed_version,
            references,
            cwes,
            published: osv.published.and_then(|s| chrono::DateTime::parse_from_rfc3339(&s).ok().map(|d| d.with_timezone(&chrono::Utc))),
            modified: osv.modified.and_then(|s| chrono::DateTime::parse_from_rfc3339(&s).ok().map(|d| d.with_timezone(&chrono::Utc))),
            source: VulnerabilitySource::Osv,
        })
    }

    /// Analyze a package.json file for npm dependencies.
    pub async fn analyze_package_json(&self, path: &Path) -> Result<Vec<Finding>> {
        info!("Analyzing package.json: {}", path.display());

        let content = tokio::fs::read_to_string(path).await?;
        let package: serde_json::Value = serde_json::from_str(&content)?;

        let mut findings = Vec::new();

        // Check dependencies
        if let Some(deps) = package.get("dependencies").and_then(|d| d.as_object()) {
            for (name, version) in deps {
                if let Some(version_str) = version.as_str() {
                    let clean_version = version_str.trim_start_matches(|c| c == '^' || c == '~' || c == '=' || c == '>' || c == '<');

                    self.apply_rate_limit().await;
                    match self.query_osv(name, clean_version, "npm").await {
                        Ok(vulns) => {
                            for vuln in vulns {
                                findings.push(Finding::sca(vuln, name, clean_version));
                            }
                        }
                        Err(e) => {
                            warn!("Failed to query OSV for npm/{}: {}", name, e);
                        }
                    }
                }
            }
        }

        // Check devDependencies
        if let Some(deps) = package.get("devDependencies").and_then(|d| d.as_object()) {
            for (name, version) in deps {
                if let Some(version_str) = version.as_str() {
                    let clean_version = version_str.trim_start_matches(|c| c == '^' || c == '~' || c == '=' || c == '>' || c == '<');

                    self.apply_rate_limit().await;
                    match self.query_osv(name, clean_version, "npm").await {
                        Ok(vulns) => {
                            for vuln in vulns {
                                findings.push(Finding::sca(vuln, name, clean_version));
                            }
                        }
                        Err(e) => {
                            debug!("Failed to query OSV for npm/{}: {}", name, e);
                        }
                    }
                }
            }
        }

        info!("npm SCA complete. Found {} vulnerabilities", findings.len());
        Ok(findings)
    }

    /// Analyze a requirements.txt file for Python dependencies.
    pub async fn analyze_requirements_txt(&self, path: &Path) -> Result<Vec<Finding>> {
        info!("Analyzing requirements.txt: {}", path.display());

        let content = tokio::fs::read_to_string(path).await?;
        let mut findings = Vec::new();

        for line in content.lines() {
            let line = line.trim();

            // Skip comments and empty lines
            if line.is_empty() || line.starts_with('#') {
                continue;
            }

            // Parse package==version or package>=version etc.
            let parts: Vec<&str> = line.split(|c| c == '=' || c == '>' || c == '<' || c == '~' || c == '!' || c == '[')
                .collect();

            if parts.len() >= 2 {
                let name = parts[0].trim();
                let version = parts.last().unwrap_or(&"").trim();

                if !name.is_empty() && !version.is_empty() {
                    self.apply_rate_limit().await;
                    match self.query_osv(name, version, "PyPI").await {
                        Ok(vulns) => {
                            for vuln in vulns {
                                findings.push(Finding::sca(vuln, name, version));
                            }
                        }
                        Err(e) => {
                            debug!("Failed to query OSV for PyPI/{}: {}", name, e);
                        }
                    }
                }
            }
        }

        info!("Python SCA complete. Found {} vulnerabilities", findings.len());
        Ok(findings)
    }

    /// Analyze a go.sum file for Go dependencies.
    pub async fn analyze_go_sum(&self, path: &Path) -> Result<Vec<Finding>> {
        info!("Analyzing go.sum: {}", path.display());

        let content = tokio::fs::read_to_string(path).await?;
        let mut findings = Vec::new();
        let mut seen = std::collections::HashSet::new();

        for line in content.lines() {
            let parts: Vec<&str> = line.split_whitespace().collect();

            if parts.len() >= 2 {
                let module = parts[0];
                let version = parts[1].trim_start_matches('v').split('/').next().unwrap_or("");

                // Skip duplicates (go.sum has both direct and /go.mod entries)
                let key = format!("{}@{}", module, version);
                if seen.contains(&key) {
                    continue;
                }
                seen.insert(key);

                self.apply_rate_limit().await;
                match self.query_osv(module, version, "Go").await {
                    Ok(vulns) => {
                        for vuln in vulns {
                            findings.push(Finding::sca(vuln, module, version));
                        }
                    }
                    Err(e) => {
                        debug!("Failed to query OSV for Go/{}: {}", module, e);
                    }
                }
            }
        }

        info!("Go SCA complete. Found {} vulnerabilities", findings.len());
        Ok(findings)
    }

    /// Detect and analyze the appropriate lock file in a repository.
    pub async fn analyze_repository(&self, repo_path: &Path) -> Result<Vec<Finding>> {
        let mut findings = Vec::new();

        // Check for Cargo.lock (Rust)
        let cargo_lock = repo_path.join("Cargo.lock");
        if cargo_lock.exists() {
            findings.extend(self.analyze_cargo_lock(&cargo_lock).await?);
        }

        // Check for package-lock.json or package.json (npm)
        let package_json = repo_path.join("package.json");
        if package_json.exists() {
            findings.extend(self.analyze_package_json(&package_json).await?);
        }

        // Check for requirements.txt (Python)
        let requirements = repo_path.join("requirements.txt");
        if requirements.exists() {
            findings.extend(self.analyze_requirements_txt(&requirements).await?);
        }

        // Check for go.sum (Go)
        let go_sum = repo_path.join("go.sum");
        if go_sum.exists() {
            findings.extend(self.analyze_go_sum(&go_sum).await?);
        }

        Ok(findings)
    }
}

impl Default for ScaEngine {
    fn default() -> Self {
        Self::new()
    }
}

// ============================================================================
// deps.dev API Integration
// ============================================================================

/// deps.dev API client for batch dependency analysis.
///
/// deps.dev (https://deps.dev) is a Google-sponsored service that provides:
/// - Unified dependency metadata across ecosystems
/// - License information
/// - Known vulnerabilities (aggregated from multiple sources)
/// - Dependency graphs
/// - Scorecard security metrics
pub struct DepsDevClient {
    /// HTTP client for API requests.
    client: Client,
    /// Base URL for the deps.dev API.
    base_url: String,
    /// Maximum batch size for queries.
    max_batch_size: usize,
}

impl Default for DepsDevClient {
    fn default() -> Self {
        Self::new()
    }
}

impl DepsDevClient {
    /// Create a new deps.dev client.
    pub fn new() -> Self {
        Self {
            client: Client::builder()
                .timeout(Duration::from_secs(30))
                .user_agent("sec_auditor/0.1.0")
                .build()
                .expect("Failed to create HTTP client"),
            base_url: "https://api.deps.dev/v3".to_string(),
            max_batch_size: 100,
        }
    }

    /// Set custom base URL (for testing).
    pub fn with_base_url(mut self, url: String) -> Self {
        self.base_url = url;
        self
    }

    /// Set maximum batch size.
    pub fn with_batch_size(mut self, size: usize) -> Self {
        self.max_batch_size = size;
        self
    }

    /// Query version information for a single package.
    pub async fn get_version(&self, purl: &PackageUrl) -> Result<DepsDevVersion> {
        let system = purl_to_deps_dev_system(&purl.pkg_type)?;
        let name = if let Some(ref ns) = purl.namespace {
            format!("{}/{}", ns, purl.name)
        } else {
            purl.name.clone()
        };
        let version = purl.version.as_deref().unwrap_or("latest");

        let url = format!(
            "{}/systems/{}/packages/{}/versions/{}",
            self.base_url,
            system,
            urlencoding::encode(&name),
            urlencoding::encode(version)
        );

        debug!("Querying deps.dev: {}", url);

        let response = self.client.get(&url).send().await?;

        if !response.status().is_success() {
            return Err(AuditorError::Analysis(format!(
                "deps.dev API error: {} for {}",
                response.status(),
                url
            )));
        }

        let version_info: DepsDevVersion = response.json().await?;
        Ok(version_info)
    }

    /// Query dependencies for a package version.
    pub async fn get_dependencies(&self, purl: &PackageUrl) -> Result<DepsDevDependencies> {
        let system = purl_to_deps_dev_system(&purl.pkg_type)?;
        let name = if let Some(ref ns) = purl.namespace {
            format!("{}/{}", ns, purl.name)
        } else {
            purl.name.clone()
        };
        let version = purl.version.as_deref().unwrap_or("latest");

        let url = format!(
            "{}/systems/{}/packages/{}/versions/{}:dependencies",
            self.base_url,
            system,
            urlencoding::encode(&name),
            urlencoding::encode(version)
        );

        let response = self.client.get(&url).send().await?;

        if !response.status().is_success() {
            return Err(AuditorError::Analysis(format!(
                "deps.dev API error: {}",
                response.status()
            )));
        }

        let deps: DepsDevDependencies = response.json().await?;
        Ok(deps)
    }

    /// Batch query for multiple packages.
    ///
    /// This is more efficient than individual queries as deps.dev supports
    /// batch requests (up to 100 packages per request).
    pub async fn batch_query(&self, purls: &[PackageUrl]) -> Result<Vec<DepsDevBatchResult>> {
        let mut results = Vec::new();

        // Process in batches
        for chunk in purls.chunks(self.max_batch_size) {
            let batch_results = self.query_batch_chunk(chunk).await?;
            results.extend(batch_results);
        }

        Ok(results)
    }

    /// Query a single batch chunk.
    async fn query_batch_chunk(&self, purls: &[PackageUrl]) -> Result<Vec<DepsDevBatchResult>> {
        // Build batch request
        let requests: Vec<DepsDevBatchRequest> = purls
            .iter()
            .filter_map(|purl| {
                let system = purl_to_deps_dev_system(&purl.pkg_type).ok()?;
                let name = if let Some(ref ns) = purl.namespace {
                    format!("{}/{}", ns, purl.name)
                } else {
                    purl.name.clone()
                };

                Some(DepsDevBatchRequest {
                    version_key: DepsDevVersionKey {
                        system: system.to_string(),
                        name,
                        version: purl.version.clone().unwrap_or_else(|| "latest".to_string()),
                    },
                })
            })
            .collect();

        if requests.is_empty() {
            return Ok(Vec::new());
        }

        let url = format!("{}/query", self.base_url);

        let response = self
            .client
            .post(&url)
            .json(&DepsDevBatchQuery { requests })
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(AuditorError::Analysis(format!(
                "deps.dev batch API error: {}",
                response.status()
            )));
        }

        let batch_response: DepsDevBatchResponse = response.json().await?;
        Ok(batch_response.results)
    }

    /// Get security advisories for a package.
    pub async fn get_advisories(&self, purl: &PackageUrl) -> Result<Vec<DepsDevAdvisory>> {
        let version = self.get_version(purl).await?;
        Ok(version.advisories)
    }

    /// Analyze a list of packages and return security findings.
    pub async fn analyze_packages(&self, purls: &[PackageUrl]) -> Result<Vec<Finding>> {
        let mut findings = Vec::new();

        let batch_results = self.batch_query(purls).await?;

        for result in batch_results {
            // Check for advisories
            for advisory in &result.version.advisories {
                let severity = match advisory.severity.to_lowercase().as_str() {
                    "critical" => Severity::Critical,
                    "high" => Severity::High,
                    "medium" | "moderate" => Severity::Medium,
                    "low" => Severity::Low,
                    _ => Severity::Unknown,
                };

                let vuln = Vulnerability {
                    id: advisory.advisory_key.id.clone(),
                    aliases: vec![],
                    source: VulnerabilitySource::DepsDevAdvisory,
                    summary: advisory.title.clone().unwrap_or_else(|| advisory.advisory_key.id.clone()),
                    details: advisory.description.clone(),
                    severity,
                    cvss_score: None,
                    package: result.version.version_key.name.clone(),
                    affected_versions: vec![VersionRange {
                        introduced: None,
                        fixed: None,
                        last_affected: None,
                    }],
                    fixed_version: None,
                    references: advisory.url.as_ref().map(|u| vec![Reference {
                        ref_type: ReferenceType::Advisory,
                        url: u.clone(),
                    }]).unwrap_or_default(),
                    cwes: vec![],
                    published: None,
                    modified: None,
                };

                findings.push(Finding::sca(
                    vuln,
                    &result.version.version_key.name,
                    &result.version.version_key.version,
                ));
            }
        }

        Ok(findings)
    }
}

/// Convert PURL type to deps.dev system name.
fn purl_to_deps_dev_system(purl_type: &str) -> Result<&'static str> {
    match purl_type.to_lowercase().as_str() {
        "cargo" => Ok("CARGO"),
        "npm" => Ok("NPM"),
        "pypi" => Ok("PYPI"),
        "golang" | "go" => Ok("GO"),
        "maven" => Ok("MAVEN"),
        "nuget" => Ok("NUGET"),
        _ => Err(AuditorError::Analysis(format!(
            "Unsupported deps.dev system: {}",
            purl_type
        ))),
    }
}

// deps.dev API types

/// Version information from deps.dev.
#[derive(Debug, Clone, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DepsDevVersion {
    /// Version identifier.
    pub version_key: DepsDevVersionKey,
    /// Whether this is the default version.
    #[serde(default)]
    pub is_default: bool,
    /// License information.
    #[serde(default)]
    pub licenses: Vec<String>,
    /// Security advisories affecting this version.
    #[serde(default)]
    pub advisories: Vec<DepsDevAdvisory>,
    /// Links to related resources.
    #[serde(default)]
    pub links: Vec<DepsDevLink>,
    /// Published timestamp.
    pub published_at: Option<String>,
}

/// Version key (system/name/version).
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DepsDevVersionKey {
    /// Ecosystem (NPM, PYPI, etc.).
    pub system: String,
    /// Package name.
    pub name: String,
    /// Version string.
    pub version: String,
}

/// Security advisory from deps.dev.
#[derive(Debug, Clone, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DepsDevAdvisory {
    /// Advisory identifier.
    pub advisory_key: DepsDevAdvisoryKey,
    /// Advisory URL.
    pub url: Option<String>,
    /// Title.
    pub title: Option<String>,
    /// Description.
    pub description: Option<String>,
    /// Severity (CRITICAL, HIGH, MEDIUM, LOW).
    #[serde(default)]
    pub severity: String,
    /// Related CVE IDs.
    #[serde(default)]
    pub aliases: Vec<String>,
}

/// Advisory key.
#[derive(Debug, Clone, Deserialize)]
pub struct DepsDevAdvisoryKey {
    /// Advisory ID (e.g., GHSA-xxxx-xxxx-xxxx).
    pub id: String,
}

/// Link to related resource.
#[derive(Debug, Clone, Deserialize)]
pub struct DepsDevLink {
    /// Link label.
    pub label: String,
    /// URL.
    pub url: String,
}

/// Dependencies response.
#[derive(Debug, Clone, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DepsDevDependencies {
    /// Direct dependencies.
    #[serde(default)]
    pub nodes: Vec<DepsDevDependencyNode>,
    /// Dependency edges.
    #[serde(default)]
    pub edges: Vec<DepsDevDependencyEdge>,
}

/// Dependency node.
#[derive(Debug, Clone, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DepsDevDependencyNode {
    /// Node version key.
    pub version_key: DepsDevVersionKey,
    /// Relationship type.
    #[serde(default)]
    pub relation: String,
}

/// Dependency edge.
#[derive(Debug, Clone, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct DepsDevDependencyEdge {
    /// From node index.
    pub from_node: usize,
    /// To node index.
    pub to_node: usize,
    /// Requirement string.
    pub requirement: Option<String>,
}

/// Batch query request.
#[derive(Debug, Serialize)]
struct DepsDevBatchQuery {
    requests: Vec<DepsDevBatchRequest>,
}

/// Single request in batch.
#[derive(Debug, Serialize)]
#[serde(rename_all = "camelCase")]
struct DepsDevBatchRequest {
    version_key: DepsDevVersionKey,
}

/// Batch query response.
#[derive(Debug, Deserialize)]
struct DepsDevBatchResponse {
    #[serde(default)]
    results: Vec<DepsDevBatchResult>,
}

/// Single result in batch response.
#[derive(Debug, Clone, Deserialize)]
pub struct DepsDevBatchResult {
    /// Version information.
    pub version: DepsDevVersion,
}

// ============================================================================
// NVD (National Vulnerability Database) Client for CVSS Enrichment
// ============================================================================

/// NVD API client for fetching CVSS scores and enriching vulnerability data.
///
/// The NVD provides authoritative CVSS scores that may be more accurate than
/// scores from other sources. This client fetches CVE details to enrich
/// vulnerability findings with official CVSS metrics.
pub struct NvdClient {
    /// HTTP client.
    client: Client,
    /// NVD API base URL.
    base_url: String,
    /// API key (optional, increases rate limits).
    api_key: Option<String>,
    /// Request timeout.
    timeout: Duration,
    /// Rate limit delay between requests (2 seconds without API key, 0.6s with key).
    rate_limit_delay: Duration,
}

impl NvdClient {
    /// Create a new NVD client without an API key.
    pub fn new() -> Self {
        Self {
            client: Client::new(),
            base_url: "https://services.nvd.nist.gov/rest/json/cves/2.0".to_string(),
            api_key: None,
            timeout: Duration::from_secs(30),
            rate_limit_delay: Duration::from_secs(2), // 5 requests per 30 seconds without key
        }
    }

    /// Create a new NVD client with an API key.
    pub fn with_api_key(api_key: String) -> Self {
        Self {
            client: Client::new(),
            base_url: "https://services.nvd.nist.gov/rest/json/cves/2.0".to_string(),
            api_key: Some(api_key),
            timeout: Duration::from_secs(30),
            rate_limit_delay: Duration::from_millis(600), // 50 requests per 30 seconds with key
        }
    }

    /// Fetch CVE details from NVD.
    pub async fn get_cve(&self, cve_id: &str) -> Result<Option<NvdCveData>> {
        // Validate CVE ID format
        if !cve_id.starts_with("CVE-") {
            return Ok(None);
        }

        let url = format!("{}?cveId={}", self.base_url, cve_id);

        let mut request = self.client.get(&url).timeout(self.timeout);

        if let Some(ref key) = self.api_key {
            request = request.header("apiKey", key);
        }

        let response = request.send().await?;

        if !response.status().is_success() {
            if response.status() == reqwest::StatusCode::NOT_FOUND {
                return Ok(None);
            }
            return Err(AuditorError::Analysis(format!(
                "NVD API error: {}",
                response.status()
            )));
        }

        let data: NvdApiResponse = response.json().await?;

        if let Some(vuln) = data.vulnerabilities.into_iter().next() {
            return Ok(Some(vuln.cve));
        }

        Ok(None)
    }

    /// Extract CVSS v3.1 score from CVE data.
    pub fn extract_cvss_v3(&self, cve: &NvdCveData) -> Option<CvssV3Data> {
        // Try CVSS v3.1 first, then v3.0
        if let Some(ref metrics) = cve.metrics {
            // Check cvssMetricV31 first
            if let Some(v31_list) = &metrics.cvss_metric_v31 {
                if let Some(metric) = v31_list.first() {
                    return Some(CvssV3Data {
                        version: "3.1".to_string(),
                        vector_string: metric.cvss_data.vector_string.clone(),
                        base_score: metric.cvss_data.base_score,
                        base_severity: metric.cvss_data.base_severity.clone(),
                        exploitability_score: metric.exploitability_score,
                        impact_score: metric.impact_score,
                    });
                }
            }

            // Fall back to cvssMetricV30
            if let Some(v30_list) = &metrics.cvss_metric_v30 {
                if let Some(metric) = v30_list.first() {
                    return Some(CvssV3Data {
                        version: "3.0".to_string(),
                        vector_string: metric.cvss_data.vector_string.clone(),
                        base_score: metric.cvss_data.base_score,
                        base_severity: metric.cvss_data.base_severity.clone(),
                        exploitability_score: metric.exploitability_score,
                        impact_score: metric.impact_score,
                    });
                }
            }
        }

        None
    }

    /// Extract CWE IDs from CVE data.
    pub fn extract_cwes(&self, cve: &NvdCveData) -> Vec<String> {
        let mut cwes = Vec::new();

        if let Some(ref weaknesses) = cve.weaknesses {
            for weakness in weaknesses {
                for desc in &weakness.description {
                    if desc.lang == "en" && desc.value.starts_with("CWE-") {
                        cwes.push(desc.value.clone());
                    }
                }
            }
        }

        cwes
    }

    /// Enrich a vulnerability with NVD data.
    pub async fn enrich_vulnerability(&self, vuln: &mut Vulnerability) -> Result<bool> {
        // Find a CVE ID in the vulnerability's ID or aliases
        let cve_id = if vuln.id.starts_with("CVE-") {
            Some(vuln.id.clone())
        } else {
            vuln.aliases.iter().find(|a| a.starts_with("CVE-")).cloned()
        };

        let cve_id = match cve_id {
            Some(id) => id,
            None => return Ok(false), // No CVE to look up
        };

        // Rate limit
        tokio::time::sleep(self.rate_limit_delay).await;

        // Fetch CVE data
        let cve = match self.get_cve(&cve_id).await? {
            Some(cve) => cve,
            None => return Ok(false),
        };

        let mut enriched = false;

        // Extract CVSS score if we don't have one or NVD has a more authoritative one
        if let Some(cvss) = self.extract_cvss_v3(&cve) {
            if vuln.cvss_score.is_none() || vuln.source != VulnerabilitySource::Nvd {
                vuln.cvss_score = Some(cvss.base_score);
                vuln.severity = Severity::from_cvss(cvss.base_score);
                enriched = true;
            }
        }

        // Add CWEs if we don't have them
        if vuln.cwes.is_empty() {
            let cwes = self.extract_cwes(&cve);
            if !cwes.is_empty() {
                vuln.cwes = cwes;
                enriched = true;
            }
        }

        // Add description if we don't have details
        if vuln.details.is_none() {
            if let Some(desc) = cve.descriptions.iter().find(|d| d.lang == "en") {
                vuln.details = Some(desc.value.clone());
                enriched = true;
            }
        }

        // Add NVD reference
        let nvd_url = format!("https://nvd.nist.gov/vuln/detail/{}", cve_id);
        if !vuln.references.iter().any(|r| r.url == nvd_url) {
            vuln.references.push(Reference {
                ref_type: ReferenceType::Advisory,
                url: nvd_url,
            });
            enriched = true;
        }

        Ok(enriched)
    }

    /// Batch enrich multiple vulnerabilities.
    pub async fn enrich_vulnerabilities(&self, vulns: &mut [Vulnerability]) -> Result<usize> {
        let mut enriched_count = 0;

        for vuln in vulns.iter_mut() {
            match self.enrich_vulnerability(vuln).await {
                Ok(true) => enriched_count += 1,
                Ok(false) => {}
                Err(e) => {
                    debug!("Failed to enrich {}: {}", vuln.id, e);
                }
            }
        }

        Ok(enriched_count)
    }
}

impl Default for NvdClient {
    fn default() -> Self {
        Self::new()
    }
}

/// Extracted CVSS v3 data.
#[derive(Debug, Clone)]
pub struct CvssV3Data {
    /// CVSS version (3.0 or 3.1).
    pub version: String,
    /// CVSS vector string (e.g., "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H").
    pub vector_string: String,
    /// Base score (0.0 - 10.0).
    pub base_score: f64,
    /// Base severity (NONE, LOW, MEDIUM, HIGH, CRITICAL).
    pub base_severity: String,
    /// Exploitability score (0.0 - 10.0).
    pub exploitability_score: Option<f64>,
    /// Impact score (0.0 - 10.0).
    pub impact_score: Option<f64>,
}

// NVD API response types

#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
struct NvdApiResponse {
    #[serde(default)]
    vulnerabilities: Vec<NvdVulnerabilityWrapper>,
}

#[derive(Debug, Deserialize)]
struct NvdVulnerabilityWrapper {
    cve: NvdCveData,
}

/// CVE data from NVD API.
#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct NvdCveData {
    /// CVE ID.
    pub id: String,
    /// Source identifier.
    pub source_identifier: Option<String>,
    /// Publication date.
    pub published: Option<String>,
    /// Last modification date.
    pub last_modified: Option<String>,
    /// Vulnerability status.
    pub vuln_status: Option<String>,
    /// Descriptions in various languages.
    #[serde(default)]
    pub descriptions: Vec<NvdDescription>,
    /// CVSS metrics.
    pub metrics: Option<NvdMetrics>,
    /// Weakness enumeration (CWEs).
    pub weaknesses: Option<Vec<NvdWeakness>>,
    /// Configuration data.
    pub configurations: Option<serde_json::Value>,
    /// References.
    pub references: Option<Vec<NvdReference>>,
}

#[derive(Debug, Deserialize)]
pub struct NvdDescription {
    pub lang: String,
    pub value: String,
}

#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct NvdMetrics {
    pub cvss_metric_v31: Option<Vec<NvdCvssMetric>>,
    pub cvss_metric_v30: Option<Vec<NvdCvssMetric>>,
    pub cvss_metric_v2: Option<Vec<serde_json::Value>>,
}

#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct NvdCvssMetric {
    pub source: String,
    #[serde(rename = "type")]
    pub metric_type: String,
    pub cvss_data: NvdCvssData,
    pub exploitability_score: Option<f64>,
    pub impact_score: Option<f64>,
}

#[derive(Debug, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct NvdCvssData {
    pub version: String,
    pub vector_string: String,
    pub base_score: f64,
    pub base_severity: String,
}

#[derive(Debug, Deserialize)]
pub struct NvdWeakness {
    pub source: String,
    #[serde(rename = "type")]
    pub weakness_type: String,
    #[serde(default)]
    pub description: Vec<NvdDescription>,
}

#[derive(Debug, Deserialize)]
pub struct NvdReference {
    pub url: String,
    pub source: Option<String>,
    #[serde(default)]
    pub tags: Vec<String>,
}

// OSV API request/response types

#[derive(Debug, Serialize)]
struct OsvQueryRequest {
    package: OsvPackage,
    version: String,
}

#[derive(Debug, Serialize, Deserialize)]
struct OsvPackage {
    name: String,
    ecosystem: String,
}

#[derive(Debug, Deserialize)]
struct OsvQueryResponse {
    #[serde(default)]
    vulns: Vec<OsvVulnerability>,
}

#[derive(Debug, Deserialize)]
struct OsvVulnerability {
    id: String,
    #[serde(default)]
    aliases: Vec<String>,
    summary: Option<String>,
    details: Option<String>,
    #[serde(default)]
    severity: Vec<OsvSeverity>,
    #[serde(default)]
    affected: Vec<OsvAffected>,
    #[serde(default)]
    references: Vec<OsvReference>,
    published: Option<String>,
    modified: Option<String>,
    database_specific: Option<serde_json::Value>,
}

#[derive(Debug, Deserialize)]
struct OsvSeverity {
    #[serde(rename = "type")]
    severity_type: Option<String>,
    score: Option<String>,
}

#[derive(Debug, Deserialize)]
struct OsvAffected {
    package: OsvPackage,
    #[serde(default)]
    ranges: Vec<OsvRange>,
    #[serde(default)]
    versions: Vec<String>,
}

#[derive(Debug, Deserialize)]
struct OsvRange {
    #[serde(rename = "type")]
    range_type: String,
    #[serde(default)]
    events: Vec<OsvEvent>,
}

#[derive(Debug, Deserialize)]
struct OsvEvent {
    introduced: Option<String>,
    fixed: Option<String>,
    last_affected: Option<String>,
}

#[derive(Debug, Deserialize)]
struct OsvReference {
    #[serde(rename = "type")]
    ref_type: String,
    url: String,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_osv_query() {
        let engine = ScaEngine::new();

        // Query for a known vulnerable package
        let result = engine.query_osv("lodash", "4.17.20", "npm").await;

        // This should succeed (though may or may not find vulnerabilities)
        assert!(result.is_ok());
    }

    #[test]
    fn test_purl_parse_cargo() {
        let purl = PackageUrl::parse("pkg:cargo/serde@1.0.193").unwrap();
        assert_eq!(purl.pkg_type, "cargo");
        assert_eq!(purl.name, "serde");
        assert_eq!(purl.version, Some("1.0.193".to_string()));
        assert!(purl.namespace.is_none());
    }

    #[test]
    fn test_purl_parse_npm_scoped() {
        let purl = PackageUrl::parse("pkg:npm/@types/node@20.10.0").unwrap();
        assert_eq!(purl.pkg_type, "npm");
        assert_eq!(purl.name, "node");
        assert_eq!(purl.namespace, Some("@types".to_string()));
        assert_eq!(purl.version, Some("20.10.0".to_string()));
    }

    #[test]
    fn test_purl_parse_npm_unscoped() {
        let purl = PackageUrl::parse("pkg:npm/lodash@4.17.21").unwrap();
        assert_eq!(purl.pkg_type, "npm");
        assert_eq!(purl.name, "lodash");
        assert!(purl.namespace.is_none());
        assert_eq!(purl.version, Some("4.17.21".to_string()));
    }

    #[test]
    fn test_purl_parse_pypi() {
        let purl = PackageUrl::parse("pkg:pypi/requests@2.31.0").unwrap();
        assert_eq!(purl.pkg_type, "pypi");
        assert_eq!(purl.name, "requests");
        assert_eq!(purl.version, Some("2.31.0".to_string()));
    }

    #[test]
    fn test_purl_parse_golang() {
        let purl = PackageUrl::parse("pkg:golang/github.com/gin-gonic/gin@1.9.1").unwrap();
        assert_eq!(purl.pkg_type, "golang");
        assert_eq!(purl.name, "github.com/gin-gonic/gin");
        assert_eq!(purl.version, Some("1.9.1".to_string()));
    }

    #[test]
    fn test_purl_parse_with_qualifiers() {
        let purl = PackageUrl::parse("pkg:npm/lodash@4.17.21?vcs_url=git://github.com").unwrap();
        assert_eq!(purl.qualifiers.get("vcs_url"), Some(&"git://github.com".to_string()));
    }

    #[test]
    fn test_purl_roundtrip_cargo() {
        let original = PackageUrl::from_cargo("serde", "1.0.193");
        let serialized = original.to_string();
        assert_eq!(serialized, "pkg:cargo/serde@1.0.193");
    }

    #[test]
    fn test_purl_roundtrip_npm_scoped() {
        let original = PackageUrl::from_npm("@types/node", "20.10.0");
        let serialized = original.to_string();
        assert_eq!(serialized, "pkg:npm/%40types/node@20.10.0");
    }

    #[test]
    fn test_purl_osv_ecosystem() {
        assert_eq!(PackageUrl::from_cargo("serde", "1.0").osv_ecosystem(), "crates.io");
        assert_eq!(PackageUrl::from_npm("lodash", "4.17").osv_ecosystem(), "npm");
        assert_eq!(PackageUrl::from_pypi("requests", "2.31").osv_ecosystem(), "PyPI");
        assert_eq!(PackageUrl::from_golang("github.com/gin-gonic/gin", "1.9").osv_ecosystem(), "Go");
    }

    #[test]
    fn test_purl_pypi_normalization() {
        // PyPI normalizes underscores to hyphens
        let purl = PackageUrl::from_pypi("my_package", "1.0.0");
        assert_eq!(purl.name, "my-package");
    }

    // Ecosystem-specific version parsing tests

    #[test]
    fn test_npm_version_in_range() {
        // Basic range check
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::Npm,
            "1.5.0",
            Some("1.0.0"),
            Some("2.0.0"),
            None
        ));

        // Below introduced
        assert!(!EcosystemVersionParser::version_in_range(
            Ecosystem::Npm,
            "0.9.0",
            Some("1.0.0"),
            Some("2.0.0"),
            None
        ));

        // At or above fixed
        assert!(!EcosystemVersionParser::version_in_range(
            Ecosystem::Npm,
            "2.0.0",
            Some("1.0.0"),
            Some("2.0.0"),
            None
        ));

        // Prerelease versions
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::Npm,
            "1.0.0-beta.1",
            Some("1.0.0-alpha.1"),
            Some("1.0.0"),
            None
        ));
    }

    #[test]
    fn test_npm_satisfies_range() {
        // Caret range
        assert!(EcosystemVersionParser::npm_satisfies("1.2.3", "^1.0.0"));
        assert!(!EcosystemVersionParser::npm_satisfies("2.0.0", "^1.0.0"));

        // Tilde range
        assert!(EcosystemVersionParser::npm_satisfies("1.2.5", "~1.2.0"));
        assert!(!EcosystemVersionParser::npm_satisfies("1.3.0", "~1.2.0"));

        // Hyphen range
        assert!(EcosystemVersionParser::npm_satisfies("1.5.0", "1.0.0 - 2.0.0"));
    }

    #[test]
    fn test_pypi_version_in_range() {
        // Basic PEP 440 versions
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::PyPI,
            "2.0.0",
            Some("1.0.0"),
            Some("3.0.0"),
            None
        ));

        // Pre-release versions
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::PyPI,
            "1.0a1",
            Some("1.0a0"),
            Some("1.0"),
            None
        ));

        // Post release
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::PyPI,
            "1.0.post1",
            Some("1.0"),
            Some("1.1"),
            None
        ));
    }

    #[test]
    fn test_pypi_satisfies_specifier() {
        // Compatible release
        assert!(EcosystemVersionParser::pypi_satisfies("1.4.2", "~=1.4.0"));
        assert!(!EcosystemVersionParser::pypi_satisfies("1.5.0", "~=1.4.0"));

        // Version matching
        assert!(EcosystemVersionParser::pypi_satisfies("1.0", "==1.0"));
        assert!(EcosystemVersionParser::pypi_satisfies("1.0.0", "==1.0.*"));

        // Range specifiers
        assert!(EcosystemVersionParser::pypi_satisfies("1.5.0", ">=1.0,<2.0"));
        assert!(!EcosystemVersionParser::pypi_satisfies("2.0.0", ">=1.0,<2.0"));
    }

    #[test]
    fn test_cargo_version_in_range() {
        // Standard semver
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::Cargo,
            "1.5.0",
            Some("1.0.0"),
            Some("2.0.0"),
            None
        ));

        // Pre-release
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::Cargo,
            "1.0.0-alpha.2",
            Some("1.0.0-alpha.1"),
            Some("1.0.0"),
            None
        ));
    }

    #[test]
    fn test_cargo_satisfies_requirement() {
        // Caret (default)
        assert!(EcosystemVersionParser::cargo_satisfies("1.2.3", "^1.0"));
        assert!(!EcosystemVersionParser::cargo_satisfies("2.0.0", "^1.0"));

        // Exact
        assert!(EcosystemVersionParser::cargo_satisfies("1.0.0", "=1.0.0"));
        assert!(!EcosystemVersionParser::cargo_satisfies("1.0.1", "=1.0.0"));

        // Wildcard
        assert!(EcosystemVersionParser::cargo_satisfies("1.2.3", "1.*"));
        assert!(!EcosystemVersionParser::cargo_satisfies("2.0.0", "1.*"));
    }

    #[test]
    fn test_go_version_in_range() {
        // Go uses v prefix
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::Go,
            "v1.5.0",
            Some("v1.0.0"),
            Some("v2.0.0"),
            None
        ));

        // Mixed v prefix
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::Go,
            "1.5.0",
            Some("v1.0.0"),
            Some("2.0.0"),
            None
        ));
    }

    #[test]
    fn test_fallback_version_comparison() {
        // Non-standard versions fall back to numeric-aware comparison
        assert!(EcosystemVersionParser::version_in_range(
            Ecosystem::Unknown,
            "2.10.0",
            Some("2.9.0"),
            Some("2.11.0"),
            None
        ));

        // Should handle 2.10 > 2.9 correctly (not string comparison)
        assert!(!EcosystemVersionParser::version_in_range(
            Ecosystem::Unknown,
            "2.8.0",
            Some("2.9.0"),
            None,
            None
        ));
    }

    #[test]
    fn test_version_range_ext() {
        let range = VersionRange {
            introduced: Some("1.0.0".to_string()),
            fixed: Some("2.0.0".to_string()),
            last_affected: None,
        };

        assert!(range.contains_version("1.5.0", Ecosystem::Cargo));
        assert!(!range.contains_version("2.0.0", Ecosystem::Cargo));
        assert!(!range.contains_version("0.9.0", Ecosystem::Cargo));
    }

    #[test]
    fn test_ecosystem_detection() {
        assert_eq!(Ecosystem::from_purl_type("cargo"), Ecosystem::Cargo);
        assert_eq!(Ecosystem::from_purl_type("npm"), Ecosystem::Npm);
        assert_eq!(Ecosystem::from_purl_type("pypi"), Ecosystem::PyPI);
        assert_eq!(Ecosystem::from_purl_type("golang"), Ecosystem::Go);
        assert_eq!(Ecosystem::from_purl_type("go"), Ecosystem::Go);
        assert_eq!(Ecosystem::from_purl_type("maven"), Ecosystem::Maven);
        assert_eq!(Ecosystem::from_purl_type("unknown"), Ecosystem::Unknown);
    }

    // NVD Client tests

    #[test]
    fn test_nvd_client_creation() {
        let client = NvdClient::new();
        assert!(client.api_key.is_none());

        let client_with_key = NvdClient::with_api_key("test-key".to_string());
        assert_eq!(client_with_key.api_key, Some("test-key".to_string()));
    }

    #[test]
    fn test_nvd_cvss_extraction() {
        let client = NvdClient::new();

        // Create mock CVE data with CVSS v3.1
        let cve = NvdCveData {
            id: "CVE-2024-1234".to_string(),
            source_identifier: None,
            published: None,
            last_modified: None,
            vuln_status: None,
            descriptions: vec![NvdDescription {
                lang: "en".to_string(),
                value: "Test vulnerability description".to_string(),
            }],
            metrics: Some(NvdMetrics {
                cvss_metric_v31: Some(vec![NvdCvssMetric {
                    source: "nvd@nist.gov".to_string(),
                    metric_type: "Primary".to_string(),
                    cvss_data: NvdCvssData {
                        version: "3.1".to_string(),
                        vector_string: "CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H".to_string(),
                        base_score: 9.8,
                        base_severity: "CRITICAL".to_string(),
                    },
                    exploitability_score: Some(3.9),
                    impact_score: Some(5.9),
                }]),
                cvss_metric_v30: None,
                cvss_metric_v2: None,
            }),
            weaknesses: Some(vec![NvdWeakness {
                source: "nvd@nist.gov".to_string(),
                weakness_type: "Primary".to_string(),
                description: vec![NvdDescription {
                    lang: "en".to_string(),
                    value: "CWE-89".to_string(),
                }],
            }]),
            configurations: None,
            references: None,
        };

        // Test CVSS extraction
        let cvss = client.extract_cvss_v3(&cve).unwrap();
        assert_eq!(cvss.version, "3.1");
        assert_eq!(cvss.base_score, 9.8);
        assert_eq!(cvss.base_severity, "CRITICAL");
        assert_eq!(cvss.exploitability_score, Some(3.9));

        // Test CWE extraction
        let cwes = client.extract_cwes(&cve);
        assert_eq!(cwes, vec!["CWE-89"]);
    }

    #[test]
    fn test_nvd_cvss_v30_fallback() {
        let client = NvdClient::new();

        // Create CVE data with only CVSS v3.0 (no v3.1)
        let cve = NvdCveData {
            id: "CVE-2024-5678".to_string(),
            source_identifier: None,
            published: None,
            last_modified: None,
            vuln_status: None,
            descriptions: vec![],
            metrics: Some(NvdMetrics {
                cvss_metric_v31: None,
                cvss_metric_v30: Some(vec![NvdCvssMetric {
                    source: "nvd@nist.gov".to_string(),
                    metric_type: "Primary".to_string(),
                    cvss_data: NvdCvssData {
                        version: "3.0".to_string(),
                        vector_string: "CVSS:3.0/AV:N/AC:L/PR:N/UI:R/S:C/C:L/I:L/A:N".to_string(),
                        base_score: 6.1,
                        base_severity: "MEDIUM".to_string(),
                    },
                    exploitability_score: Some(2.8),
                    impact_score: Some(2.7),
                }]),
                cvss_metric_v2: None,
            }),
            weaknesses: None,
            configurations: None,
            references: None,
        };

        let cvss = client.extract_cvss_v3(&cve).unwrap();
        assert_eq!(cvss.version, "3.0");
        assert_eq!(cvss.base_score, 6.1);
    }

    #[test]
    fn test_nvd_no_metrics() {
        let client = NvdClient::new();

        let cve = NvdCveData {
            id: "CVE-2024-0000".to_string(),
            source_identifier: None,
            published: None,
            last_modified: None,
            vuln_status: None,
            descriptions: vec![],
            metrics: None,
            weaknesses: None,
            configurations: None,
            references: None,
        };

        assert!(client.extract_cvss_v3(&cve).is_none());
        assert!(client.extract_cwes(&cve).is_empty());
    }
}

================================================================================
END: src\analyzer\sca.rs
================================================================================

================================================================================
FILE: src\analyzer\secrets.rs
================================================================================
//! Secret detection module with entropy analysis.
//!
//! This module provides advanced secret detection with charset-aware dynamic
//! entropy thresholds, as recommended by security research. Key improvements:
//!
//! - **Dynamic thresholds**: Different thresholds for hex (3.0), base64 (4.5),
//!   and alphanumeric (3.7) strings based on theoretical entropy maximums.
//! - **False positive filtering**: UUID and git hash patterns are excluded.
//! - **Context-aware detection**: Keyword weighting improves confidence.

use crate::models::{CodeSnippet, Confidence, Finding, Language, Location, Severity};
use regex::Regex;
use std::collections::HashMap;
use std::path::Path;
use tracing::debug;

/// Dynamic entropy thresholds per character set (based on research).
/// These values are derived from theoretical maximum entropy and empirical analysis.
#[derive(Debug, Clone)]
pub struct EntropyThresholds {
    /// Threshold for hexadecimal strings (max theoretical: 4.0 bits/char)
    pub hex: f64,
    /// Threshold for base64 strings (max theoretical: 6.0 bits/char)
    pub base64: f64,
    /// Threshold for alphanumeric strings (max theoretical: ~5.95 bits/char)
    pub alphanumeric: f64,
    /// Default threshold for mixed character sets
    pub default: f64,
}

impl Default for EntropyThresholds {
    fn default() -> Self {
        Self {
            hex: 3.0,          // ~75% of max (4.0)
            base64: 4.5,       // ~75% of max (6.0)
            alphanumeric: 3.7, // ~62% of max (5.95)
            default: 4.0,
        }
    }
}

/// Secret detector with pattern matching and charset-aware entropy analysis.
pub struct SecretDetector {
    /// Patterns for detecting secrets
    patterns: Vec<SecretPattern>,

    /// Dynamic entropy thresholds per character set
    thresholds: EntropyThresholds,

    /// Compiled regex patterns for false positive filtering
    uuid_pattern: Regex,
    git_hash_pattern: Regex,
    semantic_version_pattern: Regex,

    /// Keywords that increase suspicion when found in context
    suspicious_keywords: Vec<&'static str>,
}

/// A pattern for detecting secrets.
#[derive(Debug, Clone)]
pub struct SecretPattern {
    /// Pattern name
    pub name: &'static str,

    /// Description
    pub description: &'static str,

    /// Regex pattern
    pub pattern: Regex,

    /// Severity level
    pub severity: Severity,

    /// Confidence level
    pub confidence: Confidence,

    /// Whether entropy check is required
    pub require_entropy: bool,
}

/// Character set classification for entropy calculation and threshold selection.
#[derive(Debug, Clone, Copy, Hash, PartialEq, Eq)]
pub enum CharSet {
    /// Only hexadecimal characters (0-9, a-f, A-F)
    Hex,
    /// Base64 characters (A-Z, a-z, 0-9, +, /, =)
    Base64,
    /// Alphanumeric only (A-Z, a-z, 0-9)
    Alphanumeric,
    /// Alphanumeric with common symbols
    AlphanumericSymbol,
    /// Unknown or mixed character set
    Mixed,
}

impl CharSet {
    /// Detect the character set of a string.
    pub fn detect(s: &str) -> Self {
        if s.is_empty() {
            return CharSet::Mixed;
        }

        let mut has_hex_letter = false;      // a-f, A-F
        let mut has_non_hex_letter = false;  // g-z, G-Z
        let mut has_digit = false;
        let mut has_base64_special = false;  // +, /, =, -, _
        let mut has_other_symbol = false;

        for c in s.chars() {
            match c {
                'a'..='f' | 'A'..='F' => has_hex_letter = true,
                'g'..='z' | 'G'..='Z' => has_non_hex_letter = true,
                '0'..='9' => has_digit = true,
                '+' | '/' | '=' => has_base64_special = true,
                '-' | '_' => has_base64_special = true, // URL-safe base64
                _ => has_other_symbol = true,
            }
        }

        // Determine charset based on character composition
        if has_other_symbol {
            // Contains symbols not in base64/alphanumeric
            CharSet::AlphanumericSymbol
        } else if has_base64_special {
            // Contains base64-specific characters (+, /, =, -, _)
            CharSet::Base64
        } else if has_non_hex_letter {
            // Has letters outside hex range (g-z, G-Z), so it's alphanumeric
            CharSet::Alphanumeric
        } else if has_hex_letter || has_digit {
            // Only hex-compatible characters (0-9, a-f, A-F)
            // Verify all characters are valid hex
            if s.chars().all(|c| c.is_ascii_hexdigit()) {
                CharSet::Hex
            } else {
                CharSet::Alphanumeric
            }
        } else {
            CharSet::Mixed
        }
    }

    /// Get the theoretical maximum entropy for this character set.
    pub fn max_entropy(&self) -> f64 {
        match self {
            CharSet::Hex => 4.0,              // log2(16)
            CharSet::Base64 => 6.0,           // log2(64)
            CharSet::Alphanumeric => 5.95,    // log2(62)
            CharSet::AlphanumericSymbol => 6.5, // ~log2(94) for printable ASCII
            CharSet::Mixed => 6.5,
        }
    }
}

impl SecretDetector {
    /// Create a new secret detector with default thresholds.
    pub fn new(_entropy_threshold: f64) -> Self {
        Self::with_thresholds(EntropyThresholds::default())
    }

    /// Create a new secret detector with custom thresholds.
    pub fn with_thresholds(thresholds: EntropyThresholds) -> Self {
        let patterns = Self::default_patterns();

        // UUID v4 pattern (standard format with hyphens)
        let uuid_pattern = Regex::new(
            r"(?i)^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$"
        ).unwrap();

        // Git commit hash patterns (SHA-1 = 40 chars, SHA-256 = 64 chars)
        let git_hash_pattern = Regex::new(
            r"^[0-9a-f]{40}$|^[0-9a-f]{64}$"
        ).unwrap();

        // Semantic version pattern
        let semantic_version_pattern = Regex::new(
            r"^\d+\.\d+\.\d+(-[a-zA-Z0-9.]+)?(\+[a-zA-Z0-9.]+)?$"
        ).unwrap();

        // Keywords that indicate a potential secret in context
        let suspicious_keywords = vec![
            "key", "secret", "token", "password", "passwd", "pwd",
            "auth", "credential", "api", "private", "access",
            "bearer", "jwt", "session", "encryption", "signing",
        ];

        Self {
            patterns,
            thresholds,
            uuid_pattern,
            git_hash_pattern,
            semantic_version_pattern,
            suspicious_keywords,
        }
    }

    /// Get the appropriate entropy threshold for a string based on its character set.
    pub fn get_threshold_for_string(&self, s: &str) -> f64 {
        let charset = CharSet::detect(s);
        self.get_threshold_for_charset(charset)
    }

    /// Get the entropy threshold for a specific character set.
    pub fn get_threshold_for_charset(&self, charset: CharSet) -> f64 {
        match charset {
            CharSet::Hex => self.thresholds.hex,
            CharSet::Base64 => self.thresholds.base64,
            CharSet::Alphanumeric => self.thresholds.alphanumeric,
            _ => self.thresholds.default,
        }
    }

    /// Check if a string is a false positive (UUID, git hash, etc.).
    fn is_false_positive(&self, s: &str) -> bool {
        // Check for UUID pattern
        if self.uuid_pattern.is_match(s) {
            debug!("Filtered UUID: {}", s);
            return true;
        }

        // Check for git hash pattern (only for hex strings)
        let clean_s = s.to_lowercase();
        if clean_s.chars().all(|c| c.is_ascii_hexdigit()) {
            if self.git_hash_pattern.is_match(&clean_s) {
                debug!("Filtered git hash: {}", s);
                return true;
            }
        }

        // Check for semantic version
        if self.semantic_version_pattern.is_match(s) {
            debug!("Filtered semantic version: {}", s);
            return true;
        }

        // Check for common non-secret patterns
        if self.is_common_non_secret(s) {
            return true;
        }

        false
    }

    /// Check for common non-secret patterns.
    fn is_common_non_secret(&self, s: &str) -> bool {
        // Check for repeated characters (unlikely to be a real secret)
        if s.len() > 10 {
            let chars: Vec<char> = s.chars().collect();
            let unique_chars: std::collections::HashSet<_> = chars.iter().collect();
            if unique_chars.len() <= 3 {
                return true; // Too few unique characters
            }
        }

        // Check for common placeholder patterns
        let lower = s.to_lowercase();
        let placeholder_patterns = [
            "xxxxxxxx", "00000000", "ffffffff", "12345678",
            "abcdefgh", "testtest", "exampl", "dummy",
            "changeme", "replace", "insert", "your_",
        ];
        for pattern in placeholder_patterns {
            if lower.contains(pattern) {
                return true;
            }
        }

        false
    }

    /// Calculate context weight based on surrounding text.
    /// Returns a value between 0.0 and 1.0 indicating suspicion level.
    fn calculate_context_weight(&self, context: &str) -> f64 {
        let lower_context = context.to_lowercase();
        let mut weight: f64 = 0.0;

        for keyword in &self.suspicious_keywords {
            if lower_context.contains(keyword) {
                weight += 0.2;
            }
        }

        // Check for assignment patterns
        if lower_context.contains('=') || lower_context.contains(':') {
            weight += 0.1;
        }

        // Check for environment variable patterns
        if lower_context.contains("env") || lower_context.contains("getenv") {
            weight -= 0.2; // Reduces suspicion (proper usage)
        }

        // Cap at 1.0
        weight.min(1.0).max(0.0)
    }

    /// Check if a string is high entropy considering its character set.
    pub fn is_high_entropy_dynamic(&self, s: &str) -> bool {
        let entropy = self.calculate_entropy(s);
        let threshold = self.get_threshold_for_string(s);
        entropy >= threshold
    }

    /// Check if a string is high entropy with context weighting.
    pub fn is_high_entropy_with_context(&self, s: &str, context: &str) -> bool {
        let entropy = self.calculate_entropy(s);
        let base_threshold = self.get_threshold_for_string(s);
        let context_weight = self.calculate_context_weight(context);

        // Lower the threshold if suspicious keywords are present
        let adjusted_threshold = base_threshold - (context_weight * 0.5);

        entropy >= adjusted_threshold
    }

    /// Get default secret patterns.
    fn default_patterns() -> Vec<SecretPattern> {
        vec![
            // AWS
            SecretPattern {
                name: "AWS Access Key ID",
                description: "AWS Access Key ID (starts with AKIA, ABIA, ACCA, ASIA)",
                pattern: Regex::new(r"(?i)\b(A3T[A-Z0-9]|AKIA|ABIA|ACCA|ASIA)[A-Z0-9]{16}\b").unwrap(),
                severity: Severity::Critical,
                confidence: Confidence::High,
                require_entropy: false,
            },
            SecretPattern {
                name: "AWS Secret Access Key",
                description: "AWS Secret Access Key (40 character base64)",
                pattern: Regex::new(r#"(?i)aws.{0,20}secret.{0,20}['"][A-Za-z0-9/+=]{40}['"]"#).unwrap(),
                severity: Severity::Critical,
                confidence: Confidence::High,
                require_entropy: true,
            },
            // GitHub
            SecretPattern {
                name: "GitHub Personal Access Token",
                description: "GitHub Personal Access Token (ghp_, gho_, ghu_, ghs_, ghr_)",
                pattern: Regex::new(r"\b(ghp_[a-zA-Z0-9]{36}|gho_[a-zA-Z0-9]{36}|ghu_[a-zA-Z0-9]{36}|ghs_[a-zA-Z0-9]{36}|ghr_[a-zA-Z0-9]{36})\b").unwrap(),
                severity: Severity::Critical,
                confidence: Confidence::High,
                require_entropy: false,
            },
            // Private Keys
            SecretPattern {
                name: "Private Key",
                description: "Private key (RSA, DSA, EC, PGP, SSH)",
                pattern: Regex::new(r"-----BEGIN (RSA |DSA |EC |PGP |OPENSSH )?PRIVATE KEY-----").unwrap(),
                severity: Severity::Critical,
                confidence: Confidence::High,
                require_entropy: false,
            },
            // Generic API Keys
            SecretPattern {
                name: "Generic API Key",
                description: "Generic API key pattern",
                pattern: Regex::new(r#"(?i)(api[_-]?key|apikey)\s*[:=]\s*['"]?[a-zA-Z0-9_-]{20,}['"]?"#).unwrap(),
                severity: Severity::High,
                confidence: Confidence::Medium,
                require_entropy: true,
            },
            // Passwords in config
            SecretPattern {
                name: "Password in Config",
                description: "Password assignment in configuration",
                pattern: Regex::new(r#"(?i)(password|passwd|pwd)\s*[:=]\s*['"][^'"]{8,}['"]"#).unwrap(),
                severity: Severity::High,
                confidence: Confidence::Medium,
                require_entropy: true,
            },
            // Bearer tokens
            SecretPattern {
                name: "Bearer Token",
                description: "Bearer token in authorization header",
                pattern: Regex::new(r#"(?i)bearer\s+[a-zA-Z0-9_-]{20,}"#).unwrap(),
                severity: Severity::High,
                confidence: Confidence::Medium,
                require_entropy: true,
            },
            // JWT
            SecretPattern {
                name: "JSON Web Token",
                description: "JWT token (three base64 parts separated by dots)",
                pattern: Regex::new(r"\beyJ[a-zA-Z0-9_-]*\.eyJ[a-zA-Z0-9_-]*\.[a-zA-Z0-9_-]*\b").unwrap(),
                severity: Severity::High,
                confidence: Confidence::High,
                require_entropy: false,
            },
            // Slack
            SecretPattern {
                name: "Slack Token",
                description: "Slack API token",
                pattern: Regex::new(r"\bxox[baprs]-[a-zA-Z0-9-]{10,}\b").unwrap(),
                severity: Severity::High,
                confidence: Confidence::High,
                require_entropy: false,
            },
            // Google
            SecretPattern {
                name: "Google API Key",
                description: "Google API key (AIza prefix)",
                pattern: Regex::new(r"\bAIza[a-zA-Z0-9_-]{35}\b").unwrap(),
                severity: Severity::High,
                confidence: Confidence::High,
                require_entropy: false,
            },
            // Database URLs
            SecretPattern {
                name: "Database Connection String",
                description: "Database connection string with credentials",
                pattern: Regex::new(r#"(?i)(mysql|postgres|mongodb|redis|amqp)://[^:]+:[^@]+@[^\s]+"#).unwrap(),
                severity: Severity::Critical,
                confidence: Confidence::High,
                require_entropy: false,
            },
            // Generic secrets
            SecretPattern {
                name: "Generic Secret",
                description: "Generic secret assignment",
                pattern: Regex::new(r#"(?i)(secret|token|auth)[_-]?(key|token|secret)?\s*[:=]\s*['"][a-zA-Z0-9+/=_-]{16,}['"]"#).unwrap(),
                severity: Severity::Medium,
                confidence: Confidence::Low,
                require_entropy: true,
            },
            // Stripe
            SecretPattern {
                name: "Stripe API Key",
                description: "Stripe API key (sk_live or rk_live)",
                pattern: Regex::new(r"\b(sk|rk)_live_[a-zA-Z0-9]{24,}\b").unwrap(),
                severity: Severity::Critical,
                confidence: Confidence::High,
                require_entropy: false,
            },
            // Twilio
            SecretPattern {
                name: "Twilio API Key",
                description: "Twilio API key",
                pattern: Regex::new(r"\bSK[a-f0-9]{32}\b").unwrap(),
                severity: Severity::High,
                confidence: Confidence::High,
                require_entropy: false,
            },
            // Heroku
            SecretPattern {
                name: "Heroku API Key",
                description: "Heroku API key",
                pattern: Regex::new(r#"(?i)heroku.*['"][0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}['"]"#).unwrap(),
                severity: Severity::High,
                confidence: Confidence::High,
                require_entropy: false,
            },
        ]
    }

    /// Calculate Shannon entropy of a string.
    pub fn calculate_entropy(&self, s: &str) -> f64 {
        if s.is_empty() {
            return 0.0;
        }

        let mut char_counts: HashMap<char, usize> = HashMap::new();
        for c in s.chars() {
            *char_counts.entry(c).or_insert(0) += 1;
        }

        let len = s.len() as f64;
        let mut entropy = 0.0;

        for count in char_counts.values() {
            let p = *count as f64 / len;
            entropy -= p * p.log2();
        }

        entropy
    }

    /// Check if a string has high entropy using dynamic charset-aware thresholds.
    pub fn is_high_entropy(&self, s: &str) -> bool {
        // First check for false positives
        if self.is_false_positive(s) {
            return false;
        }

        // Use charset-aware dynamic thresholds
        self.is_high_entropy_dynamic(s)
    }

    /// Detect secrets in source code.
    pub fn detect(&self, content: &str, file_path: &Path, language: Language) -> Vec<Finding> {
        let mut findings = Vec::new();

        // Check each pattern
        for pattern in &self.patterns {
            for mat in pattern.pattern.find_iter(content) {
                let matched_text = mat.as_str();

                // Apply entropy check if required
                if pattern.require_entropy {
                    // Extract the actual secret value (remove variable names, quotes, etc.)
                    let secret_value = Self::extract_secret_value(matched_text);
                    if !self.is_high_entropy(&secret_value) {
                        debug!(
                            "Skipping low-entropy match: {} (entropy: {:.2})",
                            pattern.name,
                            self.calculate_entropy(&secret_value)
                        );
                        continue;
                    }
                }

                // Calculate line and column
                let (line, column) = Self::get_position(content, mat.start());

                let location = Location::new(file_path.to_path_buf(), line, column)
                    .with_language(language);

                let snippet = CodeSnippet::from_content(content, line, 2);

                // Redact the secret in the description
                let redacted = Self::redact_secret(matched_text);

                let finding = Finding::sast(
                    format!("secret/{}", pattern.name.to_lowercase().replace(' ', "-")),
                    pattern.name,
                    format!("{}: {}", pattern.description, redacted),
                    location,
                    pattern.severity,
                )
                .with_confidence(pattern.confidence)
                .with_snippet(snippet)
                .with_remediation("Remove hardcoded secret. Use environment variables or a secrets manager.")
                .with_metadata("pattern_name", serde_json::json!(pattern.name));

                findings.push(finding);
            }
        }

        // High-entropy string detection (for strings that don't match patterns)
        findings.extend(self.detect_high_entropy_strings(content, file_path, language));

        findings
    }

    /// Extract the actual secret value from a matched string.
    fn extract_secret_value(matched: &str) -> String {
        // Remove common prefixes and extract the value
        let cleaned = matched
            .split(|c| c == '=' || c == ':')
            .last()
            .unwrap_or(matched)
            .trim()
            .trim_matches(|c| c == '"' || c == '\'' || c == ' ');

        cleaned.to_string()
    }

    /// Get line and column position from byte offset.
    fn get_position(content: &str, offset: usize) -> (usize, usize) {
        let before = &content[..offset.min(content.len())];
        let line = before.chars().filter(|&c| c == '\n').count() + 1;
        let last_newline = before.rfind('\n').map(|i| i + 1).unwrap_or(0);
        let column = offset - last_newline + 1;
        (line, column)
    }

    /// Redact a secret for safe display.
    fn redact_secret(secret: &str) -> String {
        if secret.len() <= 8 {
            return "*".repeat(secret.len());
        }

        let prefix_len = 4.min(secret.len() / 4);
        let suffix_len = 4.min(secret.len() / 4);
        let middle_len = secret.len() - prefix_len - suffix_len;

        format!(
            "{}{}{}",
            &secret[..prefix_len],
            "*".repeat(middle_len),
            &secret[secret.len() - suffix_len..]
        )
    }

    /// Detect high-entropy strings that might be secrets.
    fn detect_high_entropy_strings(
        &self,
        content: &str,
        file_path: &Path,
        language: Language,
    ) -> Vec<Finding> {
        let mut findings = Vec::new();

        // Pattern to find quoted strings (minimum 16 chars for meaningful secrets)
        let string_pattern = Regex::new(r#"['"]([a-zA-Z0-9+/=_-]{16,})['"]"#).unwrap();

        for mat in string_pattern.find_iter(content) {
            let matched = mat.as_str();
            let inner = &matched[1..matched.len() - 1]; // Remove quotes

            // Skip if it looks like a path or URL component
            if inner.contains('/') && !inner.contains("://") {
                continue;
            }

            // Skip if it matches known patterns (already handled)
            if self.patterns.iter().any(|p| p.pattern.is_match(matched)) {
                continue;
            }

            // Skip false positives (UUIDs, git hashes, etc.)
            if self.is_false_positive(inner) {
                continue;
            }

            // Get context (preceding 50 characters)
            let context_start = mat.start().saturating_sub(50);
            let context = &content[context_start..mat.start()];

            // Calculate entropy and get charset-aware threshold
            let entropy = self.calculate_entropy(inner);
            let charset = CharSet::detect(inner);
            let threshold = self.get_threshold_for_charset(charset);

            // Check if high entropy with context weighting
            if self.is_high_entropy_with_context(inner, context) {
                let (line, column) = Self::get_position(content, mat.start());

                let location = Location::new(file_path.to_path_buf(), line, column)
                    .with_language(language);

                let snippet = CodeSnippet::from_content(content, line, 2);

                // Determine confidence based on context weight
                let context_weight = self.calculate_context_weight(context);
                let confidence = if context_weight > 0.5 {
                    Confidence::High
                } else if context_weight > 0.2 {
                    Confidence::Medium
                } else {
                    Confidence::Low
                };

                let finding = Finding::sast(
                    "secret/high-entropy-string",
                    "High Entropy String",
                    format!(
                        "High entropy {} string detected (entropy: {:.2}, threshold: {:.2}): {}",
                        format!("{:?}", charset).to_lowercase(),
                        entropy,
                        threshold,
                        Self::redact_secret(inner)
                    ),
                    location,
                    Severity::Medium,
                )
                .with_confidence(confidence)
                .with_snippet(snippet)
                .with_remediation("Review if this is a hardcoded secret. Use environment variables for secrets.")
                .with_metadata("entropy", serde_json::json!(entropy))
                .with_metadata("threshold", serde_json::json!(threshold))
                .with_metadata("charset", serde_json::json!(format!("{:?}", charset)))
                .with_metadata("context_weight", serde_json::json!(context_weight));

                findings.push(finding);
            }
        }

        findings
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_entropy_calculation() {
        let detector = SecretDetector::new(4.5);

        // Low entropy
        assert!(detector.calculate_entropy("aaaaaaaaaa") < 1.0);

        // High entropy
        let high_entropy = detector.calculate_entropy("aB3$xY9!mK2@pL5#");
        assert!(high_entropy > 3.5);
    }

    #[test]
    fn test_aws_key_detection() {
        let detector = SecretDetector::new(4.5);
        let code = r#"
        aws_access_key = "AKIAIOSFODNN7EXAMPLE"
        aws_secret_key = "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
        "#;

        let findings = detector.detect(code, Path::new("test.py"), Language::Python);
        assert!(
            findings.iter().any(|f| f.title.contains("AWS")),
            "Should detect AWS keys"
        );
    }

    #[test]
    fn test_github_token_detection() {
        let detector = SecretDetector::new(4.5);
        let code = r#"
        token = "ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
        "#;

        let findings = detector.detect(code, Path::new("test.py"), Language::Python);
        assert!(
            findings.iter().any(|f| f.title.contains("GitHub")),
            "Should detect GitHub token"
        );
    }

    #[test]
    fn test_charset_detection() {
        // Hex strings
        assert_eq!(CharSet::detect("deadbeef12345678"), CharSet::Hex);
        assert_eq!(CharSet::detect("DEADBEEF12345678"), CharSet::Hex);

        // Base64 strings
        assert_eq!(CharSet::detect("aGVsbG8gd29ybGQ="), CharSet::Base64);
        assert_eq!(CharSet::detect("abc+def/ghi="), CharSet::Base64);

        // Alphanumeric
        assert_eq!(CharSet::detect("HelloWorld123"), CharSet::Alphanumeric);
    }

    #[test]
    fn test_dynamic_thresholds() {
        let detector = SecretDetector::new(4.5);

        // Hex threshold should be 3.0
        assert_eq!(detector.get_threshold_for_charset(CharSet::Hex), 3.0);

        // Base64 threshold should be 4.5
        assert_eq!(detector.get_threshold_for_charset(CharSet::Base64), 4.5);

        // Alphanumeric threshold should be 3.7
        assert_eq!(detector.get_threshold_for_charset(CharSet::Alphanumeric), 3.7);
    }

    #[test]
    fn test_uuid_filtering() {
        let detector = SecretDetector::new(4.5);

        // UUID should be filtered out
        let uuid = "550e8400-e29b-41d4-a716-446655440000";
        assert!(detector.is_false_positive(uuid), "UUID should be filtered");

        // High entropy non-UUID should not be filtered
        let not_uuid = "aB3xY9mK2pL5qR7sT0uV8wX1";
        assert!(!detector.is_false_positive(not_uuid), "Non-UUID should not be filtered");
    }

    #[test]
    fn test_git_hash_filtering() {
        let detector = SecretDetector::new(4.5);

        // SHA-1 hash (40 chars)
        let sha1 = "da39a3ee5e6b4b0d3255bfef95601890afd80709";
        assert!(detector.is_false_positive(sha1), "SHA-1 hash should be filtered");

        // SHA-256 hash (64 chars)
        let sha256 = "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855";
        assert!(detector.is_false_positive(sha256), "SHA-256 hash should be filtered");
    }

    #[test]
    fn test_context_weighting() {
        let detector = SecretDetector::new(4.5);

        // Context with suspicious keywords should have higher weight
        let suspicious_context = "api_key = ";
        let weight = detector.calculate_context_weight(suspicious_context);
        assert!(weight > 0.0, "Context with 'api' and 'key' should have positive weight");

        // Context with environment patterns should have reduced weight
        let env_context = "os.getenv('API_KEY')";
        let env_weight = detector.calculate_context_weight(env_context);
        // The weight is still positive due to 'api' and 'key', but getenv reduces it
        assert!(env_weight < weight || env_weight >= 0.0, "Env patterns should not increase weight");
    }

    #[test]
    fn test_hex_secret_detection() {
        let detector = SecretDetector::new(4.5);

        // A hex secret with entropy > 3.0 should be detected
        // Using a random-looking hex string
        let code = r#"
        api_secret = "a1b2c3d4e5f67890abcdef1234567890"
        "#;

        let findings = detector.detect(code, Path::new("test.py"), Language::Python);
        // The hex string should be evaluated with the lower 3.0 threshold
        // This is a significant improvement over the 4.5 threshold that would miss all hex secrets
    }

    #[test]
    fn test_placeholder_filtering() {
        let detector = SecretDetector::new(4.5);

        // Placeholder patterns should be filtered
        assert!(detector.is_common_non_secret("xxxxxxxxxxxxxxxxxxxxxxxx"));
        assert!(detector.is_common_non_secret("00000000000000000000"));
        assert!(detector.is_common_non_secret("changeme123456789"));
        assert!(detector.is_common_non_secret("your_api_key_here"));
    }
}

================================================================================
END: src\analyzer\secrets.rs
================================================================================

================================================================================
FILE: src\analyzer\taint.rs
================================================================================
//! Local taint analysis using tree-sitter-graph.
//!
//! This module provides intra-procedural taint tracking to reduce false positives
//! in SAST analysis. It constructs a data flow graph from the AST and traces
//! taint propagation from sources to sinks.

use crate::error::Result;
use crate::models::Language;
use std::collections::{HashMap, HashSet, VecDeque};
use tree_sitter::Node;
use tracing::{debug, trace};

/// Represents a taint source (where untrusted data enters).
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct TaintSource {
    /// Identifier or pattern that marks a source.
    pub pattern: String,
    /// Source category (e.g., "user_input", "network", "file").
    pub category: TaintCategory,
    /// The byte range in source code.
    pub byte_range: (usize, usize),
    /// Variable name if applicable.
    pub variable: Option<String>,
}

/// Represents a taint sink (where tainted data causes vulnerabilities).
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct TaintSink {
    /// Function or pattern that marks a sink.
    pub pattern: String,
    /// Sink category (e.g., "sql_query", "command_exec", "file_write").
    pub category: SinkCategory,
    /// The byte range in source code.
    pub byte_range: (usize, usize),
    /// Which argument index is sensitive (0-indexed).
    pub sensitive_arg: usize,
}

/// Represents a sanitizer (where taint is removed).
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct Sanitizer {
    /// Function or pattern that sanitizes data.
    pub pattern: String,
    /// What kind of taint it removes.
    pub removes: Vec<TaintCategory>,
    /// The byte range in source code.
    pub byte_range: (usize, usize),
}

/// Categories of taint sources.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum TaintCategory {
    /// User input from web requests, CLI args, etc.
    UserInput,
    /// Data from network sources.
    Network,
    /// Data from file system.
    FileSystem,
    /// Data from environment variables.
    Environment,
    /// Data from database queries.
    Database,
    /// Generic untrusted data.
    Untrusted,
}

/// Categories of taint sinks.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum SinkCategory {
    /// SQL query execution.
    SqlQuery,
    /// Command/shell execution.
    CommandExec,
    /// File path operations.
    FilePath,
    /// HTML/template rendering (XSS).
    HtmlOutput,
    /// Code evaluation (eval, exec).
    CodeEval,
    /// Deserialization operations.
    Deserialization,
    /// Log injection.
    LogOutput,
    /// LDAP queries.
    LdapQuery,
    /// XPath queries.
    XPathQuery,
}

/// A node in the taint flow graph.
#[derive(Debug, Clone)]
pub struct TaintNode {
    /// Unique identifier for this node.
    pub id: usize,
    /// The kind of node (variable, call, etc.).
    pub kind: TaintNodeKind,
    /// Variable name if this represents a variable.
    pub name: Option<String>,
    /// Byte range in source.
    pub byte_range: (usize, usize),
    /// Line number (1-indexed).
    pub line: usize,
    /// Current taint status.
    pub tainted: HashSet<TaintCategory>,
}

/// The kind of taint node.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum TaintNodeKind {
    /// A variable definition or reference.
    Variable,
    /// A function/method call.
    Call,
    /// A binary operation (concatenation, etc.).
    BinaryOp,
    /// A parameter definition.
    Parameter,
    /// A return statement.
    Return,
    /// A literal value (safe).
    Literal,
    /// An assignment target.
    Assignment,
}

/// An edge in the taint flow graph representing data flow.
#[derive(Debug, Clone)]
pub struct TaintEdge {
    /// Source node ID.
    pub from: usize,
    /// Target node ID.
    pub to: usize,
    /// Edge type.
    pub kind: TaintEdgeKind,
}

/// The kind of taint edge.
#[derive(Debug, Clone, PartialEq, Eq)]
pub enum TaintEdgeKind {
    /// Direct assignment (x = y).
    Assignment,
    /// Argument passing.
    Argument,
    /// Return value.
    Return,
    /// Data flows through an operation.
    DataFlow,
    /// Implicit flow (control dependence).
    ImplicitFlow,
}

/// The local taint flow graph for a single function/method.
#[derive(Debug, Default)]
pub struct TaintGraph {
    /// All nodes in the graph.
    nodes: Vec<TaintNode>,
    /// All edges in the graph.
    edges: Vec<TaintEdge>,
    /// Map from byte position to node ID for quick lookups.
    position_to_node: HashMap<usize, usize>,
    /// Map from variable name to node IDs.
    variable_nodes: HashMap<String, Vec<usize>>,
}

impl TaintGraph {
    /// Create a new empty taint graph.
    pub fn new() -> Self {
        Self::default()
    }

    /// Add a node to the graph.
    pub fn add_node(&mut self, node: TaintNode) -> usize {
        let id = self.nodes.len();
        self.position_to_node.insert(node.byte_range.0, id);
        if let Some(ref name) = node.name {
            self.variable_nodes
                .entry(name.clone())
                .or_default()
                .push(id);
        }
        self.nodes.push(TaintNode { id, ..node });
        id
    }

    /// Add an edge to the graph.
    pub fn add_edge(&mut self, edge: TaintEdge) {
        self.edges.push(edge);
    }

    /// Get a node by ID.
    pub fn get_node(&self, id: usize) -> Option<&TaintNode> {
        self.nodes.get(id)
    }

    /// Get a mutable node by ID.
    pub fn get_node_mut(&mut self, id: usize) -> Option<&mut TaintNode> {
        self.nodes.get_mut(id)
    }

    /// Find nodes by variable name.
    pub fn find_by_name(&self, name: &str) -> Vec<&TaintNode> {
        self.variable_nodes
            .get(name)
            .map(|ids| ids.iter().filter_map(|&id| self.nodes.get(id)).collect())
            .unwrap_or_default()
    }

    /// Get all edges from a node.
    pub fn edges_from(&self, node_id: usize) -> Vec<&TaintEdge> {
        self.edges.iter().filter(|e| e.from == node_id).collect()
    }

    /// Get all edges to a node.
    pub fn edges_to(&self, node_id: usize) -> Vec<&TaintEdge> {
        self.edges.iter().filter(|e| e.to == node_id).collect()
    }

    /// Propagate taint through the graph using a worklist algorithm.
    pub fn propagate_taint(&mut self, sources: &[TaintSource], sanitizers: &[Sanitizer]) {
        // Initialize taint from sources
        for source in sources {
            if let Some(&node_id) = self.position_to_node.get(&source.byte_range.0) {
                if let Some(node) = self.nodes.get_mut(node_id) {
                    node.tainted.insert(source.category);
                    trace!("Initialized taint at node {} from source {:?}", node_id, source.pattern);
                }
            }
            // Also check by variable name
            if let Some(ref var) = source.variable {
                if let Some(ids) = self.variable_nodes.get(var) {
                    for &id in ids {
                        if let Some(node) = self.nodes.get_mut(id) {
                            node.tainted.insert(source.category);
                        }
                    }
                }
            }
        }

        // Build sanitizer position set
        let sanitizer_positions: HashSet<usize> = sanitizers
            .iter()
            .map(|s| s.byte_range.0)
            .collect();

        // Worklist algorithm for taint propagation
        let mut worklist: VecDeque<usize> = self
            .nodes
            .iter()
            .filter(|n| !n.tainted.is_empty())
            .map(|n| n.id)
            .collect();

        let mut visited: HashSet<usize> = HashSet::new();

        while let Some(node_id) = worklist.pop_front() {
            if visited.contains(&node_id) {
                continue;
            }
            visited.insert(node_id);

            let current_taint: HashSet<TaintCategory> = self
                .nodes
                .get(node_id)
                .map(|n| n.tainted.clone())
                .unwrap_or_default();

            if current_taint.is_empty() {
                continue;
            }

            // Propagate to successors
            let outgoing: Vec<usize> = self
                .edges
                .iter()
                .filter(|e| e.from == node_id)
                .map(|e| e.to)
                .collect();

            for target_id in outgoing {
                // Check if target is sanitized
                if let Some(target) = self.nodes.get(target_id) {
                    if sanitizer_positions.contains(&target.byte_range.0) {
                        // Find which sanitizer and what it removes
                        for sanitizer in sanitizers {
                            if sanitizer.byte_range.0 == target.byte_range.0 {
                                let remaining: HashSet<TaintCategory> = current_taint
                                    .iter()
                                    .filter(|t| !sanitizer.removes.contains(t))
                                    .copied()
                                    .collect();
                                if !remaining.is_empty() {
                                    if let Some(target_node) = self.nodes.get_mut(target_id) {
                                        let old_len = target_node.tainted.len();
                                        target_node.tainted.extend(remaining);
                                        if target_node.tainted.len() > old_len {
                                            worklist.push_back(target_id);
                                        }
                                    }
                                }
                                break;
                            }
                        }
                        continue;
                    }
                }

                // Normal propagation
                if let Some(target_node) = self.nodes.get_mut(target_id) {
                    let old_len = target_node.tainted.len();
                    target_node.tainted.extend(current_taint.iter());
                    if target_node.tainted.len() > old_len {
                        worklist.push_back(target_id);
                    }
                }
            }
        }
    }

    /// Check if a sink receives tainted data.
    pub fn check_sink(&self, sink: &TaintSink) -> Option<Vec<TaintCategory>> {
        // Find the node at the sink position
        if let Some(&node_id) = self.position_to_node.get(&sink.byte_range.0) {
            if let Some(node) = self.nodes.get(node_id) {
                if !node.tainted.is_empty() {
                    return Some(node.tainted.iter().copied().collect());
                }
            }
        }

        // Also check edges going into the sink position
        for edge in &self.edges {
            if let Some(target) = self.nodes.get(edge.to) {
                if target.byte_range.0 >= sink.byte_range.0
                    && target.byte_range.1 <= sink.byte_range.1
                    && !target.tainted.is_empty()
                {
                    return Some(target.tainted.iter().copied().collect());
                }
            }
        }

        None
    }

    /// Get all tainted nodes.
    pub fn tainted_nodes(&self) -> Vec<&TaintNode> {
        self.nodes.iter().filter(|n| !n.tainted.is_empty()).collect()
    }
}

/// Builder for constructing taint graphs from tree-sitter ASTs.
pub struct TaintGraphBuilder {
    /// The language being analyzed.
    language: Language,
    /// Known source patterns for this language.
    source_patterns: Vec<SourcePattern>,
    /// Known sink patterns for this language.
    sink_patterns: Vec<SinkPattern>,
    /// Known sanitizer patterns for this language.
    sanitizer_patterns: Vec<SanitizerPattern>,
}

/// A pattern for identifying taint sources.
#[derive(Debug, Clone)]
pub struct SourcePattern {
    /// Function names or patterns that introduce taint.
    pub functions: Vec<String>,
    /// Parameter positions that are tainted (for callbacks).
    pub tainted_params: Vec<usize>,
    /// The category of taint introduced.
    pub category: TaintCategory,
}

/// A pattern for identifying taint sinks.
#[derive(Debug, Clone)]
pub struct SinkPattern {
    /// Function names that are sinks.
    pub functions: Vec<String>,
    /// Which argument positions are sensitive.
    pub sensitive_args: Vec<usize>,
    /// The category of sink.
    pub category: SinkCategory,
}

/// A pattern for identifying sanitizers.
#[derive(Debug, Clone)]
pub struct SanitizerPattern {
    /// Function names that sanitize.
    pub functions: Vec<String>,
    /// What categories they sanitize.
    pub sanitizes: Vec<TaintCategory>,
}

impl TaintGraphBuilder {
    /// Create a new builder for the given language.
    pub fn new(language: Language) -> Self {
        let (source_patterns, sink_patterns, sanitizer_patterns) = match language {
            Language::Python => Self::python_patterns(),
            Language::JavaScript | Language::TypeScript => Self::javascript_patterns(),
            Language::Rust => Self::rust_patterns(),
            Language::Go => Self::go_patterns(),
            _ => (vec![], vec![], vec![]),
        };

        Self {
            language,
            source_patterns,
            sink_patterns,
            sanitizer_patterns,
        }
    }

    /// Define Python-specific taint patterns.
    fn python_patterns() -> (Vec<SourcePattern>, Vec<SinkPattern>, Vec<SanitizerPattern>) {
        let sources = vec![
            SourcePattern {
                functions: vec![
                    "input".into(),
                    "raw_input".into(),
                    "request.GET.get".into(),
                    "request.POST.get".into(),
                    "request.args.get".into(),
                    "request.form.get".into(),
                    "request.values.get".into(),
                    "request.json".into(),
                    "request.data".into(),
                    "sys.argv".into(),
                    "os.environ.get".into(),
                    "os.getenv".into(),
                ],
                tainted_params: vec![],
                category: TaintCategory::UserInput,
            },
            SourcePattern {
                functions: vec![
                    "open".into(),
                    "read".into(),
                    "readlines".into(),
                    "readline".into(),
                ],
                tainted_params: vec![],
                category: TaintCategory::FileSystem,
            },
        ];

        let sinks = vec![
            SinkPattern {
                functions: vec![
                    "execute".into(),
                    "executemany".into(),
                    "executescript".into(),
                    "raw".into(),
                    "cursor.execute".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::SqlQuery,
            },
            SinkPattern {
                functions: vec![
                    "eval".into(),
                    "exec".into(),
                    "compile".into(),
                    "execfile".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::CodeEval,
            },
            SinkPattern {
                functions: vec![
                    "os.system".into(),
                    "os.popen".into(),
                    "subprocess.call".into(),
                    "subprocess.run".into(),
                    "subprocess.Popen".into(),
                    "commands.getoutput".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::CommandExec,
            },
            SinkPattern {
                functions: vec![
                    "render_template_string".into(),
                    "Markup".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::HtmlOutput,
            },
            SinkPattern {
                functions: vec![
                    "open".into(),
                    "os.path.join".into(),
                    "pathlib.Path".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::FilePath,
            },
        ];

        let sanitizers = vec![
            SanitizerPattern {
                functions: vec![
                    "escape".into(),
                    "html.escape".into(),
                    "markupsafe.escape".into(),
                    "cgi.escape".into(),
                    "bleach.clean".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput],
            },
            SanitizerPattern {
                functions: vec![
                    "int".into(),
                    "float".into(),
                    "bool".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput, TaintCategory::Network],
            },
            SanitizerPattern {
                functions: vec![
                    "shlex.quote".into(),
                    "pipes.quote".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput],
            },
        ];

        (sources, sinks, sanitizers)
    }

    /// Define JavaScript-specific taint patterns.
    fn javascript_patterns() -> (Vec<SourcePattern>, Vec<SinkPattern>, Vec<SanitizerPattern>) {
        let sources = vec![
            SourcePattern {
                functions: vec![
                    "req.query".into(),
                    "req.body".into(),
                    "req.params".into(),
                    "req.headers".into(),
                    "process.argv".into(),
                    "process.env".into(),
                    "document.location".into(),
                    "window.location".into(),
                    "location.search".into(),
                    "location.hash".into(),
                    "document.URL".into(),
                    "document.referrer".into(),
                    "document.cookie".into(),
                ],
                tainted_params: vec![],
                category: TaintCategory::UserInput,
            },
        ];

        let sinks = vec![
            SinkPattern {
                functions: vec![
                    "eval".into(),
                    "Function".into(),
                    "setTimeout".into(),
                    "setInterval".into(),
                    "setImmediate".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::CodeEval,
            },
            SinkPattern {
                functions: vec![
                    "innerHTML".into(),
                    "outerHTML".into(),
                    "document.write".into(),
                    "document.writeln".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::HtmlOutput,
            },
            SinkPattern {
                functions: vec![
                    "exec".into(),
                    "execSync".into(),
                    "spawn".into(),
                    "spawnSync".into(),
                    "execFile".into(),
                    "execFileSync".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::CommandExec,
            },
            SinkPattern {
                functions: vec![
                    "query".into(),
                    "execute".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::SqlQuery,
            },
        ];

        let sanitizers = vec![
            SanitizerPattern {
                functions: vec![
                    "encodeURIComponent".into(),
                    "encodeURI".into(),
                    "escape".into(),
                    "sanitizeHtml".into(),
                    "DOMPurify.sanitize".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput],
            },
            SanitizerPattern {
                functions: vec![
                    "parseInt".into(),
                    "parseFloat".into(),
                    "Number".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput, TaintCategory::Network],
            },
        ];

        (sources, sinks, sanitizers)
    }

    /// Define Rust-specific taint patterns.
    fn rust_patterns() -> (Vec<SourcePattern>, Vec<SinkPattern>, Vec<SanitizerPattern>) {
        let sources = vec![
            SourcePattern {
                functions: vec![
                    "std::env::args".into(),
                    "std::env::var".into(),
                    "std::env::vars".into(),
                    "std::io::stdin".into(),
                    "read_line".into(),
                ],
                tainted_params: vec![],
                category: TaintCategory::UserInput,
            },
            SourcePattern {
                functions: vec![
                    "std::fs::read".into(),
                    "std::fs::read_to_string".into(),
                    "tokio::fs::read".into(),
                ],
                tainted_params: vec![],
                category: TaintCategory::FileSystem,
            },
        ];

        let sinks = vec![
            SinkPattern {
                functions: vec![
                    "std::process::Command::new".into(),
                    "Command::new".into(),
                    "tokio::process::Command::new".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::CommandExec,
            },
            SinkPattern {
                functions: vec![
                    "sqlx::query".into(),
                    "diesel::sql_query".into(),
                    "rusqlite::Connection::execute".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::SqlQuery,
            },
            SinkPattern {
                functions: vec![
                    "std::fs::write".into(),
                    "std::fs::create_dir".into(),
                    "std::fs::remove_file".into(),
                    "std::path::Path::new".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::FilePath,
            },
        ];

        let sanitizers = vec![
            SanitizerPattern {
                functions: vec![
                    "html_escape::encode_text".into(),
                    "askama::MarkupDisplay".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput],
            },
            SanitizerPattern {
                functions: vec![
                    "parse".into(),
                    "from_str".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput],
            },
        ];

        (sources, sinks, sanitizers)
    }

    /// Define Go-specific taint patterns.
    fn go_patterns() -> (Vec<SourcePattern>, Vec<SinkPattern>, Vec<SanitizerPattern>) {
        let sources = vec![
            SourcePattern {
                functions: vec![
                    "http.Request.FormValue".into(),
                    "http.Request.URL.Query".into(),
                    "http.Request.Body".into(),
                    "http.Request.Header.Get".into(),
                    "os.Args".into(),
                    "os.Getenv".into(),
                    "flag.String".into(),
                    "flag.Arg".into(),
                    "bufio.NewReader".into(),
                    "bufio.NewScanner".into(),
                ],
                tainted_params: vec![],
                category: TaintCategory::UserInput,
            },
        ];

        let sinks = vec![
            SinkPattern {
                functions: vec![
                    "exec.Command".into(),
                    "os/exec.Command".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::CommandExec,
            },
            SinkPattern {
                functions: vec![
                    "db.Query".into(),
                    "db.Exec".into(),
                    "db.QueryRow".into(),
                    "sql.DB.Query".into(),
                    "sql.DB.Exec".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::SqlQuery,
            },
            SinkPattern {
                functions: vec![
                    "template.HTML".into(),
                    "http.ResponseWriter.Write".into(),
                    "fmt.Fprintf".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::HtmlOutput,
            },
            SinkPattern {
                functions: vec![
                    "os.Open".into(),
                    "os.Create".into(),
                    "ioutil.ReadFile".into(),
                    "ioutil.WriteFile".into(),
                    "filepath.Join".into(),
                ],
                sensitive_args: vec![0],
                category: SinkCategory::FilePath,
            },
        ];

        let sanitizers = vec![
            SanitizerPattern {
                functions: vec![
                    "html.EscapeString".into(),
                    "template.HTMLEscapeString".into(),
                    "url.QueryEscape".into(),
                    "url.PathEscape".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput],
            },
            SanitizerPattern {
                functions: vec![
                    "strconv.Atoi".into(),
                    "strconv.ParseInt".into(),
                    "strconv.ParseFloat".into(),
                ],
                sanitizes: vec![TaintCategory::UserInput, TaintCategory::Network],
            },
        ];

        (sources, sinks, sanitizers)
    }

    /// Build a taint graph from a parsed tree-sitter tree.
    pub fn build_graph(
        &self,
        tree: &tree_sitter::Tree,
        source_code: &str,
    ) -> Result<(TaintGraph, Vec<TaintSource>, Vec<TaintSink>, Vec<Sanitizer>)> {
        let mut graph = TaintGraph::new();
        let mut sources = Vec::new();
        let mut sinks = Vec::new();
        let mut sanitizers = Vec::new();

        // Walk the tree and build nodes
        self.walk_node(
            tree.root_node(),
            source_code,
            &mut graph,
            &mut sources,
            &mut sinks,
            &mut sanitizers,
        )?;

        Ok((graph, sources, sinks, sanitizers))
    }

    /// Recursively walk tree nodes and build the taint graph.
    fn walk_node(
        &self,
        node: Node,
        source_code: &str,
        graph: &mut TaintGraph,
        sources: &mut Vec<TaintSource>,
        sinks: &mut Vec<TaintSink>,
        sanitizers: &mut Vec<Sanitizer>,
    ) -> Result<Option<usize>> {
        let kind = node.kind();
        let byte_range = (node.start_byte(), node.end_byte());
        let line = node.start_position().row + 1;

        let node_text = node
            .utf8_text(source_code.as_bytes())
            .unwrap_or("")
            .to_string();

        let node_id = match kind {
            // Variable declarations and references
            "identifier" | "variable_name" | "name" => {
                let id = graph.add_node(TaintNode {
                    id: 0,
                    kind: TaintNodeKind::Variable,
                    name: Some(node_text.clone()),
                    byte_range,
                    line,
                    tainted: HashSet::new(),
                });
                Some(id)
            }

            // Function calls - check for sources, sinks, sanitizers
            "call" | "call_expression" | "method_call" | "function_call" => {
                let func_name = self.extract_function_name(&node, source_code);

                // Check if it's a source
                for pattern in &self.source_patterns {
                    if pattern.functions.iter().any(|f| func_name.contains(f)) {
                        sources.push(TaintSource {
                            pattern: func_name.clone(),
                            category: pattern.category,
                            byte_range,
                            variable: None,
                        });
                        debug!("Found taint source: {} at line {}", func_name, line);
                    }
                }

                // Check if it's a sink
                for pattern in &self.sink_patterns {
                    if pattern.functions.iter().any(|f| func_name.contains(f)) {
                        for &arg_idx in &pattern.sensitive_args {
                            sinks.push(TaintSink {
                                pattern: func_name.clone(),
                                category: pattern.category,
                                byte_range,
                                sensitive_arg: arg_idx,
                            });
                        }
                        debug!("Found taint sink: {} at line {}", func_name, line);
                    }
                }

                // Check if it's a sanitizer
                for pattern in &self.sanitizer_patterns {
                    if pattern.functions.iter().any(|f| func_name.contains(f)) {
                        sanitizers.push(Sanitizer {
                            pattern: func_name.clone(),
                            removes: pattern.sanitizes.clone(),
                            byte_range,
                        });
                        debug!("Found sanitizer: {} at line {}", func_name, line);
                    }
                }

                let id = graph.add_node(TaintNode {
                    id: 0,
                    kind: TaintNodeKind::Call,
                    name: Some(func_name),
                    byte_range,
                    line,
                    tainted: HashSet::new(),
                });
                Some(id)
            }

            // Assignments create data flow edges
            "assignment" | "assignment_expression" | "let_declaration" | "variable_declaration"
            | "short_var_declaration" => {
                let id = graph.add_node(TaintNode {
                    id: 0,
                    kind: TaintNodeKind::Assignment,
                    name: None,
                    byte_range,
                    line,
                    tainted: HashSet::new(),
                });

                // Process children and create edges
                let mut child_ids = Vec::new();
                for i in 0..node.named_child_count() {
                    if let Some(child) = node.named_child(i) {
                        if let Some(child_id) =
                            self.walk_node(child, source_code, graph, sources, sinks, sanitizers)?
                        {
                            child_ids.push((i, child_id));
                        }
                    }
                }

                // Create flow from RHS to LHS
                if child_ids.len() >= 2 {
                    let (_, lhs_id) = child_ids[0];
                    for &(idx, rhs_id) in &child_ids[1..] {
                        graph.add_edge(TaintEdge {
                            from: rhs_id,
                            to: lhs_id,
                            kind: TaintEdgeKind::Assignment,
                        });
                    }
                }

                Some(id)
            }

            // Binary operations propagate taint
            "binary_expression" | "binary_operator" | "concatenation" => {
                let id = graph.add_node(TaintNode {
                    id: 0,
                    kind: TaintNodeKind::BinaryOp,
                    name: None,
                    byte_range,
                    line,
                    tainted: HashSet::new(),
                });

                // Both operands flow into the result
                for i in 0..node.named_child_count() {
                    if let Some(child) = node.named_child(i) {
                        if let Some(child_id) =
                            self.walk_node(child, source_code, graph, sources, sinks, sanitizers)?
                        {
                            graph.add_edge(TaintEdge {
                                from: child_id,
                                to: id,
                                kind: TaintEdgeKind::DataFlow,
                            });
                        }
                    }
                }

                Some(id)
            }

            // String literals are safe (no taint)
            "string" | "string_literal" | "raw_string_literal" | "interpreted_string_literal" => {
                let id = graph.add_node(TaintNode {
                    id: 0,
                    kind: TaintNodeKind::Literal,
                    name: None,
                    byte_range,
                    line,
                    tainted: HashSet::new(),
                });
                Some(id)
            }

            // Parameters can be tainted
            "parameter" | "formal_parameter" | "parameter_declaration" => {
                let param_name = self.extract_param_name(&node, source_code);
                let id = graph.add_node(TaintNode {
                    id: 0,
                    kind: TaintNodeKind::Parameter,
                    name: Some(param_name),
                    byte_range,
                    line,
                    tainted: HashSet::new(),
                });
                Some(id)
            }

            // Return statements propagate taint to caller
            "return_statement" | "return_expression" => {
                let id = graph.add_node(TaintNode {
                    id: 0,
                    kind: TaintNodeKind::Return,
                    name: None,
                    byte_range,
                    line,
                    tainted: HashSet::new(),
                });

                // Process return value
                for i in 0..node.named_child_count() {
                    if let Some(child) = node.named_child(i) {
                        if let Some(child_id) =
                            self.walk_node(child, source_code, graph, sources, sinks, sanitizers)?
                        {
                            graph.add_edge(TaintEdge {
                                from: child_id,
                                to: id,
                                kind: TaintEdgeKind::Return,
                            });
                        }
                    }
                }

                Some(id)
            }

            // For other nodes, just recurse into children
            _ => {
                for i in 0..node.named_child_count() {
                    if let Some(child) = node.named_child(i) {
                        self.walk_node(child, source_code, graph, sources, sinks, sanitizers)?;
                    }
                }
                None
            }
        };

        Ok(node_id)
    }

    /// Extract function name from a call node.
    fn extract_function_name(&self, node: &Node, source_code: &str) -> String {
        // Try different child patterns based on language
        for i in 0..node.child_count() {
            if let Some(child) = node.child(i) {
                let kind = child.kind();
                match kind {
                    "identifier" | "property_identifier" | "field_identifier" | "attribute" => {
                        return child
                            .utf8_text(source_code.as_bytes())
                            .unwrap_or("")
                            .to_string();
                    }
                    "member_expression" | "attribute_expression" | "selector_expression" => {
                        // Get the full qualified name
                        return child
                            .utf8_text(source_code.as_bytes())
                            .unwrap_or("")
                            .to_string();
                    }
                    "function" => {
                        return child
                            .utf8_text(source_code.as_bytes())
                            .unwrap_or("")
                            .to_string();
                    }
                    _ => {}
                }
            }
        }

        // Fallback: use first named child
        node.named_child(0)
            .and_then(|c| c.utf8_text(source_code.as_bytes()).ok())
            .unwrap_or("")
            .to_string()
    }

    /// Extract parameter name from a parameter node.
    fn extract_param_name(&self, node: &Node, source_code: &str) -> String {
        for i in 0..node.named_child_count() {
            if let Some(child) = node.named_child(i) {
                if child.kind() == "identifier" || child.kind() == "name" {
                    return child
                        .utf8_text(source_code.as_bytes())
                        .unwrap_or("")
                        .to_string();
                }
            }
        }
        "unknown".to_string()
    }
}

/// Result of taint analysis.
#[derive(Debug)]
pub struct TaintAnalysisResult {
    /// Taint flows from sources to sinks.
    pub flows: Vec<TaintFlow>,
    /// Number of sources found.
    pub source_count: usize,
    /// Number of sinks found.
    pub sink_count: usize,
    /// Number of sanitizers found.
    pub sanitizer_count: usize,
}

/// A complete taint flow from source to sink.
#[derive(Debug)]
pub struct TaintFlow {
    /// The source of taint.
    pub source: TaintSource,
    /// The sink receiving tainted data.
    pub sink: TaintSink,
    /// Categories of taint that reached the sink.
    pub taint_categories: Vec<TaintCategory>,
    /// Line number of source.
    pub source_line: usize,
    /// Line number of sink.
    pub sink_line: usize,
}

/// Perform local taint analysis on a parsed file.
pub fn analyze_taint(
    language: Language,
    tree: &tree_sitter::Tree,
    source_code: &str,
) -> Result<TaintAnalysisResult> {
    let builder = TaintGraphBuilder::new(language);
    let (mut graph, sources, sinks, sanitizers) = builder.build_graph(tree, source_code)?;

    debug!(
        "Built taint graph: {} sources, {} sinks, {} sanitizers",
        sources.len(),
        sinks.len(),
        sanitizers.len()
    );

    // Propagate taint
    graph.propagate_taint(&sources, &sanitizers);

    // Check for flows to sinks
    let mut flows = Vec::new();
    for sink in &sinks {
        if let Some(categories) = graph.check_sink(sink) {
            // Find the corresponding source
            for source in &sources {
                if categories.contains(&source.category) {
                    flows.push(TaintFlow {
                        source: source.clone(),
                        sink: sink.clone(),
                        taint_categories: categories.clone(),
                        source_line: graph
                            .position_to_node
                            .get(&source.byte_range.0)
                            .and_then(|&id| graph.get_node(id))
                            .map(|n| n.line)
                            .unwrap_or(0),
                        sink_line: graph
                            .position_to_node
                            .get(&sink.byte_range.0)
                            .and_then(|&id| graph.get_node(id))
                            .map(|n| n.line)
                            .unwrap_or(0),
                    });
                }
            }
        }
    }

    debug!("Found {} taint flows", flows.len());

    Ok(TaintAnalysisResult {
        flows,
        source_count: sources.len(),
        sink_count: sinks.len(),
        sanitizer_count: sanitizers.len(),
    })
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_python_taint_detection() {
        let code = r#"
user_input = input("Enter query: ")
query = "SELECT * FROM users WHERE id = " + user_input
cursor.execute(query)
"#;

        let mut parser = tree_sitter::Parser::new();
        parser
            .set_language(&tree_sitter_python::LANGUAGE.into())
            .unwrap();
        let tree = parser.parse(code, None).unwrap();

        let result = analyze_taint(Language::Python, &tree, code).unwrap();
        assert!(result.source_count > 0, "Should find input() as source");
        assert!(result.sink_count > 0, "Should find execute() as sink");
    }

    #[test]
    fn test_taint_graph_propagation() {
        let mut graph = TaintGraph::new();

        // Create source node
        let source_id = graph.add_node(TaintNode {
            id: 0,
            kind: TaintNodeKind::Variable,
            name: Some("user_input".into()),
            byte_range: (0, 10),
            line: 1,
            tainted: HashSet::new(),
        });

        // Create intermediate node
        let mid_id = graph.add_node(TaintNode {
            id: 0,
            kind: TaintNodeKind::Variable,
            name: Some("query".into()),
            byte_range: (20, 30),
            line: 2,
            tainted: HashSet::new(),
        });

        // Create sink node
        let sink_id = graph.add_node(TaintNode {
            id: 0,
            kind: TaintNodeKind::Call,
            name: Some("execute".into()),
            byte_range: (40, 50),
            line: 3,
            tainted: HashSet::new(),
        });

        // Add edges
        graph.add_edge(TaintEdge {
            from: source_id,
            to: mid_id,
            kind: TaintEdgeKind::Assignment,
        });
        graph.add_edge(TaintEdge {
            from: mid_id,
            to: sink_id,
            kind: TaintEdgeKind::Argument,
        });

        // Propagate taint
        let sources = vec![TaintSource {
            pattern: "input".into(),
            category: TaintCategory::UserInput,
            byte_range: (0, 10),
            variable: Some("user_input".into()),
        }];

        graph.propagate_taint(&sources, &[]);

        // Check that taint reached the sink
        let sink = TaintSink {
            pattern: "execute".into(),
            category: SinkCategory::SqlQuery,
            byte_range: (40, 50),
            sensitive_arg: 0,
        };

        let result = graph.check_sink(&sink);
        assert!(result.is_some(), "Taint should reach the sink");
        assert!(
            result.unwrap().contains(&TaintCategory::UserInput),
            "UserInput taint should be present"
        );
    }

    #[test]
    fn test_sanitizer_blocks_taint() {
        let mut graph = TaintGraph::new();

        // Create source -> sanitizer -> sink chain
        let source_id = graph.add_node(TaintNode {
            id: 0,
            kind: TaintNodeKind::Variable,
            name: Some("user_input".into()),
            byte_range: (0, 10),
            line: 1,
            tainted: HashSet::new(),
        });

        let sanitizer_id = graph.add_node(TaintNode {
            id: 0,
            kind: TaintNodeKind::Call,
            name: Some("escape".into()),
            byte_range: (20, 30),
            line: 2,
            tainted: HashSet::new(),
        });

        let sink_id = graph.add_node(TaintNode {
            id: 0,
            kind: TaintNodeKind::Call,
            name: Some("execute".into()),
            byte_range: (40, 50),
            line: 3,
            tainted: HashSet::new(),
        });

        graph.add_edge(TaintEdge {
            from: source_id,
            to: sanitizer_id,
            kind: TaintEdgeKind::Argument,
        });
        graph.add_edge(TaintEdge {
            from: sanitizer_id,
            to: sink_id,
            kind: TaintEdgeKind::Argument,
        });

        let sources = vec![TaintSource {
            pattern: "input".into(),
            category: TaintCategory::UserInput,
            byte_range: (0, 10),
            variable: Some("user_input".into()),
        }];

        let sanitizers = vec![Sanitizer {
            pattern: "escape".into(),
            removes: vec![TaintCategory::UserInput],
            byte_range: (20, 30),
        }];

        graph.propagate_taint(&sources, &sanitizers);

        let sink = TaintSink {
            pattern: "execute".into(),
            category: SinkCategory::SqlQuery,
            byte_range: (40, 50),
            sensitive_arg: 0,
        };

        let result = graph.check_sink(&sink);
        assert!(
            result.is_none(),
            "Sanitizer should block taint from reaching sink"
        );
    }
}

================================================================================
END: src\analyzer\taint.rs
================================================================================

================================================================================
FILE: src\analyzer\benchmark.rs
================================================================================
//! Juliet Test Suite benchmark harness for SAST evaluation.
//!
//! The Juliet Test Suite from NIST SAMATE is the de facto standard for
//! evaluating static analysis tools. This module provides:
//! - Test case parsing and categorization (CWE-based)
//! - Ground truth extraction from file naming conventions
//! - Metrics computation (precision, recall, F1, false positive rate)
//! - Benchmark report generation
//!
//! Juliet test case naming convention:
//! - `CWE<num>_<name>__<variant>_<nn>.(c|cpp|java|py)` - bad (vulnerable) case
//! - Files containing `good` in the name - fixed (non-vulnerable) case
//! - Files containing `bad` in the name - vulnerable case

use crate::config::AnalysisConfig;
use crate::error::{AuditorError, Result};
use crate::models::{Confidence, Finding, Language, SourceFile};
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, HashSet};
use std::path::{Path, PathBuf};
use std::time::{Duration, Instant};
use tracing::{debug, info, warn};

/// CWE identifier extracted from Juliet test case.
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct CweId(pub u32);

impl CweId {
    pub fn new(id: u32) -> Self {
        Self(id)
    }

    pub fn as_str(&self) -> String {
        format!("CWE-{}", self.0)
    }
}

impl std::fmt::Display for CweId {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "CWE-{}", self.0)
    }
}

/// Ground truth label for a test case.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Serialize, Deserialize)]
pub enum GroundTruth {
    /// Test case contains a vulnerability (should be detected).
    Vulnerable,
    /// Test case is fixed/secure (should NOT be detected).
    Secure,
    /// Unknown ground truth (not from Juliet).
    Unknown,
}

/// A single Juliet test case.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct JulietTestCase {
    /// Path to the test file.
    pub path: PathBuf,
    /// CWE identifier.
    pub cwe: CweId,
    /// Ground truth label.
    pub ground_truth: GroundTruth,
    /// Variant name (e.g., "char_connect_socket").
    pub variant: String,
    /// Test case number within variant.
    pub case_number: u32,
    /// Language of the test case.
    pub language: Language,
}

impl JulietTestCase {
    /// Parse a Juliet test case from a file path.
    ///
    /// Expected format: `CWE<num>_<name>__<variant>_<nn>.(c|cpp|java|py)`
    pub fn from_path(path: &Path) -> Option<Self> {
        let file_name = path.file_name()?.to_str()?;
        let stem = path.file_stem()?.to_str()?;

        // Detect language from extension
        let ext = path.extension()?.to_str()?;
        let language = match ext {
            "py" => Language::Python,
            "js" => Language::JavaScript,
            "ts" => Language::TypeScript,
            "go" => Language::Go,
            "rs" => Language::Rust,
            "c" | "cpp" | "cc" | "cxx" => Language::C,
            "java" => Language::Java,
            _ => return None,
        };

        // Parse CWE number from filename
        // Format: CWE<num>_<name>__<variant>_<nn>
        if !stem.starts_with("CWE") {
            return None;
        }

        // Extract CWE number
        let cwe_part = stem.strip_prefix("CWE")?;
        let underscore_pos = cwe_part.find('_')?;
        let cwe_num: u32 = cwe_part[..underscore_pos].parse().ok()?;

        // Determine ground truth from filename
        let ground_truth = if file_name.to_lowercase().contains("good")
            || file_name.to_lowercase().contains("_good")
            || file_name.to_lowercase().contains("fixed")
        {
            GroundTruth::Secure
        } else if file_name.to_lowercase().contains("bad")
            || file_name.to_lowercase().contains("_bad")
        {
            GroundTruth::Vulnerable
        } else {
            // Default: files without explicit markers are typically vulnerable
            GroundTruth::Vulnerable
        };

        // Extract variant and case number
        let remaining = &cwe_part[underscore_pos + 1..];
        let variant_parts: Vec<&str> = remaining.split("__").collect();
        let variant = if variant_parts.len() >= 2 {
            variant_parts[1].to_string()
        } else {
            remaining.to_string()
        };

        // Try to extract case number from end
        let case_number = stem
            .rsplit('_')
            .next()
            .and_then(|s| s.parse().ok())
            .unwrap_or(0);

        Some(Self {
            path: path.to_path_buf(),
            cwe: CweId::new(cwe_num),
            ground_truth,
            variant,
            case_number,
            language,
        })
    }
}

/// Result of analyzing a single test case.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct TestCaseResult {
    /// The test case.
    pub test_case: JulietTestCase,
    /// Whether the analyzer detected a vulnerability.
    pub detected: bool,
    /// Findings produced by the analyzer.
    pub findings: Vec<FindingSummary>,
    /// Analysis time.
    pub analysis_time: Duration,
}

impl TestCaseResult {
    /// Check if this result is a true positive.
    pub fn is_true_positive(&self) -> bool {
        self.detected && self.test_case.ground_truth == GroundTruth::Vulnerable
    }

    /// Check if this result is a false positive.
    pub fn is_false_positive(&self) -> bool {
        self.detected && self.test_case.ground_truth == GroundTruth::Secure
    }

    /// Check if this result is a true negative.
    pub fn is_true_negative(&self) -> bool {
        !self.detected && self.test_case.ground_truth == GroundTruth::Secure
    }

    /// Check if this result is a false negative.
    pub fn is_false_negative(&self) -> bool {
        !self.detected && self.test_case.ground_truth == GroundTruth::Vulnerable
    }
}

/// Summary of a finding for serialization.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FindingSummary {
    pub rule_id: String,
    pub severity: String,
    pub confidence: String,
    pub line: usize,
    pub message: String,
}

impl From<&Finding> for FindingSummary {
    fn from(f: &Finding) -> Self {
        Self {
            rule_id: f.rule_id.clone(),
            severity: format!("{:?}", f.severity),
            confidence: format!("{:?}", f.confidence),
            line: f.location.start_line,
            message: f.title.clone(),
        }
    }
}

/// Benchmark metrics for a set of test cases.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct BenchmarkMetrics {
    /// Total number of test cases.
    pub total_cases: usize,
    /// Number of vulnerable test cases.
    pub vulnerable_cases: usize,
    /// Number of secure test cases.
    pub secure_cases: usize,
    /// True positives (correctly identified vulnerabilities).
    pub true_positives: usize,
    /// False positives (incorrectly flagged secure code).
    pub false_positives: usize,
    /// True negatives (correctly identified secure code).
    pub true_negatives: usize,
    /// False negatives (missed vulnerabilities).
    pub false_negatives: usize,
    /// Precision = TP / (TP + FP).
    pub precision: f64,
    /// Recall = TP / (TP + FN).
    pub recall: f64,
    /// F1 score = 2 * (precision * recall) / (precision + recall).
    pub f1_score: f64,
    /// False positive rate = FP / (FP + TN).
    pub false_positive_rate: f64,
    /// Total analysis time.
    pub total_time: Duration,
    /// Average time per test case.
    pub avg_time_per_case: Duration,
}

impl BenchmarkMetrics {
    /// Compute metrics from test case results.
    pub fn compute(results: &[TestCaseResult]) -> Self {
        let total_cases = results.len();
        let mut vulnerable_cases = 0;
        let mut secure_cases = 0;
        let mut true_positives = 0;
        let mut false_positives = 0;
        let mut true_negatives = 0;
        let mut false_negatives = 0;
        let mut total_time = Duration::ZERO;

        for result in results {
            total_time += result.analysis_time;

            match result.test_case.ground_truth {
                GroundTruth::Vulnerable => {
                    vulnerable_cases += 1;
                    if result.detected {
                        true_positives += 1;
                    } else {
                        false_negatives += 1;
                    }
                }
                GroundTruth::Secure => {
                    secure_cases += 1;
                    if result.detected {
                        false_positives += 1;
                    } else {
                        true_negatives += 1;
                    }
                }
                GroundTruth::Unknown => {}
            }
        }

        // Compute derived metrics
        let precision = if true_positives + false_positives > 0 {
            true_positives as f64 / (true_positives + false_positives) as f64
        } else {
            0.0
        };

        let recall = if true_positives + false_negatives > 0 {
            true_positives as f64 / (true_positives + false_negatives) as f64
        } else {
            0.0
        };

        let f1_score = if precision + recall > 0.0 {
            2.0 * precision * recall / (precision + recall)
        } else {
            0.0
        };

        let false_positive_rate = if false_positives + true_negatives > 0 {
            false_positives as f64 / (false_positives + true_negatives) as f64
        } else {
            0.0
        };

        let avg_time_per_case = if total_cases > 0 {
            total_time / total_cases as u32
        } else {
            Duration::ZERO
        };

        Self {
            total_cases,
            vulnerable_cases,
            secure_cases,
            true_positives,
            false_positives,
            true_negatives,
            false_negatives,
            precision,
            recall,
            f1_score,
            false_positive_rate,
            total_time,
            avg_time_per_case,
        }
    }
}

/// Per-CWE breakdown of metrics.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CweMetrics {
    /// CWE identifier.
    pub cwe: CweId,
    /// CWE name (if available).
    pub cwe_name: Option<String>,
    /// Metrics for this CWE.
    pub metrics: BenchmarkMetrics,
}

/// Complete benchmark report.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkReport {
    /// Overall metrics.
    pub overall: BenchmarkMetrics,
    /// Per-CWE metrics.
    pub by_cwe: Vec<CweMetrics>,
    /// Per-language metrics.
    pub by_language: HashMap<String, BenchmarkMetrics>,
    /// Individual test case results.
    pub results: Vec<TestCaseResult>,
    /// Benchmark metadata.
    pub metadata: BenchmarkMetadata,
}

/// Benchmark metadata.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkMetadata {
    /// Juliet test suite version.
    pub juliet_version: Option<String>,
    /// Test suite path.
    pub test_suite_path: PathBuf,
    /// Analyzer version.
    pub analyzer_version: String,
    /// Timestamp.
    pub timestamp: chrono::DateTime<chrono::Utc>,
    /// Configuration used.
    pub config: String,
}

/// Juliet benchmark harness.
pub struct JulietBenchmark {
    /// Test suite root path.
    test_suite_path: PathBuf,
    /// Discovered test cases.
    test_cases: Vec<JulietTestCase>,
    /// CWE name mappings.
    cwe_names: HashMap<u32, String>,
    /// Languages to include in benchmark.
    languages: HashSet<Language>,
    /// CWEs to include (empty = all).
    include_cwes: HashSet<u32>,
    /// Analysis configuration.
    config: AnalysisConfig,
}

impl JulietBenchmark {
    /// Create a new benchmark harness.
    pub fn new(test_suite_path: PathBuf, config: AnalysisConfig) -> Self {
        let mut harness = Self {
            test_suite_path,
            test_cases: Vec::new(),
            cwe_names: Self::default_cwe_names(),
            languages: HashSet::new(),
            include_cwes: HashSet::new(),
            config,
        };

        // Default to supporting all languages
        harness.languages.insert(Language::Python);
        harness.languages.insert(Language::JavaScript);
        harness.languages.insert(Language::Go);
        harness.languages.insert(Language::Rust);

        harness
    }

    /// Set languages to include in benchmark.
    pub fn with_languages(mut self, languages: Vec<Language>) -> Self {
        self.languages = languages.into_iter().collect();
        self
    }

    /// Set CWEs to include in benchmark (empty = all).
    pub fn with_cwes(mut self, cwes: Vec<u32>) -> Self {
        self.include_cwes = cwes.into_iter().collect();
        self
    }

    /// Discover test cases from the test suite directory.
    pub fn discover_test_cases(&mut self) -> Result<usize> {
        info!(
            "Discovering Juliet test cases in {}",
            self.test_suite_path.display()
        );

        self.test_cases.clear();

        // Walk the directory tree
        self.discover_recursive(&self.test_suite_path.clone())?;

        // Filter by language and CWE
        self.test_cases.retain(|tc| {
            let language_ok = self.languages.contains(&tc.language);
            let cwe_ok = self.include_cwes.is_empty() || self.include_cwes.contains(&tc.cwe.0);
            language_ok && cwe_ok
        });

        info!("Discovered {} test cases", self.test_cases.len());
        Ok(self.test_cases.len())
    }

    /// Recursively discover test cases.
    fn discover_recursive(&mut self, dir: &Path) -> Result<()> {
        if !dir.is_dir() {
            return Ok(());
        }

        let entries = std::fs::read_dir(dir)
            .map_err(|e| AuditorError::Analysis(format!("Failed to read directory: {}", e)))?;

        for entry in entries.flatten() {
            let path = entry.path();

            if path.is_dir() {
                self.discover_recursive(&path)?;
            } else if path.is_file() {
                if let Some(test_case) = JulietTestCase::from_path(&path) {
                    self.test_cases.push(test_case);
                }
            }
        }

        Ok(())
    }

    /// Run the benchmark using the provided analyzer function.
    ///
    /// The analyzer function takes a SourceFile and returns findings.
    pub fn run_benchmark<F>(&self, mut analyzer: F) -> Result<BenchmarkReport>
    where
        F: FnMut(&mut SourceFile) -> Result<Vec<Finding>>,
    {
        info!("Running benchmark on {} test cases", self.test_cases.len());

        let mut results = Vec::with_capacity(self.test_cases.len());
        let benchmark_start = Instant::now();

        for test_case in &self.test_cases {
            let result = self.run_single_test(test_case, &mut analyzer)?;
            results.push(result);
        }

        let total_time = benchmark_start.elapsed();

        // Compute overall metrics
        let overall = BenchmarkMetrics::compute(&results);

        // Compute per-CWE metrics
        let by_cwe = self.compute_cwe_metrics(&results);

        // Compute per-language metrics
        let by_language = self.compute_language_metrics(&results);

        // Build metadata
        let metadata = BenchmarkMetadata {
            juliet_version: None,
            test_suite_path: self.test_suite_path.clone(),
            analyzer_version: env!("CARGO_PKG_VERSION").to_string(),
            timestamp: chrono::Utc::now(),
            config: format!("{:?}", self.config),
        };

        info!(
            "Benchmark complete: {:.1}% precision, {:.1}% recall, {:.1}% F1",
            overall.precision * 100.0,
            overall.recall * 100.0,
            overall.f1_score * 100.0
        );

        Ok(BenchmarkReport {
            overall,
            by_cwe,
            by_language,
            results,
            metadata,
        })
    }

    /// Run a single test case.
    fn run_single_test<F>(
        &self,
        test_case: &JulietTestCase,
        analyzer: &mut F,
    ) -> Result<TestCaseResult>
    where
        F: FnMut(&mut SourceFile) -> Result<Vec<Finding>>,
    {
        debug!("Testing {} ({})", test_case.path.display(), test_case.cwe);

        // Read file content
        let content = std::fs::read_to_string(&test_case.path).map_err(|e| {
            AuditorError::Analysis(format!("Failed to read {}: {}", test_case.path.display(), e))
        })?;

        let mut source_file = SourceFile {
            path: test_case.path.clone(),
            absolute_path: test_case.path.clone(),
            language: test_case.language,
            size: content.len() as u64,
            content: Some(content),
        };

        // Run analysis
        let start = Instant::now();
        let findings = analyzer(&mut source_file).unwrap_or_default();
        let analysis_time = start.elapsed();

        // Check if any findings match the expected CWE
        let detected = self.check_detection(&findings, test_case);

        // Convert findings to summaries
        let finding_summaries: Vec<FindingSummary> = findings.iter().map(|f| f.into()).collect();

        Ok(TestCaseResult {
            test_case: test_case.clone(),
            detected,
            findings: finding_summaries,
            analysis_time,
        })
    }

    /// Check if findings indicate detection of the expected vulnerability.
    fn check_detection(&self, findings: &[Finding], test_case: &JulietTestCase) -> bool {
        if findings.is_empty() {
            return false;
        }

        // Check for CWE match in findings
        for finding in findings {
            // Check metadata for CWE
            if let Some(cwe_value) = finding.metadata.get("cwe") {
                if let Some(cwe_str) = cwe_value.as_str() {
                    // Handle formats like "CWE-89" or "89"
                    let cwe_num = cwe_str
                        .strip_prefix("CWE-")
                        .unwrap_or(cwe_str)
                        .parse::<u32>()
                        .unwrap_or(0);

                    if cwe_num == test_case.cwe.0 {
                        return true;
                    }
                }
            }

            // Check rule ID for CWE reference
            let cwe_in_id = test_case.cwe.as_str().to_lowercase();
            if finding.rule_id.to_lowercase().contains(&cwe_in_id) {
                return true;
            }

            // For medium/high confidence findings, count as detection
            // even without exact CWE match
            if matches!(finding.confidence, Confidence::High | Confidence::Medium) {
                return true;
            }
        }

        // Any finding on a vulnerable case counts as detection
        // (conservative approach - may overcount)
        !findings.is_empty()
    }

    /// Compute per-CWE metrics.
    fn compute_cwe_metrics(&self, results: &[TestCaseResult]) -> Vec<CweMetrics> {
        let mut by_cwe: HashMap<CweId, Vec<&TestCaseResult>> = HashMap::new();

        for result in results {
            by_cwe
                .entry(result.test_case.cwe.clone())
                .or_default()
                .push(result);
        }

        let mut cwe_metrics: Vec<CweMetrics> = by_cwe
            .into_iter()
            .map(|(cwe, cwe_results)| {
                let cwe_name = self.cwe_names.get(&cwe.0).cloned();
                let results_owned: Vec<TestCaseResult> =
                    cwe_results.into_iter().cloned().collect();
                let metrics = BenchmarkMetrics::compute(&results_owned);

                CweMetrics {
                    cwe,
                    cwe_name,
                    metrics,
                }
            })
            .collect();

        // Sort by CWE number
        cwe_metrics.sort_by_key(|m| m.cwe.0);

        cwe_metrics
    }

    /// Compute per-language metrics.
    fn compute_language_metrics(&self, results: &[TestCaseResult]) -> HashMap<String, BenchmarkMetrics> {
        let mut by_language: HashMap<Language, Vec<&TestCaseResult>> = HashMap::new();

        for result in results {
            by_language
                .entry(result.test_case.language)
                .or_default()
                .push(result);
        }

        by_language
            .into_iter()
            .map(|(lang, lang_results)| {
                let results_owned: Vec<TestCaseResult> =
                    lang_results.into_iter().cloned().collect();
                let metrics = BenchmarkMetrics::compute(&results_owned);
                (format!("{:?}", lang), metrics)
            })
            .collect()
    }

    /// Get default CWE name mappings for common vulnerability types.
    fn default_cwe_names() -> HashMap<u32, String> {
        let mut names = HashMap::new();

        // Injection vulnerabilities
        names.insert(78, "OS Command Injection".to_string());
        names.insert(89, "SQL Injection".to_string());
        names.insert(90, "LDAP Injection".to_string());
        names.insert(91, "XML Injection".to_string());
        names.insert(94, "Code Injection".to_string());
        names.insert(95, "Eval Injection".to_string());

        // XSS
        names.insert(79, "Cross-site Scripting (XSS)".to_string());
        names.insert(80, "Basic XSS".to_string());

        // Path traversal
        names.insert(22, "Path Traversal".to_string());
        names.insert(23, "Relative Path Traversal".to_string());
        names.insert(36, "Absolute Path Traversal".to_string());

        // Authentication
        names.insert(259, "Hard-coded Password".to_string());
        names.insert(321, "Hard-coded Cryptographic Key".to_string());
        names.insert(798, "Hard-coded Credentials".to_string());

        // Cryptography
        names.insert(326, "Inadequate Encryption Strength".to_string());
        names.insert(327, "Broken Crypto Algorithm".to_string());
        names.insert(328, "Weak Hash".to_string());
        names.insert(330, "Insufficient Randomness".to_string());

        // Memory safety
        names.insert(119, "Buffer Overflow".to_string());
        names.insert(120, "Buffer Copy without Size Check".to_string());
        names.insert(121, "Stack-based Buffer Overflow".to_string());
        names.insert(122, "Heap-based Buffer Overflow".to_string());
        names.insert(125, "Out-of-bounds Read".to_string());
        names.insert(126, "Buffer Over-read".to_string());
        names.insert(127, "Buffer Under-read".to_string());
        names.insert(415, "Double Free".to_string());
        names.insert(416, "Use After Free".to_string());

        // Race conditions
        names.insert(362, "Race Condition".to_string());
        names.insert(366, "Race in Signal Handler".to_string());
        names.insert(367, "TOCTOU Race Condition".to_string());

        // Integer issues
        names.insert(190, "Integer Overflow".to_string());
        names.insert(191, "Integer Underflow".to_string());
        names.insert(681, "Numeric Truncation".to_string());

        // Null pointer
        names.insert(476, "NULL Pointer Dereference".to_string());

        // Deserialization
        names.insert(502, "Unsafe Deserialization".to_string());

        // XXE
        names.insert(611, "XML External Entity (XXE)".to_string());

        // SSRF
        names.insert(918, "Server-Side Request Forgery".to_string());

        names
    }

    /// Get the number of discovered test cases.
    pub fn test_case_count(&self) -> usize {
        self.test_cases.len()
    }

    /// Get discovered test cases.
    pub fn test_cases(&self) -> &[JulietTestCase] {
        &self.test_cases
    }
}

/// Format a benchmark report as a text summary.
pub fn format_benchmark_summary(report: &BenchmarkReport) -> String {
    let mut output = String::new();

    output.push_str("\n");
    output.push_str("               JULIET BENCHMARK RESULTS                           \n");
    output.push_str("\n");

    // Overall metrics
    output.push_str(&format!(
        " Total Test Cases:    {:>6}                                     \n",
        report.overall.total_cases
    ));
    output.push_str(&format!(
        " Vulnerable Cases:    {:>6}                                     \n",
        report.overall.vulnerable_cases
    ));
    output.push_str(&format!(
        " Secure Cases:        {:>6}                                     \n",
        report.overall.secure_cases
    ));
    output.push_str("\n");
    output.push_str(&format!(
        " True Positives:      {:>6}                                     \n",
        report.overall.true_positives
    ));
    output.push_str(&format!(
        " False Positives:     {:>6}                                     \n",
        report.overall.false_positives
    ));
    output.push_str(&format!(
        " True Negatives:      {:>6}                                     \n",
        report.overall.true_negatives
    ));
    output.push_str(&format!(
        " False Negatives:     {:>6}                                     \n",
        report.overall.false_negatives
    ));
    output.push_str("\n");
    output.push_str(&format!(
        " Precision:           {:>6.1}%                                    \n",
        report.overall.precision * 100.0
    ));
    output.push_str(&format!(
        " Recall:              {:>6.1}%                                    \n",
        report.overall.recall * 100.0
    ));
    output.push_str(&format!(
        " F1 Score:            {:>6.1}%                                    \n",
        report.overall.f1_score * 100.0
    ));
    output.push_str(&format!(
        " False Positive Rate: {:>6.1}%                                    \n",
        report.overall.false_positive_rate * 100.0
    ));
    output.push_str("\n");
    output.push_str(&format!(
        " Total Analysis Time: {:>6.2}s                                    \n",
        report.overall.total_time.as_secs_f64()
    ));
    output.push_str(&format!(
        " Avg Time Per Case:   {:>6.2}ms                                   \n",
        report.overall.avg_time_per_case.as_secs_f64() * 1000.0
    ));
    output.push_str("\n");

    // Top CWEs by recall
    output.push_str("\nTop 10 CWEs by Recall:\n");
    output.push_str("\n");

    let mut sorted_cwes = report.by_cwe.clone();
    sorted_cwes.sort_by(|a, b| {
        b.metrics
            .recall
            .partial_cmp(&a.metrics.recall)
            .unwrap_or(std::cmp::Ordering::Equal)
    });

    for cwe_metric in sorted_cwes.iter().take(10) {
        let name = cwe_metric
            .cwe_name
            .clone()
            .unwrap_or_else(|| "Unknown".to_string());
        output.push_str(&format!(
            "  {:8} ({:25}) - Recall: {:5.1}%, Precision: {:5.1}%\n",
            cwe_metric.cwe,
            if name.len() > 25 { &name[..25] } else { &name },
            cwe_metric.metrics.recall * 100.0,
            cwe_metric.metrics.precision * 100.0
        ));
    }

    // Per-language breakdown
    output.push_str("\nPer-Language Metrics:\n");
    output.push_str("\n");

    for (lang, metrics) in &report.by_language {
        output.push_str(&format!(
            "  {:12} - Cases: {:5}, Precision: {:5.1}%, Recall: {:5.1}%, F1: {:5.1}%\n",
            lang,
            metrics.total_cases,
            metrics.precision * 100.0,
            metrics.recall * 100.0,
            metrics.f1_score * 100.0
        ));
    }

    output
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_juliet_path_parsing() {
        let test_cases = vec![
            (
                "CWE89_SQL_Injection__basic_01_bad.py",
                Some((89, GroundTruth::Vulnerable)),
            ),
            (
                "CWE89_SQL_Injection__basic_01_good.py",
                Some((89, GroundTruth::Secure)),
            ),
            (
                "CWE78_OS_Command_Injection__basic_01.py",
                Some((78, GroundTruth::Vulnerable)),
            ),
            ("not_a_cwe_file.py", None),
            ("README.md", None),
        ];

        for (filename, expected) in test_cases {
            let path = PathBuf::from(filename);
            let result = JulietTestCase::from_path(&path);

            match expected {
                Some((cwe, ground_truth)) => {
                    assert!(result.is_some(), "Expected to parse: {}", filename);
                    let tc = result.unwrap();
                    assert_eq!(tc.cwe.0, cwe, "CWE mismatch for {}", filename);
                    assert_eq!(tc.ground_truth, ground_truth, "Ground truth mismatch for {}", filename);
                }
                None => {
                    assert!(result.is_none(), "Should not parse: {}", filename);
                }
            }
        }
    }

    #[test]
    fn test_metrics_computation() {
        let test_cases = vec![
            (GroundTruth::Vulnerable, true),  // TP
            (GroundTruth::Vulnerable, true),  // TP
            (GroundTruth::Vulnerable, false), // FN
            (GroundTruth::Secure, false),     // TN
            (GroundTruth::Secure, true),      // FP
        ];

        let results: Vec<TestCaseResult> = test_cases
            .into_iter()
            .enumerate()
            .map(|(i, (gt, detected))| TestCaseResult {
                test_case: JulietTestCase {
                    path: PathBuf::from(format!("test_{}.py", i)),
                    cwe: CweId::new(89),
                    ground_truth: gt,
                    variant: "test".to_string(),
                    case_number: i as u32,
                    language: Language::Python,
                },
                detected,
                findings: vec![],
                analysis_time: Duration::from_millis(10),
            })
            .collect();

        let metrics = BenchmarkMetrics::compute(&results);

        assert_eq!(metrics.total_cases, 5);
        assert_eq!(metrics.vulnerable_cases, 3);
        assert_eq!(metrics.secure_cases, 2);
        assert_eq!(metrics.true_positives, 2);
        assert_eq!(metrics.false_positives, 1);
        assert_eq!(metrics.true_negatives, 1);
        assert_eq!(metrics.false_negatives, 1);

        // Precision = 2 / (2 + 1) = 0.667
        assert!((metrics.precision - 0.667).abs() < 0.01);

        // Recall = 2 / (2 + 1) = 0.667
        assert!((metrics.recall - 0.667).abs() < 0.01);
    }

    #[test]
    fn test_cwe_id_display() {
        let cwe = CweId::new(89);
        assert_eq!(cwe.to_string(), "CWE-89");
        assert_eq!(cwe.as_str(), "CWE-89");
    }
}

================================================================================
END: src\analyzer\benchmark.rs
================================================================================

================================================================================
FILE: src\analyzer\name_resolution.rs
================================================================================
//! Cross-file name resolution using stack-graphs.
//!
//! This module provides inter-procedural analysis capabilities by building
//! stack-graphs that resolve names across file boundaries. This enables:
//! - Tracking data flow through function calls
//! - Resolving imported symbols to their definitions
//! - Cross-module taint propagation
//!
//! Stack-graphs were developed by GitHub for code navigation and provide
//! incremental, precise name resolution without requiring a full type system.

use crate::error::{AuditorError, Result};
use crate::models::Language;
use stack_graphs::arena::Handle;
use stack_graphs::graph::{Node, NodeID, StackGraph};
use stack_graphs::partial::PartialPaths;
use stack_graphs::stitching::{Database, ForwardPartialPathStitcher, GraphEdgeCandidates, StitcherConfig};
use std::collections::{HashMap, HashSet};
use std::path::{Path, PathBuf};
use tracing::{debug, info, warn};

/// A resolved reference to a symbol definition.
#[derive(Debug, Clone)]
pub struct ResolvedReference {
    /// The file where the reference occurs.
    pub reference_file: PathBuf,
    /// Line number of the reference.
    pub reference_line: usize,
    /// The symbol name being referenced.
    pub symbol: String,
    /// The file where the definition is located.
    pub definition_file: PathBuf,
    /// Line number of the definition.
    pub definition_line: usize,
    /// Kind of definition (function, class, variable, etc.).
    pub definition_kind: DefinitionKind,
}

/// The kind of symbol definition.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DefinitionKind {
    /// Function or method definition.
    Function,
    /// Class or struct definition.
    Class,
    /// Variable or constant.
    Variable,
    /// Module or namespace.
    Module,
    /// Import/export binding.
    Import,
    /// Unknown or other.
    Unknown,
}

/// Cross-file name resolution context.
#[derive(Debug, Clone)]
pub struct ResolutionContext {
    /// File where the query originates.
    pub file: PathBuf,
    /// Symbol being resolved.
    pub symbol: String,
    /// Line number of the reference.
    pub line: usize,
    /// Column number of the reference.
    pub column: usize,
}

/// Cross-file symbol index entry.
#[derive(Debug, Clone)]
pub struct SymbolEntry {
    /// File containing the symbol.
    pub file: PathBuf,
    /// Symbol name (possibly qualified).
    pub name: String,
    /// Kind of symbol.
    pub kind: DefinitionKind,
    /// Line number.
    pub line: usize,
    /// Column number.
    pub column: usize,
    /// Whether this symbol is exported.
    pub exported: bool,
}

/// Cross-file name resolver using stack-graphs.
///
/// This builds an incremental index of symbol definitions and references
/// across multiple files, enabling cross-file data flow analysis.
pub struct NameResolver {
    /// The underlying stack graph.
    graph: StackGraph,
    /// Partial paths database for incremental resolution.
    partials: PartialPaths,
    /// Database for path stitching.
    database: Database,
    /// File handles in the graph.
    file_handles: HashMap<PathBuf, Handle<stack_graphs::graph::File>>,
    /// Symbol index for fast lookups.
    symbol_index: HashMap<String, Vec<SymbolEntry>>,
    /// Languages supported.
    languages: HashSet<Language>,
}

impl Default for NameResolver {
    fn default() -> Self {
        Self::new()
    }
}

impl NameResolver {
    /// Create a new name resolver.
    pub fn new() -> Self {
        let graph = StackGraph::new();
        let partials = PartialPaths::new();
        let database = Database::new();

        let mut languages = HashSet::new();
        languages.insert(Language::Python);
        languages.insert(Language::JavaScript);
        languages.insert(Language::Rust);
        languages.insert(Language::Go);

        Self {
            graph,
            partials,
            database,
            file_handles: HashMap::new(),
            symbol_index: HashMap::new(),
            languages,
        }
    }

    /// Check if a language is supported.
    pub fn supports_language(&self, language: Language) -> bool {
        self.languages.contains(&language)
    }

    /// Index a file's symbols into the graph.
    ///
    /// This parses the file, extracts symbol definitions and references,
    /// and adds them to the stack graph for later resolution.
    pub fn index_file(&mut self, path: &Path, content: &str, language: Language) -> Result<usize> {
        if !self.supports_language(language) {
            return Ok(0);
        }

        debug!("Indexing symbols in {}", path.display());

        // Get or create file handle
        let file_name = path
            .to_string_lossy()
            .to_string();
        let file_handle = self.graph.get_or_create_file(&file_name);
        self.file_handles.insert(path.to_path_buf(), file_handle);

        // Extract symbols based on language
        let symbols = match language {
            Language::Python => self.extract_python_symbols(path, content)?,
            Language::JavaScript | Language::TypeScript => {
                self.extract_javascript_symbols(path, content)?
            }
            Language::Rust => self.extract_rust_symbols(path, content)?,
            Language::Go => self.extract_go_symbols(path, content)?,
            _ => Vec::new(),
        };

        // Add symbols to index
        let count = symbols.len();
        for symbol in symbols {
            self.symbol_index
                .entry(symbol.name.clone())
                .or_default()
                .push(symbol);
        }

        debug!("Indexed {} symbols from {}", count, path.display());
        Ok(count)
    }

    /// Extract symbols from Python source code.
    fn extract_python_symbols(&self, path: &Path, content: &str) -> Result<Vec<SymbolEntry>> {
        let mut symbols = Vec::new();
        let mut parser = tree_sitter::Parser::new();
        parser
            .set_language(&tree_sitter_python::LANGUAGE.into())
            .map_err(|e| AuditorError::Parse(format!("Failed to set Python language: {}", e)))?;

        let tree = parser
            .parse(content, None)
            .ok_or_else(|| AuditorError::Parse("Failed to parse Python file".to_string()))?;

        self.walk_python_tree(path, content, tree.root_node(), &mut symbols, true);
        Ok(symbols)
    }

    /// Walk Python AST to extract symbols.
    fn walk_python_tree(
        &self,
        path: &Path,
        content: &str,
        node: tree_sitter::Node,
        symbols: &mut Vec<SymbolEntry>,
        is_top_level: bool,
    ) {
        match node.kind() {
            "function_definition" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Function,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported: is_top_level,
                        });
                    }
                }
            }
            "class_definition" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Class,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported: is_top_level,
                        });
                    }
                }
            }
            "import_statement" | "import_from_statement" => {
                // Track imports
                for child in node.children(&mut node.walk()) {
                    if child.kind() == "dotted_name" || child.kind() == "aliased_import" {
                        if let Ok(text) = child.utf8_text(content.as_bytes()) {
                            let name = if let Some(alias_pos) = text.find(" as ") {
                                text[alias_pos + 4..].trim().to_string()
                            } else {
                                text.split('.').last().unwrap_or(text).to_string()
                            };

                            symbols.push(SymbolEntry {
                                file: path.to_path_buf(),
                                name,
                                kind: DefinitionKind::Import,
                                line: child.start_position().row + 1,
                                column: child.start_position().column + 1,
                                exported: false,
                            });
                        }
                    }
                }
            }
            "assignment" => {
                // Track module-level variable assignments
                if is_top_level {
                    if let Some(left) = node.child_by_field_name("left") {
                        if left.kind() == "identifier" {
                            if let Ok(name) = left.utf8_text(content.as_bytes()) {
                                symbols.push(SymbolEntry {
                                    file: path.to_path_buf(),
                                    name: name.to_string(),
                                    kind: DefinitionKind::Variable,
                                    line: left.start_position().row + 1,
                                    column: left.start_position().column + 1,
                                    exported: true,
                                });
                            }
                        }
                    }
                }
            }
            _ => {}
        }

        // Recurse into children
        let mut cursor = node.walk();
        for child in node.children(&mut cursor) {
            let child_is_top_level = is_top_level && node.kind() == "module";
            self.walk_python_tree(path, content, child, symbols, child_is_top_level);
        }
    }

    /// Extract symbols from JavaScript/TypeScript source code.
    fn extract_javascript_symbols(&self, path: &Path, content: &str) -> Result<Vec<SymbolEntry>> {
        let mut symbols = Vec::new();
        let mut parser = tree_sitter::Parser::new();
        parser
            .set_language(&tree_sitter_javascript::LANGUAGE.into())
            .map_err(|e| AuditorError::Parse(format!("Failed to set JS language: {}", e)))?;

        let tree = parser
            .parse(content, None)
            .ok_or_else(|| AuditorError::Parse("Failed to parse JavaScript file".to_string()))?;

        self.walk_javascript_tree(path, content, tree.root_node(), &mut symbols, true, false);
        Ok(symbols)
    }

    /// Walk JavaScript AST to extract symbols.
    fn walk_javascript_tree(
        &self,
        path: &Path,
        content: &str,
        node: tree_sitter::Node,
        symbols: &mut Vec<SymbolEntry>,
        is_top_level: bool,
        is_exported: bool,
    ) {
        match node.kind() {
            "function_declaration" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Function,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported: is_exported || is_top_level,
                        });
                    }
                }
            }
            "class_declaration" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Class,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported: is_exported || is_top_level,
                        });
                    }
                }
            }
            "lexical_declaration" | "variable_declaration" => {
                // Track const/let/var declarations
                for child in node.children(&mut node.walk()) {
                    if child.kind() == "variable_declarator" {
                        if let Some(name_node) = child.child_by_field_name("name") {
                            if name_node.kind() == "identifier" {
                                if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                                    symbols.push(SymbolEntry {
                                        file: path.to_path_buf(),
                                        name: name.to_string(),
                                        kind: DefinitionKind::Variable,
                                        line: name_node.start_position().row + 1,
                                        column: name_node.start_position().column + 1,
                                        exported: is_exported,
                                    });
                                }
                            }
                        }
                    }
                }
            }
            "export_statement" => {
                // Mark children as exported
                let mut cursor = node.walk();
                for child in node.children(&mut cursor) {
                    self.walk_javascript_tree(path, content, child, symbols, false, true);
                }
                return;
            }
            "import_statement" => {
                // Track imports
                for child in node.children(&mut node.walk()) {
                    if child.kind() == "import_specifier" || child.kind() == "identifier" {
                        if let Ok(name) = child.utf8_text(content.as_bytes()) {
                            let name = if let Some(alias_pos) = name.find(" as ") {
                                name[alias_pos + 4..].trim().to_string()
                            } else {
                                name.to_string()
                            };

                            symbols.push(SymbolEntry {
                                file: path.to_path_buf(),
                                name,
                                kind: DefinitionKind::Import,
                                line: child.start_position().row + 1,
                                column: child.start_position().column + 1,
                                exported: false,
                            });
                        }
                    }
                }
            }
            _ => {}
        }

        // Recurse into children
        let mut cursor = node.walk();
        for child in node.children(&mut cursor) {
            let child_is_top_level = is_top_level && node.kind() == "program";
            self.walk_javascript_tree(path, content, child, symbols, child_is_top_level, false);
        }
    }

    /// Extract symbols from Rust source code.
    fn extract_rust_symbols(&self, path: &Path, content: &str) -> Result<Vec<SymbolEntry>> {
        let mut symbols = Vec::new();
        let mut parser = tree_sitter::Parser::new();
        parser
            .set_language(&tree_sitter_rust::LANGUAGE.into())
            .map_err(|e| AuditorError::Parse(format!("Failed to set Rust language: {}", e)))?;

        let tree = parser
            .parse(content, None)
            .ok_or_else(|| AuditorError::Parse("Failed to parse Rust file".to_string()))?;

        self.walk_rust_tree(path, content, tree.root_node(), &mut symbols, true, false);
        Ok(symbols)
    }

    /// Walk Rust AST to extract symbols.
    fn walk_rust_tree(
        &self,
        path: &Path,
        content: &str,
        node: tree_sitter::Node,
        symbols: &mut Vec<SymbolEntry>,
        is_top_level: bool,
        is_pub: bool,
    ) {
        // Check for pub modifier
        let has_pub = node.children(&mut node.walk()).any(|c| {
            c.kind() == "visibility_modifier"
                && c.utf8_text(content.as_bytes())
                    .map(|t| t.starts_with("pub"))
                    .unwrap_or(false)
        });

        match node.kind() {
            "function_item" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Function,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported: has_pub || is_pub,
                        });
                    }
                }
            }
            "struct_item" | "enum_item" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Class,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported: has_pub || is_pub,
                        });
                    }
                }
            }
            "impl_item" => {
                // Track impl blocks - extract methods
                if let Some(body) = node.child_by_field_name("body") {
                    let mut cursor = body.walk();
                    for child in body.children(&mut cursor) {
                        self.walk_rust_tree(path, content, child, symbols, false, has_pub);
                    }
                }
                return;
            }
            "mod_item" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Module,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported: has_pub,
                        });
                    }
                }
            }
            "const_item" | "static_item" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Variable,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported: has_pub || is_pub,
                        });
                    }
                }
            }
            "use_declaration" => {
                // Track use imports
                for child in node.children(&mut node.walk()) {
                    if child.kind() == "use_tree" || child.kind() == "scoped_identifier" {
                        self.extract_rust_use_tree(path, content, child, symbols, has_pub);
                    }
                }
            }
            _ => {}
        }

        // Recurse into children
        let mut cursor = node.walk();
        for child in node.children(&mut cursor) {
            let child_is_top_level = is_top_level && node.kind() == "source_file";
            self.walk_rust_tree(path, content, child, symbols, child_is_top_level, false);
        }
    }

    /// Extract symbols from Rust use trees.
    fn extract_rust_use_tree(
        &self,
        path: &Path,
        content: &str,
        node: tree_sitter::Node,
        symbols: &mut Vec<SymbolEntry>,
        is_pub: bool,
    ) {
        if let Ok(text) = node.utf8_text(content.as_bytes()) {
            // Get the final name being imported
            let name = if let Some(alias) = text.strip_suffix(" as _") {
                return; // Skip wildcard imports
            } else if let Some((_, alias)) = text.rsplit_once(" as ") {
                alias.trim().to_string()
            } else {
                text.rsplit("::").next().unwrap_or(text).to_string()
            };

            if !name.is_empty() && name != "*" && name != "{" {
                symbols.push(SymbolEntry {
                    file: path.to_path_buf(),
                    name,
                    kind: DefinitionKind::Import,
                    line: node.start_position().row + 1,
                    column: node.start_position().column + 1,
                    exported: is_pub,
                });
            }
        }
    }

    /// Extract symbols from Go source code.
    fn extract_go_symbols(&self, path: &Path, content: &str) -> Result<Vec<SymbolEntry>> {
        let mut symbols = Vec::new();
        let mut parser = tree_sitter::Parser::new();
        parser
            .set_language(&tree_sitter_go::LANGUAGE.into())
            .map_err(|e| AuditorError::Parse(format!("Failed to set Go language: {}", e)))?;

        let tree = parser
            .parse(content, None)
            .ok_or_else(|| AuditorError::Parse("Failed to parse Go file".to_string()))?;

        self.walk_go_tree(path, content, tree.root_node(), &mut symbols);
        Ok(symbols)
    }

    /// Walk Go AST to extract symbols.
    fn walk_go_tree(
        &self,
        path: &Path,
        content: &str,
        node: tree_sitter::Node,
        symbols: &mut Vec<SymbolEntry>,
    ) {
        match node.kind() {
            "function_declaration" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        // In Go, exported symbols start with uppercase
                        let exported = name.chars().next().map(|c| c.is_uppercase()).unwrap_or(false);
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Function,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported,
                        });
                    }
                }
            }
            "method_declaration" => {
                if let Some(name_node) = node.child_by_field_name("name") {
                    if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                        let exported = name.chars().next().map(|c| c.is_uppercase()).unwrap_or(false);
                        symbols.push(SymbolEntry {
                            file: path.to_path_buf(),
                            name: name.to_string(),
                            kind: DefinitionKind::Function,
                            line: name_node.start_position().row + 1,
                            column: name_node.start_position().column + 1,
                            exported,
                        });
                    }
                }
            }
            "type_declaration" => {
                // Handle type Foo struct {...}
                for child in node.children(&mut node.walk()) {
                    if child.kind() == "type_spec" {
                        if let Some(name_node) = child.child_by_field_name("name") {
                            if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                                let exported =
                                    name.chars().next().map(|c| c.is_uppercase()).unwrap_or(false);
                                symbols.push(SymbolEntry {
                                    file: path.to_path_buf(),
                                    name: name.to_string(),
                                    kind: DefinitionKind::Class,
                                    line: name_node.start_position().row + 1,
                                    column: name_node.start_position().column + 1,
                                    exported,
                                });
                            }
                        }
                    }
                }
            }
            "const_declaration" | "var_declaration" => {
                // Track package-level constants and variables
                for child in node.children(&mut node.walk()) {
                    if child.kind() == "const_spec" || child.kind() == "var_spec" {
                        if let Some(name_node) = child.child_by_field_name("name") {
                            if let Ok(name) = name_node.utf8_text(content.as_bytes()) {
                                let exported =
                                    name.chars().next().map(|c| c.is_uppercase()).unwrap_or(false);
                                symbols.push(SymbolEntry {
                                    file: path.to_path_buf(),
                                    name: name.to_string(),
                                    kind: DefinitionKind::Variable,
                                    line: name_node.start_position().row + 1,
                                    column: name_node.start_position().column + 1,
                                    exported,
                                });
                            }
                        }
                    }
                }
            }
            "import_declaration" => {
                for child in node.children(&mut node.walk()) {
                    if child.kind() == "import_spec" {
                        if let Some(path_node) = child.child_by_field_name("path") {
                            if let Ok(import_path) = path_node.utf8_text(content.as_bytes()) {
                                let name = import_path
                                    .trim_matches('"')
                                    .rsplit('/')
                                    .next()
                                    .unwrap_or("")
                                    .to_string();

                                if !name.is_empty() {
                                    symbols.push(SymbolEntry {
                                        file: path.to_path_buf(),
                                        name,
                                        kind: DefinitionKind::Import,
                                        line: path_node.start_position().row + 1,
                                        column: path_node.start_position().column + 1,
                                        exported: false,
                                    });
                                }
                            }
                        }
                    }
                }
            }
            _ => {}
        }

        // Recurse into children
        let mut cursor = node.walk();
        for child in node.children(&mut cursor) {
            self.walk_go_tree(path, content, child, symbols);
        }
    }

    /// Resolve a symbol reference to its definition(s).
    pub fn resolve(&self, context: &ResolutionContext) -> Vec<ResolvedReference> {
        let mut results = Vec::new();

        // First try exact match
        if let Some(entries) = self.symbol_index.get(&context.symbol) {
            for entry in entries {
                // Don't resolve to the same location
                if entry.file == context.file && entry.line == context.line {
                    continue;
                }

                // Prioritize exported definitions
                if entry.exported && entry.kind != DefinitionKind::Import {
                    results.push(ResolvedReference {
                        reference_file: context.file.clone(),
                        reference_line: context.line,
                        symbol: context.symbol.clone(),
                        definition_file: entry.file.clone(),
                        definition_line: entry.line,
                        definition_kind: entry.kind,
                    });
                }
            }
        }

        // If no exported definitions, try all definitions
        if results.is_empty() {
            if let Some(entries) = self.symbol_index.get(&context.symbol) {
                for entry in entries {
                    if entry.file == context.file && entry.line == context.line {
                        continue;
                    }

                    if entry.kind != DefinitionKind::Import {
                        results.push(ResolvedReference {
                            reference_file: context.file.clone(),
                            reference_line: context.line,
                            symbol: context.symbol.clone(),
                            definition_file: entry.file.clone(),
                            definition_line: entry.line,
                            definition_kind: entry.kind,
                        });
                    }
                }
            }
        }

        results
    }

    /// Find all references to a symbol.
    pub fn find_references(&self, symbol: &str) -> Vec<&SymbolEntry> {
        self.symbol_index
            .get(symbol)
            .map(|v| v.iter().collect())
            .unwrap_or_default()
    }

    /// Get all exported symbols.
    pub fn get_exports(&self) -> Vec<&SymbolEntry> {
        self.symbol_index
            .values()
            .flat_map(|entries| entries.iter())
            .filter(|e| e.exported && e.kind != DefinitionKind::Import)
            .collect()
    }

    /// Get all symbols in a file.
    pub fn get_file_symbols(&self, path: &Path) -> Vec<&SymbolEntry> {
        self.symbol_index
            .values()
            .flat_map(|entries| entries.iter())
            .filter(|e| e.file == path)
            .collect()
    }

    /// Get the total number of indexed symbols.
    pub fn symbol_count(&self) -> usize {
        self.symbol_index.values().map(|v| v.len()).sum()
    }

    /// Get the number of indexed files.
    pub fn file_count(&self) -> usize {
        self.file_handles.len()
    }

    /// Clear all indexed data.
    pub fn clear(&mut self) {
        self.graph = StackGraph::new();
        self.partials = PartialPaths::new();
        self.database = Database::new();
        self.file_handles.clear();
        self.symbol_index.clear();
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_python_symbol_extraction() {
        let mut resolver = NameResolver::new();

        let code = r#"
import os
from pathlib import Path

class MyClass:
    def method(self):
        pass

def my_function():
    pass

MY_CONSTANT = 42
"#;

        let count = resolver
            .index_file(Path::new("test.py"), code, Language::Python)
            .unwrap();

        assert!(count >= 4, "Should find at least 4 symbols, got {}", count);

        // Check specific symbols
        let symbols = resolver.find_references("my_function");
        assert_eq!(symbols.len(), 1);
        assert_eq!(symbols[0].kind, DefinitionKind::Function);

        let symbols = resolver.find_references("MyClass");
        assert_eq!(symbols.len(), 1);
        assert_eq!(symbols[0].kind, DefinitionKind::Class);
    }

    #[test]
    fn test_javascript_symbol_extraction() {
        let mut resolver = NameResolver::new();

        let code = r#"
import { foo } from 'bar';

export function myExportedFunction() {}

function privateFunction() {}

export class MyClass {}

const myConstant = 42;
"#;

        let count = resolver
            .index_file(Path::new("test.js"), code, Language::JavaScript)
            .unwrap();

        assert!(count >= 4, "Should find at least 4 symbols, got {}", count);

        let symbols = resolver.find_references("myExportedFunction");
        assert_eq!(symbols.len(), 1);
        assert!(symbols[0].exported);
    }

    #[test]
    fn test_rust_symbol_extraction() {
        let mut resolver = NameResolver::new();

        let code = r#"
use std::collections::HashMap;

pub struct MyStruct {
    field: i32,
}

impl MyStruct {
    pub fn new() -> Self {
        Self { field: 0 }
    }

    fn private_method(&self) {}
}

pub fn public_function() {}

fn private_function() {}

pub const MY_CONST: i32 = 42;
"#;

        let count = resolver
            .index_file(Path::new("test.rs"), code, Language::Rust)
            .unwrap();

        assert!(count >= 4, "Should find at least 4 symbols, got {}", count);

        let symbols = resolver.find_references("MyStruct");
        assert_eq!(symbols.len(), 1);
        assert!(symbols[0].exported);
    }

    #[test]
    fn test_go_symbol_extraction() {
        let mut resolver = NameResolver::new();

        let code = r#"
package main

import "fmt"

type MyStruct struct {
    Field int
}

func (m *MyStruct) Method() {}

func ExportedFunc() {}

func privateFunc() {}

const MyConst = 42
var myVar = "hello"
"#;

        let count = resolver
            .index_file(Path::new("test.go"), code, Language::Go)
            .unwrap();

        assert!(count >= 4, "Should find at least 4 symbols, got {}", count);

        let symbols = resolver.find_references("ExportedFunc");
        assert_eq!(symbols.len(), 1);
        assert!(symbols[0].exported);

        let symbols = resolver.find_references("privateFunc");
        assert_eq!(symbols.len(), 1);
        assert!(!symbols[0].exported);
    }

    #[test]
    fn test_cross_file_resolution() {
        let mut resolver = NameResolver::new();

        // File 1: defines a function
        let code1 = "def my_function():\n    pass\n";
        resolver
            .index_file(Path::new("module1.py"), code1, Language::Python)
            .unwrap();

        // File 2: uses the function
        let code2 = "from module1 import my_function\nmy_function()\n";
        resolver
            .index_file(Path::new("module2.py"), code2, Language::Python)
            .unwrap();

        // Try to resolve the reference
        let context = ResolutionContext {
            file: Path::new("module2.py").to_path_buf(),
            symbol: "my_function".to_string(),
            line: 2,
            column: 1,
        };

        let refs = resolver.resolve(&context);
        assert_eq!(refs.len(), 1);
        assert_eq!(refs[0].definition_file, Path::new("module1.py"));
    }
}

================================================================================
END: src\analyzer\name_resolution.rs
================================================================================

================================================================================
FILE: src\crawler\mod.rs
================================================================================
//! GitHub crawler and Git operations module.

mod git;
mod github;

pub use git::*;
pub use github::*;

================================================================================
END: src\crawler\mod.rs
================================================================================

================================================================================
FILE: src\crawler\git.rs
================================================================================
//! Git operations using git2.

use crate::error::{AuditorError, Result};
use crate::models::Repository;
use git2::{build::RepoBuilder, Cred, FetchOptions, RemoteCallbacks};
use std::fs::File;
use std::path::{Path, PathBuf};
use tracing::{debug, info, warn};

/// Git operations handler.
pub struct GitOperations {
    /// Base directory for cloning repositories
    base_dir: PathBuf,

    /// GitHub token for authentication
    token: Option<String>,

    /// Shallow clone depth (0 = full clone)
    depth: u32,
}

impl GitOperations {
    /// Create a new Git operations handler.
    pub fn new(base_dir: PathBuf) -> Self {
        Self {
            base_dir,
            token: std::env::var("GITHUB_TOKEN").ok(),
            depth: 1, // Shallow clone by default
        }
    }

    /// Set the authentication token.
    pub fn with_token(mut self, token: impl Into<String>) -> Self {
        self.token = Some(token.into());
        self
    }

    /// Set the clone depth (0 = full clone).
    pub fn with_depth(mut self, depth: u32) -> Self {
        self.depth = depth;
        self
    }

    /// Clone a repository.
    ///
    /// Uses a lock file to prevent TOCTOU race conditions when multiple
    /// processes attempt to clone the same repository.
    pub fn clone_repository(&self, repo: &mut Repository) -> Result<PathBuf> {
        let target_dir = self.base_dir.join(&repo.owner).join(&repo.name);

        // Create parent directories first
        if let Some(parent) = target_dir.parent() {
            std::fs::create_dir_all(parent)?;
        }

        // Use a lock file to prevent race conditions
        let lock_path = target_dir.with_extension("lock");
        let _lock = self.acquire_lock(&lock_path)?;

        // Now check if repository exists (protected by lock)
        if target_dir.exists() {
            if let Ok(git_repo) = git2::Repository::open(&target_dir) {
                if !git_repo.is_bare() {
                    info!("Repository already cloned: {}", target_dir.display());
                    repo.local_path = Some(target_dir.clone());
                    return Ok(target_dir);
                }
            }
            // Invalid repo, remove it (safe - we hold the lock)
            std::fs::remove_dir_all(&target_dir)?;
        }

        info!("Cloning repository: {} -> {}", repo.clone_url, target_dir.display());

        let mut callbacks = RemoteCallbacks::new();

        // Set up authentication
        if let Some(ref token) = self.token {
            let token = token.clone();
            callbacks.credentials(move |_url, username_from_url, _allowed_types| {
                // Use token as password with any username for HTTPS
                Cred::userpass_plaintext(username_from_url.unwrap_or("git"), &token)
            });
        }

        // Progress callback
        callbacks.transfer_progress(|progress| {
            debug!(
                "Clone progress: {}/{} objects",
                progress.received_objects(),
                progress.total_objects()
            );
            true
        });

        let mut fetch_options = FetchOptions::new();
        fetch_options.remote_callbacks(callbacks);

        if self.depth > 0 {
            fetch_options.depth(self.depth as i32);
        }

        let mut builder = RepoBuilder::new();
        builder.fetch_options(fetch_options);

        // Clone the repository
        let git_repo = builder
            .clone(&repo.clone_url, &target_dir)
            .map_err(|e| AuditorError::Clone(format!("Failed to clone {}: {}", repo.full_name, e)))?;

        // Get the HEAD commit SHA
        if let Ok(head) = git_repo.head() {
            if let Some(oid) = head.target() {
                repo.commit_sha = Some(oid.to_string());
            }
        }

        repo.local_path = Some(target_dir.clone());
        info!("Successfully cloned: {}", repo.full_name);

        Ok(target_dir)
    }

    /// Acquire a file lock for the given path.
    ///
    /// Returns a guard that releases the lock when dropped.
    fn acquire_lock(&self, lock_path: &Path) -> Result<LockGuard> {
        use std::io::Write;

        // Try to create the lock file exclusively
        let mut attempts = 0;
        const MAX_ATTEMPTS: u32 = 10;
        const RETRY_DELAY_MS: u64 = 100;

        loop {
            match File::options()
                .write(true)
                .create_new(true)
                .open(lock_path)
            {
                Ok(mut file) => {
                    // Write our PID to the lock file for debugging
                    let _ = writeln!(file, "{}", std::process::id());
                    return Ok(LockGuard {
                        path: lock_path.to_path_buf(),
                    });
                }
                Err(e) if e.kind() == std::io::ErrorKind::AlreadyExists => {
                    attempts += 1;
                    if attempts >= MAX_ATTEMPTS {
                        return Err(AuditorError::Clone(format!(
                            "Could not acquire lock after {} attempts: {}",
                            MAX_ATTEMPTS,
                            lock_path.display()
                        )));
                    }
                    // Wait and retry
                    std::thread::sleep(std::time::Duration::from_millis(RETRY_DELAY_MS));
                }
                Err(e) => {
                    return Err(AuditorError::Io(e));
                }
            }
        }
    }

    /// Open an existing local repository.
    pub fn open_repository(&self, path: &Path) -> Result<git2::Repository> {
        git2::Repository::open(path).map_err(|e| e.into())
    }

    /// Get the current commit SHA.
    pub fn get_head_sha(path: &Path) -> Result<String> {
        let repo = git2::Repository::open(path)?;
        let head = repo.head()?;
        let oid = head.target().ok_or_else(|| {
            AuditorError::Git(git2::Error::from_str("HEAD has no target"))
        })?;
        Ok(oid.to_string())
    }

    /// Check if a path is a Git repository.
    pub fn is_repository(path: &Path) -> bool {
        git2::Repository::open(path).is_ok()
    }

    /// Get list of files changed in recent commits.
    pub fn get_changed_files(path: &Path, since_commit: Option<&str>) -> Result<Vec<PathBuf>> {
        let repo = git2::Repository::open(path)?;
        let head = repo.head()?.peel_to_commit()?;
        let head_tree = head.tree()?;

        let base_tree = if let Some(sha) = since_commit {
            let oid = git2::Oid::from_str(sha)?;
            let commit = repo.find_commit(oid)?;
            Some(commit.tree()?)
        } else if let Some(parent) = head.parent(0).ok() {
            Some(parent.tree()?)
        } else {
            None
        };

        let diff = repo.diff_tree_to_tree(base_tree.as_ref(), Some(&head_tree), None)?;

        let mut files = Vec::new();
        diff.foreach(
            &mut |delta, _| {
                if let Some(path) = delta.new_file().path() {
                    files.push(path.to_path_buf());
                }
                true
            },
            None,
            None,
            None,
        )?;

        Ok(files)
    }

    /// Clean up a cloned repository.
    pub fn cleanup(&self, repo: &Repository) -> Result<()> {
        if let Some(ref path) = repo.local_path {
            if path.exists() {
                info!("Cleaning up: {}", path.display());
                std::fs::remove_dir_all(path)?;
            }
        }
        Ok(())
    }

    /// Get repository information from a local path.
    pub fn get_repo_info(path: &Path) -> Result<LocalRepoInfo> {
        let repo = git2::Repository::open(path)?;

        let head = repo.head()?;
        let commit = head.peel_to_commit()?;

        let mut remotes = Vec::new();
        if let Ok(remote_names) = repo.remotes() {
            for name in remote_names.iter().flatten() {
                if let Ok(remote) = repo.find_remote(name) {
                    if let Some(url) = remote.url() {
                        remotes.push((name.to_string(), url.to_string()));
                    }
                }
            }
        }

        // Check dirty status before returning
        let is_dirty = repo.statuses(None).map(|s| !s.is_empty()).unwrap_or(false);

        Ok(LocalRepoInfo {
            path: path.to_path_buf(),
            head_sha: commit.id().to_string(),
            branch: head
                .shorthand()
                .map(|s| s.to_string())
                .unwrap_or_else(|| "HEAD".to_string()),
            remotes,
            is_dirty,
        })
    }
}

/// Information about a local repository.
#[derive(Debug, Clone)]
pub struct LocalRepoInfo {
    pub path: PathBuf,
    pub head_sha: String,
    pub branch: String,
    pub remotes: Vec<(String, String)>,
    pub is_dirty: bool,
}

impl LocalRepoInfo {
    /// Get the origin remote URL.
    pub fn origin_url(&self) -> Option<&str> {
        self.remotes
            .iter()
            .find(|(name, _)| name == "origin")
            .map(|(_, url)| url.as_str())
    }

    /// Parse owner/name from origin URL.
    pub fn parse_github_info(&self) -> Option<(String, String)> {
        let url = self.origin_url()?;
        Repository::from_url(url).map(|r| (r.owner, r.name))
    }
}

/// File system traversal for repositories.
pub struct RepoTraverser {
    ignore_builder: ignore::gitignore::GitignoreBuilder,
    max_file_size: u64,
}

impl RepoTraverser {
    /// Create a new repository traverser.
    pub fn new(repo_path: &Path) -> Self {
        let mut ignore_builder = ignore::gitignore::GitignoreBuilder::new(repo_path);

        // Load .gitignore if present
        let gitignore_path = repo_path.join(".gitignore");
        if gitignore_path.exists() {
            let _ = ignore_builder.add(&gitignore_path);
        }

        // Add default ignores
        let _ = ignore_builder.add_line(None, "**/target/");
        let _ = ignore_builder.add_line(None, "**/node_modules/");
        let _ = ignore_builder.add_line(None, "**/.git/");
        let _ = ignore_builder.add_line(None, "**/vendor/");
        let _ = ignore_builder.add_line(None, "**/dist/");
        let _ = ignore_builder.add_line(None, "**/build/");

        Self {
            ignore_builder,
            max_file_size: 1024 * 1024, // 1 MB default
        }
    }

    /// Set maximum file size.
    pub fn with_max_file_size(mut self, size: u64) -> Self {
        self.max_file_size = size;
        self
    }

    /// Add custom ignore pattern.
    pub fn add_ignore_pattern(&mut self, pattern: &str) {
        let _ = self.ignore_builder.add_line(None, pattern);
    }

    /// Get all source files in the repository.
    pub fn get_source_files(&self, repo_path: &Path) -> Result<Vec<crate::models::SourceFile>> {
        use crate::models::{Language, SourceFile};

        let gitignore = self
            .ignore_builder
            .build()
            .map_err(|e| AuditorError::Analysis(format!("Failed to build gitignore: {}", e)))?;

        let mut files = Vec::new();

        for entry in walkdir::WalkDir::new(repo_path)
            .follow_links(false)
            .into_iter()
            .filter_map(|e| e.ok())
        {
            let path = entry.path();

            // Skip directories
            if !path.is_file() {
                continue;
            }

            // Check file size
            if let Ok(metadata) = path.metadata() {
                if metadata.len() > self.max_file_size {
                    debug!("Skipping large file: {}", path.display());
                    continue;
                }
            }

            // Check gitignore
            let relative_path = path.strip_prefix(repo_path).unwrap_or(path);
            if matches!(
                gitignore.matched(relative_path, path.is_dir()),
                ignore::Match::Ignore(_)
            ) {
                continue;
            }

            // Detect language from extension
            let language = path
                .extension()
                .and_then(|e| e.to_str())
                .map(Language::from_extension)
                .unwrap_or(Language::Unknown);

            // Only include supported source files
            if language.is_supported() {
                files.push(SourceFile {
                    path: relative_path.to_path_buf(),
                    absolute_path: path.to_path_buf(),
                    language,
                    size: path.metadata().map(|m| m.len()).unwrap_or(0),
                    content: None,
                });
            }
        }

        info!("Found {} source files in {}", files.len(), repo_path.display());
        Ok(files)
    }
}

/// RAII guard for file-based locking.
///
/// Removes the lock file when dropped.
struct LockGuard {
    path: PathBuf,
}

impl Drop for LockGuard {
    fn drop(&mut self) {
        // Clean up the lock file
        if let Err(e) = std::fs::remove_file(&self.path) {
            // Only warn if it's not a "not found" error (could have been cleaned up already)
            if e.kind() != std::io::ErrorKind::NotFound {
                warn!("Failed to remove lock file {}: {}", self.path.display(), e);
            }
        }
    }
}

================================================================================
END: src\crawler\git.rs
================================================================================

================================================================================
FILE: src\crawler\github.rs
================================================================================
//! GitHub API client using octocrab.

use crate::config::GitHubConfig;
use crate::error::{AuditorError, Result};
use crate::models::Repository;
use octocrab::Octocrab;
use tracing::{debug, info, warn};

/// GitHub API client for discovering and fetching repository metadata.
pub struct GitHubClient {
    client: Octocrab,
    config: GitHubConfig,
}

impl GitHubClient {
    /// Create a new GitHub client.
    pub fn new(config: GitHubConfig) -> Result<Self> {
        let mut builder = Octocrab::builder();

        if let Some(ref token) = config.token {
            builder = builder.personal_token(token.clone());
        }

        if config.api_url != "https://api.github.com" {
            builder = builder.base_uri(&config.api_url)?;
        }

        let client = builder.build()?;

        Ok(Self { client, config })
    }

    /// Create a client from environment variables.
    pub fn from_env() -> Result<Self> {
        Self::new(GitHubConfig::default())
    }

    /// Get repository metadata.
    pub async fn get_repository(&self, owner: &str, name: &str) -> Result<Repository> {
        info!("Fetching repository metadata: {}/{}", owner, name);

        let repo = self
            .client
            .repos(owner, name)
            .get()
            .await
            .map_err(|e| AuditorError::GitHub(format!("Failed to get repo {}/{}: {}", owner, name, e)))?;

        let mut repository = Repository::new(owner, name);

        repository.default_branch = repo
            .default_branch
            .clone()
            .unwrap_or_else(|| "main".to_string());
        repository.description = repo.description.clone();
        repository.language = repo.language.as_ref().and_then(|v| v.as_str()).map(|s| s.to_string());
        repository.stars = repo.stargazers_count.unwrap_or(0) as u64;
        repository.forks = repo.forks_count.unwrap_or(0) as u64;
        repository.archived = repo.archived.unwrap_or(false);
        repository.is_fork = repo.fork.unwrap_or(false);
        repository.topics = repo.topics.clone().unwrap_or_default();
        repository.updated_at = repo.updated_at;

        if let Some(ref html_url) = repo.html_url {
            repository.url = html_url.to_string();
        }
        if let Some(ref clone_url) = repo.clone_url {
            repository.clone_url = clone_url.to_string();
        }

        // Get latest commit SHA
        if let Ok(commits) = self
            .client
            .repos(owner, name)
            .list_commits()
            .per_page(1)
            .send()
            .await
        {
            if let Some(commit) = commits.items.first() {
                repository.commit_sha = Some(commit.sha.clone());
            }
        }

        // Get languages
        if let Ok(languages) = self.get_languages(owner, name).await {
            repository.languages = languages;
        }

        debug!("Repository metadata fetched: {:?}", repository);
        Ok(repository)
    }

    /// Get languages used in a repository.
    pub async fn get_languages(&self, owner: &str, name: &str) -> Result<Vec<String>> {
        let languages = self
            .client
            .repos(owner, name)
            .list_languages()
            .await
            .map_err(|e| AuditorError::GitHub(format!("Failed to get languages: {}", e)))?;

        Ok(languages.into_keys().collect())
    }

    /// List repositories for a user.
    pub async fn list_user_repos(&self, username: &str, limit: usize) -> Result<Vec<Repository>> {
        info!("Listing repositories for user: {}", username);

        let page = self
            .client
            .users(username)
            .repos()
            .per_page(limit.min(100) as u8)
            .send()
            .await?;

        let mut repos = Vec::new();
        for repo in page.items.into_iter().take(limit) {
            if let (Some(owner), name) = (repo.owner, repo.name) {
                let mut repository = Repository::new(owner.login, name);
                repository.default_branch = repo.default_branch.unwrap_or_else(|| "main".to_string());
                repository.description = repo.description;
                repository.language = repo.language.as_ref().and_then(|v| v.as_str()).map(|s| s.to_string());
                repository.archived = repo.archived.unwrap_or(false);
                repository.is_fork = repo.fork.unwrap_or(false);
                if let Some(ref clone_url) = repo.clone_url {
                    repository.clone_url = clone_url.to_string();
                }
                repos.push(repository);
            }
        }

        info!("Found {} repositories for user {}", repos.len(), username);
        Ok(repos)
    }

    /// List repositories for an organization.
    pub async fn list_org_repos(&self, org: &str, limit: usize) -> Result<Vec<Repository>> {
        info!("Listing repositories for organization: {}", org);

        let page = self
            .client
            .orgs(org)
            .list_repos()
            .per_page(limit.min(100) as u8)
            .send()
            .await?;

        let mut repos = Vec::new();
        for repo in page.items.into_iter().take(limit) {
            if let (Some(owner), name) = (repo.owner, repo.name) {
                let mut repository = Repository::new(owner.login, name);
                repository.default_branch = repo.default_branch.unwrap_or_else(|| "main".to_string());
                repository.description = repo.description;
                repository.language = repo.language.as_ref().and_then(|v| v.as_str()).map(|s| s.to_string());
                repository.archived = repo.archived.unwrap_or(false);
                repository.is_fork = repo.fork.unwrap_or(false);
                if let Some(ref clone_url) = repo.clone_url {
                    repository.clone_url = clone_url.to_string();
                }
                repos.push(repository);
            }
        }

        info!("Found {} repositories for org {}", repos.len(), org);
        Ok(repos)
    }

    /// Search for repositories.
    pub async fn search_repos(&self, query: &str, limit: usize) -> Result<Vec<Repository>> {
        info!("Searching repositories: {}", query);

        let page = self
            .client
            .search()
            .repositories(query)
            .per_page(limit.min(100) as u8)
            .send()
            .await?;

        let mut repos = Vec::new();
        for repo in page.items.into_iter().take(limit) {
            if let (Some(owner), name) = (repo.owner, repo.name) {
                let mut repository = Repository::new(owner.login, name);
                repository.default_branch = repo.default_branch.unwrap_or_else(|| "main".to_string());
                repository.description = repo.description;
                repository.language = repo.language.as_ref().and_then(|v| v.as_str()).map(|s| s.to_string());
                repository.stars = repo.stargazers_count.unwrap_or(0) as u64;
                repository.forks = repo.forks_count.unwrap_or(0) as u64;
                repository.archived = repo.archived.unwrap_or(false);
                repository.is_fork = repo.fork.unwrap_or(false);
                if let Some(ref clone_url) = repo.clone_url {
                    repository.clone_url = clone_url.to_string();
                }
                repos.push(repository);
            }
        }

        info!("Found {} repositories matching '{}'", repos.len(), query);
        Ok(repos)
    }

    /// Get security advisories for a repository.
    pub async fn get_security_advisories(
        &self,
        owner: &str,
        name: &str,
    ) -> Result<Vec<SecurityAdvisory>> {
        debug!("Fetching security advisories for {}/{}", owner, name);

        // Use the GraphQL API or REST API for security advisories
        // For now, we'll use the REST API vulnerability alerts endpoint
        let url = format!(
            "{}/repos/{}/{}/vulnerability-alerts",
            self.config.api_url, owner, name
        );

        // This endpoint requires special headers
        let response: std::result::Result<serde_json::Value, _> = self
            .client
            .get(&url, None::<&()>)
            .await;

        match response {
            Ok(value) => {
                debug!("Security advisories response: {:?}", value);
                // Parse the response into advisories
                Ok(Vec::new()) // Placeholder - actual parsing depends on response format
            }
            Err(e) => {
                warn!("Could not fetch security advisories: {}", e);
                Ok(Vec::new())
            }
        }
    }

    /// Check rate limit status.
    pub async fn check_rate_limit(&self) -> Result<RateLimitStatus> {
        let rate_limit = self.client.ratelimit().get().await?;

        Ok(RateLimitStatus {
            limit: rate_limit.resources.core.limit as u32,
            remaining: rate_limit.resources.core.remaining as u32,
            reset: rate_limit.resources.core.reset as u64,
        })
    }

    /// Apply rate limiting delay if needed.
    pub async fn rate_limit_delay(&self) {
        if self.config.rate_limit_delay_ms > 0 {
            tokio::time::sleep(tokio::time::Duration::from_millis(
                self.config.rate_limit_delay_ms,
            ))
            .await;
        }
    }
}

/// GitHub security advisory.
#[derive(Debug, Clone)]
pub struct SecurityAdvisory {
    pub ghsa_id: String,
    pub cve_id: Option<String>,
    pub severity: String,
    pub summary: String,
    pub description: Option<String>,
    pub vulnerabilities: Vec<AdvisoryVulnerability>,
}

/// Vulnerability within a security advisory.
#[derive(Debug, Clone)]
pub struct AdvisoryVulnerability {
    pub package: String,
    pub ecosystem: String,
    pub vulnerable_version_range: String,
    pub first_patched_version: Option<String>,
}

/// Rate limit status.
#[derive(Debug, Clone)]
pub struct RateLimitStatus {
    pub limit: u32,
    pub remaining: u32,
    pub reset: u64,
}

impl RateLimitStatus {
    /// Check if we're rate limited.
    pub fn is_limited(&self) -> bool {
        self.remaining == 0
    }

    /// Seconds until reset.
    pub fn seconds_until_reset(&self) -> u64 {
        use std::time::{SystemTime, UNIX_EPOCH};
        let now = SystemTime::now()
            .duration_since(UNIX_EPOCH)
            .unwrap_or_default()
            .as_secs();
        self.reset.saturating_sub(now)
    }
}

================================================================================
END: src\crawler\github.rs
================================================================================

================================================================================
FILE: src\reporter\mod.rs
================================================================================
//! Reporting module for outputting scan results.

mod sarif;
mod text;

pub use sarif::*;
pub use text::*;

use crate::config::OutputFormat;
use crate::models::ScanResult;

/// Report generator trait.
pub trait Reporter {
    /// Generate a report from scan results.
    fn generate(&self, result: &ScanResult) -> String;
}

/// Create a reporter based on output format.
pub fn create_reporter(format: OutputFormat) -> Box<dyn Reporter> {
    match format {
        OutputFormat::Sarif => Box::new(SarifReporter::new()),
        OutputFormat::Json => Box::new(JsonReporter::new()),
        OutputFormat::Text => Box::new(TextReporter::new()),
    }
}

/// JSON reporter for simple JSON output.
pub struct JsonReporter;

impl JsonReporter {
    pub fn new() -> Self {
        Self
    }
}

impl Default for JsonReporter {
    fn default() -> Self {
        Self::new()
    }
}

impl Reporter for JsonReporter {
    fn generate(&self, result: &ScanResult) -> String {
        serde_json::to_string_pretty(result).unwrap_or_else(|e| format!("{{\"error\": \"{}\"}}", e))
    }
}

================================================================================
END: src\reporter\mod.rs
================================================================================

================================================================================
FILE: src\reporter\sarif.rs
================================================================================
//! SARIF (Static Analysis Results Interchange Format) reporter.

use super::Reporter;
use crate::models::{Finding, FindingCategory, ScanResult, Severity};
use serde::Serialize;
use serde_json::{json, Value};

/// SARIF format reporter.
pub struct SarifReporter {
    /// Tool name
    tool_name: String,

    /// Tool version
    tool_version: String,

    /// Include snippets in results
    include_snippets: bool,
}

impl SarifReporter {
    /// Create a new SARIF reporter.
    pub fn new() -> Self {
        Self {
            tool_name: "sec_auditor".to_string(),
            tool_version: env!("CARGO_PKG_VERSION").to_string(),
            include_snippets: true,
        }
    }

    /// Set whether to include code snippets.
    pub fn with_snippets(mut self, include: bool) -> Self {
        self.include_snippets = include;
        self
    }

    /// Build a SARIF rule from a finding.
    fn build_rule(&self, finding: &Finding) -> Value {
        let mut rule = json!({
            "id": finding.rule_id,
            "name": finding.title,
            "shortDescription": {
                "text": finding.title
            },
            "fullDescription": {
                "text": finding.description
            },
            "defaultConfiguration": {
                "level": self.severity_to_level(finding.severity)
            },
            "properties": {
                "security-severity": self.severity_to_score(finding.severity),
                "precision": self.confidence_to_precision(&finding.confidence)
            }
        });

        // Add help text if remediation is available
        if let Some(ref remediation) = finding.remediation {
            rule["help"] = json!({
                "text": remediation,
                "markdown": format!("**Remediation:** {}", remediation)
            });
        }

        // Add CWE tags
        let cwes: Vec<String> = finding
            .metadata
            .get("cwe")
            .and_then(|v| v.as_str())
            .map(|s| vec![s.to_string()])
            .unwrap_or_default();

        if !cwes.is_empty() {
            rule["properties"]["tags"] = json!(cwes);
        }

        rule
    }

    /// Build a SARIF result from a finding.
    fn build_result(&self, finding: &Finding) -> Value {
        let mut result = json!({
            "ruleId": finding.rule_id,
            "level": self.severity_to_level(finding.severity),
            "message": {
                "text": finding.description
            },
            "locations": [{
                "physicalLocation": {
                    "artifactLocation": {
                        "uri": finding.location.file.display().to_string(),
                        "uriBaseId": "%SRCROOT%"
                    },
                    "region": {
                        "startLine": finding.location.start_line,
                        "startColumn": finding.location.start_column,
                        "endLine": finding.location.end_line,
                        "endColumn": finding.location.end_column
                    }
                }
            }]
        });

        // Add code snippet if available
        if self.include_snippets {
            if let Some(ref snippet) = finding.snippet {
                let snippet_text: String = snippet
                    .lines
                    .iter()
                    .map(|(_, line)| line.as_str())
                    .collect::<Vec<_>>()
                    .join("\n");

                result["locations"][0]["physicalLocation"]["contextRegion"] = json!({
                    "startLine": snippet.lines.first().map(|(n, _)| *n).unwrap_or(1),
                    "endLine": snippet.lines.last().map(|(n, _)| *n).unwrap_or(1),
                    "snippet": {
                        "text": snippet_text
                    }
                });
            }
        }

        // Add fingerprint for deduplication
        result["fingerprints"] = json!({
            "primary": finding.id.clone()
        });

        // Add properties
        let mut properties = json!({
            "category": self.category_string(&finding.category),
            "confidence": format!("{:?}", finding.confidence)
        });

        // Add vulnerability info for SCA findings
        if let Some(ref vuln) = finding.vulnerability {
            properties["vulnerability"] = json!({
                "id": vuln.id,
                "cvss": vuln.cvss_score,
                "package": vuln.package,
                "fixedVersion": vuln.fixed_version
            });
        }

        result["properties"] = properties;

        result
    }

    /// Convert severity to SARIF level.
    fn severity_to_level(&self, severity: Severity) -> &'static str {
        match severity {
            Severity::Critical | Severity::High => "error",
            Severity::Medium => "warning",
            Severity::Low => "note",
            Severity::None | Severity::Unknown => "none",
        }
    }

    /// Convert severity to numeric score.
    fn severity_to_score(&self, severity: Severity) -> f32 {
        match severity {
            Severity::Critical => 10.0,
            Severity::High => 8.0,
            Severity::Medium => 5.0,
            Severity::Low => 3.0,
            Severity::None | Severity::Unknown => 0.0,
        }
    }

    /// Convert confidence to SARIF precision.
    fn confidence_to_precision(&self, confidence: &crate::models::Confidence) -> &'static str {
        match confidence {
            crate::models::Confidence::High => "high",
            crate::models::Confidence::Medium => "medium",
            crate::models::Confidence::Low => "low",
        }
    }

    /// Convert finding category to string.
    fn category_string(&self, category: &FindingCategory) -> String {
        match category {
            FindingCategory::Sast(cat) => format!("SAST/{:?}", cat),
            FindingCategory::Sca => "SCA".to_string(),
            FindingCategory::Provenance => "Provenance".to_string(),
            FindingCategory::Ai => "AI".to_string(),
            FindingCategory::Config => "Config".to_string(),
            FindingCategory::Secret => "Secret".to_string(),
        }
    }
}

impl Default for SarifReporter {
    fn default() -> Self {
        Self::new()
    }
}

impl Reporter for SarifReporter {
    fn generate(&self, result: &ScanResult) -> String {
        // Collect unique rules
        let mut rules: Vec<Value> = Vec::new();
        let mut seen_rules = std::collections::HashSet::new();

        for finding in &result.findings {
            if !seen_rules.contains(&finding.rule_id) {
                seen_rules.insert(finding.rule_id.clone());
                rules.push(self.build_rule(finding));
            }
        }

        // Build results
        let results: Vec<Value> = result.findings.iter().map(|f| self.build_result(f)).collect();

        // Build the SARIF document
        let sarif = json!({
            "$schema": "https://raw.githubusercontent.com/oasis-tcs/sarif-spec/master/Schemata/sarif-schema-2.1.0.json",
            "version": "2.1.0",
            "runs": [{
                "tool": {
                    "driver": {
                        "name": self.tool_name,
                        "version": self.tool_version,
                        "informationUri": "https://github.com/anthropics/sec_auditor",
                        "rules": rules,
                        "properties": {
                            "comments": "Security analysis tool for GitHub repositories"
                        }
                    }
                },
                "results": results,
                "invocations": [{
                    "executionSuccessful": result.success,
                    "startTimeUtc": result.started_at.to_rfc3339(),
                    "endTimeUtc": result.completed_at.to_rfc3339()
                }],
                "properties": {
                    "repository": result.repository,
                    "commitSha": result.commit_sha,
                    "stats": {
                        "filesScanned": result.stats.files_scanned,
                        "linesAnalyzed": result.stats.lines_analyzed,
                        "sastFindings": result.stats.sast_findings,
                        "scaFindings": result.stats.sca_findings,
                        "secretsFound": result.stats.secrets_found,
                        "durationMs": result.stats.duration_ms
                    }
                }
            }]
        });

        serde_json::to_string_pretty(&sarif).unwrap_or_else(|e| format!("{{\"error\": \"{}\"}}", e))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::{Finding, Location, Severity};

    #[test]
    fn test_sarif_generation() {
        let reporter = SarifReporter::new();

        let mut result = ScanResult::new("test/repo");
        result.add_finding(Finding::sast(
            "test/rule",
            "Test Finding",
            "This is a test finding",
            Location::new("src/main.rs".into(), 10, 5),
            Severity::High,
        ));

        let sarif = reporter.generate(&result);

        // Parse to verify it's valid JSON
        let parsed: serde_json::Value = serde_json::from_str(&sarif).unwrap();

        assert_eq!(parsed["version"], "2.1.0");
        assert_eq!(parsed["runs"][0]["results"].as_array().unwrap().len(), 1);
    }
}

================================================================================
END: src\reporter\sarif.rs
================================================================================

================================================================================
FILE: src\reporter\text.rs
================================================================================
//! Human-readable text reporter.

use super::Reporter;
use crate::models::{FindingCategory, ScanResult, Severity};

/// Text format reporter for terminal output.
pub struct TextReporter {
    /// Use colors in output
    use_colors: bool,

    /// Show code snippets
    show_snippets: bool,

    /// Maximum findings to show (0 = unlimited)
    max_findings: usize,
}

impl TextReporter {
    /// Create a new text reporter.
    pub fn new() -> Self {
        Self {
            use_colors: true,
            show_snippets: true,
            max_findings: 0,
        }
    }

    /// Disable colors.
    pub fn without_colors(mut self) -> Self {
        self.use_colors = false;
        self
    }

    /// Disable snippets.
    pub fn without_snippets(mut self) -> Self {
        self.show_snippets = false;
        self
    }

    /// Limit number of findings shown.
    pub fn with_max_findings(mut self, max: usize) -> Self {
        self.max_findings = max;
        self
    }

    /// Get severity color code.
    fn severity_color(&self, severity: Severity) -> &'static str {
        if !self.use_colors {
            return "";
        }
        match severity {
            Severity::Critical => "\x1b[91m", // Bright red
            Severity::High => "\x1b[31m",     // Red
            Severity::Medium => "\x1b[33m",   // Yellow
            Severity::Low => "\x1b[36m",      // Cyan
            Severity::None => "\x1b[32m",     // Green
            Severity::Unknown => "\x1b[37m",  // White
        }
    }

    /// Reset color.
    fn reset(&self) -> &'static str {
        if self.use_colors {
            "\x1b[0m"
        } else {
            ""
        }
    }

    /// Bold text.
    fn bold(&self) -> &'static str {
        if self.use_colors {
            "\x1b[1m"
        } else {
            ""
        }
    }

    /// Dim text.
    fn dim(&self) -> &'static str {
        if self.use_colors {
            "\x1b[2m"
        } else {
            ""
        }
    }

    /// Category icon.
    fn category_icon(&self, category: &FindingCategory) -> &'static str {
        match category {
            FindingCategory::Sast(_) => "[SAST]",
            FindingCategory::Sca => "[SCA]",
            FindingCategory::Provenance => "[PROV]",
            FindingCategory::Ai => "[AI]",
            FindingCategory::Config => "[CFG]",
            FindingCategory::Secret => "[SEC]",
        }
    }
}

impl Default for TextReporter {
    fn default() -> Self {
        Self::new()
    }
}

impl Reporter for TextReporter {
    fn generate(&self, result: &ScanResult) -> String {
        let mut output = String::new();

        // Header
        output.push_str(&format!(
            "\n{}=== Security Scan Report ==={}\n\n",
            self.bold(),
            self.reset()
        ));

        // Repository info
        output.push_str(&format!(
            "{}Repository:{} {}\n",
            self.bold(),
            self.reset(),
            result.repository
        ));
        if let Some(ref sha) = result.commit_sha {
            output.push_str(&format!(
                "{}Commit:{} {}\n",
                self.bold(),
                self.reset(),
                sha
            ));
        }
        output.push_str(&format!(
            "{}Scan Time:{} {} - {}\n",
            self.bold(),
            self.reset(),
            result.started_at.format("%Y-%m-%d %H:%M:%S UTC"),
            result.completed_at.format("%H:%M:%S UTC")
        ));
        output.push_str(&format!(
            "{}Duration:{} {}ms\n\n",
            self.bold(),
            self.reset(),
            result.stats.duration_ms
        ));

        // Summary
        output.push_str(&format!("{}--- Summary ---{}\n", self.bold(), self.reset()));
        output.push_str(&format!(
            "Files Scanned: {}\n",
            result.stats.files_scanned
        ));
        output.push_str(&format!(
            "Lines Analyzed: {}\n",
            result.stats.lines_analyzed
        ));
        output.push_str(&format!(
            "Dependencies Checked: {}\n\n",
            result.stats.dependencies_checked
        ));

        // Finding counts by severity
        let critical = result.findings_by_severity(Severity::Critical).len();
        let high = result.findings_by_severity(Severity::High).len();
        let medium = result.findings_by_severity(Severity::Medium).len();
        let low = result.findings_by_severity(Severity::Low).len();

        output.push_str(&format!(
            "{}Findings:{}\n",
            self.bold(),
            self.reset()
        ));
        output.push_str(&format!(
            "  {}CRITICAL:{} {}\n",
            self.severity_color(Severity::Critical),
            self.reset(),
            critical
        ));
        output.push_str(&format!(
            "  {}HIGH:{} {}\n",
            self.severity_color(Severity::High),
            self.reset(),
            high
        ));
        output.push_str(&format!(
            "  {}MEDIUM:{} {}\n",
            self.severity_color(Severity::Medium),
            self.reset(),
            medium
        ));
        output.push_str(&format!(
            "  {}LOW:{} {}\n\n",
            self.severity_color(Severity::Low),
            self.reset(),
            low
        ));

        // Findings detail
        if result.findings.is_empty() {
            output.push_str(&format!(
                "{}No security findings detected.{}\n",
                self.dim(),
                self.reset()
            ));
        } else {
            output.push_str(&format!("{}--- Findings ---{}\n\n", self.bold(), self.reset()));

            // Sort by severity
            let mut findings = result.findings.clone();
            findings.sort_by(|a, b| b.severity.cmp(&a.severity));

            let findings_to_show = if self.max_findings > 0 {
                &findings[..self.max_findings.min(findings.len())]
            } else {
                &findings[..]
            };

            for (i, finding) in findings_to_show.iter().enumerate() {
                // Finding header
                output.push_str(&format!(
                    "{}[{}]{} {}{} {}{}\n",
                    self.severity_color(finding.severity),
                    finding.severity,
                    self.reset(),
                    self.category_icon(&finding.category),
                    self.bold(),
                    finding.title,
                    self.reset()
                ));

                // Location
                output.push_str(&format!(
                    "  {}Location:{} {}:{}:{}\n",
                    self.dim(),
                    self.reset(),
                    finding.location.file.display(),
                    finding.location.start_line,
                    finding.location.start_column
                ));

                // Rule ID
                output.push_str(&format!(
                    "  {}Rule:{} {}\n",
                    self.dim(),
                    self.reset(),
                    finding.rule_id
                ));

                // Description
                output.push_str(&format!(
                    "  {}Description:{} {}\n",
                    self.dim(),
                    self.reset(),
                    finding.description
                ));

                // Code snippet
                if self.show_snippets {
                    if let Some(ref snippet) = finding.snippet {
                        output.push_str(&format!("  {}Code:{}\n", self.dim(), self.reset()));
                        for (line_num, line) in &snippet.lines {
                            let marker = if *line_num == snippet.highlight_line {
                                format!("{}>{}", self.severity_color(finding.severity), self.reset())
                            } else {
                                " ".to_string()
                            };
                            output.push_str(&format!(
                                "    {}{:>4} |{} {}\n",
                                marker,
                                line_num,
                                self.reset(),
                                line
                            ));
                        }
                    }
                }

                // Remediation
                if let Some(ref remediation) = finding.remediation {
                    output.push_str(&format!(
                        "  {}Remediation:{} {}\n",
                        self.dim(),
                        self.reset(),
                        remediation
                    ));
                }

                // Vulnerability info for SCA
                if let Some(ref vuln) = finding.vulnerability {
                    output.push_str(&format!(
                        "  {}Vulnerability:{} {} (CVSS: {})\n",
                        self.dim(),
                        self.reset(),
                        vuln.id,
                        vuln.cvss_score.unwrap_or(0.0)
                    ));
                    if let Some(ref fixed) = vuln.fixed_version {
                        output.push_str(&format!(
                            "  {}Fixed in:{} {}\n",
                            self.dim(),
                            self.reset(),
                            fixed
                        ));
                    }
                }

                output.push('\n');

                // Add separator between findings
                if i < findings_to_show.len() - 1 {
                    output.push_str(&format!("{}---{}\n\n", self.dim(), self.reset()));
                }
            }

            // Show if there are more findings
            if self.max_findings > 0 && findings.len() > self.max_findings {
                output.push_str(&format!(
                    "\n{}... and {} more findings{}\n",
                    self.dim(),
                    findings.len() - self.max_findings,
                    self.reset()
                ));
            }
        }

        // Status
        output.push_str(&format!("\n{}--- Status ---{}\n", self.bold(), self.reset()));
        if result.success {
            output.push_str(&format!(
                "{}Scan completed successfully.{}\n",
                if self.use_colors { "\x1b[32m" } else { "" },
                self.reset()
            ));
        } else {
            output.push_str(&format!(
                "{}Scan failed: {}{}\n",
                if self.use_colors { "\x1b[31m" } else { "" },
                result.error.as_deref().unwrap_or("Unknown error"),
                self.reset()
            ));
        }

        // Exit recommendation
        if critical > 0 || high > 0 {
            output.push_str(&format!(
                "\n{}WARNING: Critical or high severity issues found. Review required.{}\n",
                if self.use_colors { "\x1b[31m" } else { "" },
                self.reset()
            ));
        }

        output
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::models::{Finding, Location, Severity};

    #[test]
    fn test_text_report_generation() {
        let reporter = TextReporter::new().without_colors();

        let mut result = ScanResult::new("test/repo");
        result.add_finding(Finding::sast(
            "test/rule",
            "Test Finding",
            "This is a test finding",
            Location::new("src/main.rs".into(), 10, 5),
            Severity::High,
        ));

        let report = reporter.generate(&result);

        assert!(report.contains("Security Scan Report"));
        assert!(report.contains("test/repo"));
        assert!(report.contains("Test Finding"));
        assert!(report.contains("HIGH"));
    }

    #[test]
    fn test_empty_report() {
        let reporter = TextReporter::new().without_colors();
        let result = ScanResult::new("test/repo");

        let report = reporter.generate(&result);

        assert!(report.contains("No security findings detected"));
    }
}

================================================================================
END: src\reporter\text.rs
================================================================================

================================================================================
FILE: src\provenance\mod.rs
================================================================================
//! Supply chain provenance verification module.

mod slsa;

pub use slsa::*;

================================================================================
END: src\provenance\mod.rs
================================================================================

================================================================================
FILE: src\provenance\slsa.rs
================================================================================
//! SLSA (Supply-chain Levels for Software Artifacts) verification.
//!
//! This module provides supply chain security verification using:
//! - Sigstore verification with Rekor v2 tile-based architecture
//! - Local tile caching for high-performance verification at scale
//! - GitHub Release attestation discovery

use crate::error::{AuditorError, Result};
use crate::models::{Confidence, Finding, Location, Provenance, Severity};
use redb::{Database, ReadableTable, TableDefinition};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use sha2::{Digest, Sha256};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::{Duration, SystemTime, UNIX_EPOCH};
use tracing::{debug, info, warn};

/// Table definition for Rekor tile cache
const TILE_CACHE: TableDefinition<&str, &[u8]> = TableDefinition::new("tiles");
const VERDICT_CACHE: TableDefinition<&str, &[u8]> = TableDefinition::new("verdicts");
const CHECKPOINT_CACHE: TableDefinition<&str, &[u8]> = TableDefinition::new("checkpoints");

/// Configuration for tile-based caching
#[derive(Debug, Clone)]
pub struct TileCacheConfig {
    /// Path to the cache database
    pub cache_dir: PathBuf,
    /// Maximum age for checkpoint before refresh (in seconds)
    pub checkpoint_max_age_secs: u64,
    /// Tile size (number of entries per tile)
    pub tile_size: usize,
}

impl Default for TileCacheConfig {
    fn default() -> Self {
        Self {
            cache_dir: dirs_cache_path(),
            checkpoint_max_age_secs: 3600, // 1 hour
            tile_size: 256,
        }
    }
}

/// Get platform-appropriate cache directory
fn dirs_cache_path() -> PathBuf {
    std::env::var("HOME")
        .or_else(|_| std::env::var("USERPROFILE"))
        .map(PathBuf::from)
        .unwrap_or_else(|_| std::env::temp_dir())
        .join(".cache")
        .join("sec_auditor")
        .join("sigstore")
}

/// Tile-based Rekor cache for high-performance verification.
///
/// This implements the Rekor v2 tile-based architecture where:
/// - Full tiles are immutable and cached indefinitely
/// - Partial tiles (tree head) have a short TTL
/// - Multiple dependencies mapping to the same tile share a single fetch
pub struct RekorTileCache {
    /// Cache database
    db: Database,
    /// HTTP client
    client: Client,
    /// Rekor server URL
    rekor_url: String,
    /// Configuration
    config: TileCacheConfig,
}

impl RekorTileCache {
    /// Create a new tile cache
    pub fn new(config: TileCacheConfig) -> Result<Self> {
        // Ensure cache directory exists
        std::fs::create_dir_all(&config.cache_dir)?;

        let db_path = config.cache_dir.join("rekor_tiles.redb");
        let db = Database::create(&db_path).map_err(|e| {
            AuditorError::Sigstore(format!("Failed to create tile cache database: {}", e))
        })?;

        // Initialize tables
        let write_txn = db.begin_write().map_err(|e| {
            AuditorError::Sigstore(format!("Failed to begin write transaction: {}", e))
        })?;
        {
            let _ = write_txn.open_table(TILE_CACHE);
            let _ = write_txn.open_table(VERDICT_CACHE);
            let _ = write_txn.open_table(CHECKPOINT_CACHE);
        }
        write_txn.commit().map_err(|e| {
            AuditorError::Sigstore(format!("Failed to commit tables: {}", e))
        })?;

        Ok(Self {
            db,
            client: Client::new(),
            rekor_url: "https://rekor.sigstore.dev".to_string(),
            config,
        })
    }

    /// Get a tile from cache or fetch from Rekor
    pub async fn get_tile(&self, tile_index: u64) -> Result<Vec<u8>> {
        let key = format!("tile:{}", tile_index);

        // Check cache first
        if let Some(tile) = self.get_cached(&key)? {
            debug!("Tile {} found in cache", tile_index);
            return Ok(tile);
        }

        // Fetch from Rekor
        debug!("Fetching tile {} from Rekor", tile_index);
        let tile = self.fetch_tile(tile_index).await?;

        // Cache the tile (immutable, indefinite TTL)
        self.cache_tile(&key, &tile)?;

        Ok(tile)
    }

    /// Get multiple tiles in parallel (deduplicated)
    pub async fn get_tiles(&self, tile_indices: &[u64]) -> Result<HashMap<u64, Vec<u8>>> {
        let mut result = HashMap::new();
        let mut to_fetch = Vec::new();

        // Check cache for each tile
        for &idx in tile_indices {
            let key = format!("tile:{}", idx);
            if let Some(tile) = self.get_cached(&key)? {
                result.insert(idx, tile);
            } else {
                to_fetch.push(idx);
            }
        }

        // Fetch missing tiles in parallel
        if !to_fetch.is_empty() {
            let fetches: Vec<_> = to_fetch
                .iter()
                .map(|&idx| {
                    let client = self.client.clone();
                    let url = format!("{}/api/v1/log/entries/tile/{}", self.rekor_url, idx);
                    async move {
                        let resp = client.get(&url).send().await;
                        (idx, resp)
                    }
                })
                .collect();

            let results = futures::future::join_all(fetches).await;

            for (idx, resp_result) in results {
                match resp_result {
                    Ok(resp) if resp.status().is_success() => {
                        if let Ok(bytes) = resp.bytes().await {
                            let tile = bytes.to_vec();
                            let key = format!("tile:{}", idx);
                            let _ = self.cache_tile(&key, &tile);
                            result.insert(idx, tile);
                        }
                    }
                    _ => {
                        debug!("Failed to fetch tile {}", idx);
                    }
                }
            }
        }

        Ok(result)
    }

    /// Get the latest checkpoint (Signed Tree Head)
    pub async fn get_checkpoint(&self) -> Result<RekorCheckpoint> {
        let key = "checkpoint:latest";

        // Check if we have a recent checkpoint
        if let Some(cached) = self.get_cached(key)? {
            if let Ok(checkpoint) = serde_json::from_slice::<CachedCheckpoint>(&cached) {
                let age = SystemTime::now()
                    .duration_since(UNIX_EPOCH)
                    .unwrap_or_default()
                    .as_secs()
                    - checkpoint.timestamp;

                if age < self.config.checkpoint_max_age_secs {
                    debug!("Using cached checkpoint (age: {}s)", age);
                    return Ok(checkpoint.checkpoint);
                }
            }
        }

        // Fetch fresh checkpoint
        let checkpoint = self.fetch_checkpoint().await?;

        // Cache with timestamp
        let cached = CachedCheckpoint {
            checkpoint: checkpoint.clone(),
            timestamp: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
        };
        let _ = self.cache_tile(key, &serde_json::to_vec(&cached).unwrap_or_default());

        Ok(checkpoint)
    }

    /// Cache a verification verdict for an artifact
    pub fn cache_verdict(&self, artifact_hash: &str, verdict: &VerificationVerdict) -> Result<()> {
        let key = format!("verdict:{}", artifact_hash);
        let value = serde_json::to_vec(verdict)
            .map_err(|e| AuditorError::Sigstore(format!("Failed to serialize verdict: {}", e)))?;
        self.cache_tile(&key, &value)
    }

    /// Get a cached verdict for an artifact
    pub fn get_cached_verdict(&self, artifact_hash: &str) -> Result<Option<VerificationVerdict>> {
        let key = format!("verdict:{}", artifact_hash);
        if let Some(data) = self.get_cached(&key)? {
            let verdict: VerificationVerdict = serde_json::from_slice(&data)
                .map_err(|e| AuditorError::Sigstore(format!("Failed to deserialize verdict: {}", e)))?;
            return Ok(Some(verdict));
        }
        Ok(None)
    }

    fn get_cached(&self, key: &str) -> Result<Option<Vec<u8>>> {
        let read_txn = self.db.begin_read().map_err(|e| {
            AuditorError::Sigstore(format!("Failed to begin read transaction: {}", e))
        })?;
        let table = read_txn.open_table(TILE_CACHE).map_err(|e| {
            AuditorError::Sigstore(format!("Failed to open tile cache table: {}", e))
        })?;

        match table.get(key) {
            Ok(Some(value)) => Ok(Some(value.value().to_vec())),
            Ok(None) => Ok(None),
            Err(e) => Err(AuditorError::Sigstore(format!("Failed to read from cache: {}", e))),
        }
    }

    fn cache_tile(&self, key: &str, value: &[u8]) -> Result<()> {
        let write_txn = self.db.begin_write().map_err(|e| {
            AuditorError::Sigstore(format!("Failed to begin write transaction: {}", e))
        })?;
        {
            let mut table = write_txn.open_table(TILE_CACHE).map_err(|e| {
                AuditorError::Sigstore(format!("Failed to open tile cache table: {}", e))
            })?;
            table.insert(key, value).map_err(|e| {
                AuditorError::Sigstore(format!("Failed to insert into cache: {}", e))
            })?;
        }
        write_txn.commit().map_err(|e| {
            AuditorError::Sigstore(format!("Failed to commit cache write: {}", e))
        })?;
        Ok(())
    }

    async fn fetch_tile(&self, tile_index: u64) -> Result<Vec<u8>> {
        let url = format!("{}/api/v1/log/entries/tile/{}", self.rekor_url, tile_index);
        let response = self.client.get(&url).send().await?;

        if !response.status().is_success() {
            return Err(AuditorError::Sigstore(format!(
                "Failed to fetch tile {}: {}",
                tile_index,
                response.status()
            )));
        }

        Ok(response.bytes().await?.to_vec())
    }

    async fn fetch_checkpoint(&self) -> Result<RekorCheckpoint> {
        let url = format!("{}/api/v1/log", self.rekor_url);
        let response = self.client.get(&url).send().await?;

        if !response.status().is_success() {
            return Err(AuditorError::Sigstore(format!(
                "Failed to fetch checkpoint: {}",
                response.status()
            )));
        }

        let data: serde_json::Value = response.json().await?;

        Ok(RekorCheckpoint {
            tree_size: data.get("treeSize").and_then(|v| v.as_u64()).unwrap_or(0),
            root_hash: data
                .get("rootHash")
                .and_then(|v| v.as_str())
                .unwrap_or("")
                .to_string(),
            timestamp: data
                .get("timestamp")
                .and_then(|v| v.as_u64())
                .unwrap_or(0),
        })
    }
}

/// Rekor checkpoint (Signed Tree Head)
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RekorCheckpoint {
    pub tree_size: u64,
    pub root_hash: String,
    pub timestamp: u64,
}

/// Cached checkpoint with fetch timestamp
#[derive(Debug, Serialize, Deserialize)]
struct CachedCheckpoint {
    checkpoint: RekorCheckpoint,
    timestamp: u64,
}

/// Verification verdict cached for an artifact
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VerificationVerdict {
    /// Whether verification succeeded
    pub verified: bool,
    /// SLSA level achieved
    pub slsa_level: u8,
    /// Builder identity
    pub builder: Option<String>,
    /// Source repository
    pub source_repo: Option<String>,
    /// Source commit
    pub source_commit: Option<String>,
    /// Verification timestamp
    pub verified_at: u64,
    /// Any errors encountered
    pub errors: Vec<String>,
}

/// GitHub Release attestation discoverer.
///
/// Discovers SLSA attestations attached to GitHub Releases,
/// which is the primary location for Rust crate attestations.
pub struct GitHubAttestationDiscovery {
    client: Client,
}

impl GitHubAttestationDiscovery {
    pub fn new() -> Self {
        Self {
            client: Client::new(),
        }
    }

    /// Discover attestation URL for a crate from its GitHub repository.
    ///
    /// Follows the pattern: `https://github.com/{owner}/{repo}/releases/download/v{version}/provenance.intoto.jsonl`
    pub async fn discover_attestation(
        &self,
        repo_url: &str,
        version: &str,
    ) -> Result<Option<AttestationBundle>> {
        // Parse GitHub repository URL
        let (owner, repo) = self.parse_github_url(repo_url)?;

        // Try common attestation file patterns
        let patterns = [
            format!(
                "https://github.com/{}/{}/releases/download/v{}/provenance.intoto.jsonl",
                owner, repo, version
            ),
            format!(
                "https://github.com/{}/{}/releases/download/{}/provenance.intoto.jsonl",
                owner, repo, version
            ),
            format!(
                "https://github.com/{}/{}/releases/download/v{}/{}-{}.intoto.jsonl",
                owner, repo, version, repo, version
            ),
            // SLSA GitHub Generator pattern
            format!(
                "https://github.com/{}/{}/releases/download/v{}/multiple.intoto.jsonl",
                owner, repo, version
            ),
        ];

        for pattern in &patterns {
            debug!("Trying attestation URL: {}", pattern);
            match self.fetch_attestation(pattern).await {
                Ok(bundle) => {
                    info!("Found attestation at: {}", pattern);
                    return Ok(Some(bundle));
                }
                Err(_) => continue,
            }
        }

        // Try GitHub Attestations API (newer mechanism)
        if let Ok(Some(bundle)) = self.fetch_github_attestations_api(&owner, &repo, version).await {
            return Ok(Some(bundle));
        }

        Ok(None)
    }

    /// Fetch attestation from URL
    async fn fetch_attestation(&self, url: &str) -> Result<AttestationBundle> {
        let response = self
            .client
            .get(url)
            .header("Accept", "application/json")
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(AuditorError::Sigstore(format!(
                "Failed to fetch attestation: {}",
                response.status()
            )));
        }

        let text = response.text().await?;

        // Parse as JSONL (multiple JSON objects, one per line)
        let mut statements = Vec::new();
        for line in text.lines() {
            if line.trim().is_empty() {
                continue;
            }
            if let Ok(stmt) = serde_json::from_str::<InTotoStatement>(line) {
                statements.push(stmt);
            }
        }

        if statements.is_empty() {
            return Err(AuditorError::Sigstore("No valid attestations found".into()));
        }

        Ok(AttestationBundle {
            statements,
            source_url: url.to_string(),
        })
    }

    /// Fetch from GitHub Attestations API
    async fn fetch_github_attestations_api(
        &self,
        owner: &str,
        repo: &str,
        version: &str,
    ) -> Result<Option<AttestationBundle>> {
        let url = format!(
            "https://api.github.com/repos/{}/{}/attestations?per_page=100",
            owner, repo
        );

        let response = self
            .client
            .get(&url)
            .header("Accept", "application/vnd.github+json")
            .header("User-Agent", "sec_auditor/0.1.0")
            .send()
            .await?;

        if !response.status().is_success() {
            return Ok(None);
        }

        let data: serde_json::Value = response.json().await?;

        let attestations = data
            .get("attestations")
            .and_then(|a| a.as_array())
            .unwrap_or(&Vec::new())
            .clone();

        let mut statements = Vec::new();
        for attestation in attestations {
            // Check if this attestation matches our version
            if let Some(bundle) = attestation.get("bundle") {
                if let Ok(stmt) = serde_json::from_value::<InTotoStatement>(bundle.clone()) {
                    // Check if any subject matches our version
                    let version_match = stmt.subject.iter().any(|s| {
                        s.name.contains(version) || s.name.contains(&format!("v{}", version))
                    });
                    if version_match {
                        statements.push(stmt);
                    }
                }
            }
        }

        if statements.is_empty() {
            return Ok(None);
        }

        Ok(Some(AttestationBundle {
            statements,
            source_url: url,
        }))
    }

    fn parse_github_url(&self, url: &str) -> Result<(String, String)> {
        // Handle various GitHub URL formats
        let url = url
            .trim_end_matches('/')
            .trim_end_matches(".git")
            .replace("git@github.com:", "https://github.com/");

        let parts: Vec<&str> = url.split('/').collect();
        let len = parts.len();

        if len >= 2 {
            let owner = parts[len - 2].to_string();
            let repo = parts[len - 1].to_string();
            return Ok((owner, repo));
        }

        Err(AuditorError::Sigstore(format!(
            "Invalid GitHub URL: {}",
            url
        )))
    }
}

impl Default for GitHubAttestationDiscovery {
    fn default() -> Self {
        Self::new()
    }
}

/// Attestation bundle containing multiple in-toto statements
#[derive(Debug, Clone)]
pub struct AttestationBundle {
    pub statements: Vec<InTotoStatement>,
    pub source_url: String,
}

/// In-toto attestation statement
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct InTotoStatement {
    #[serde(rename = "_type")]
    pub statement_type: String,
    pub predicate_type: String,
    pub subject: Vec<Subject>,
    pub predicate: serde_json::Value,
}

/// Subject of an attestation
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Subject {
    pub name: String,
    pub digest: HashMap<String, String>,
}

/// SLSA provenance verifier with Rekor v2 support.
pub struct SlsaVerifier {
    /// HTTP client
    client: Client,

    /// Trusted builder identities
    trusted_builders: Vec<String>,

    /// Rekor tile cache
    tile_cache: Option<RekorTileCache>,

    /// GitHub attestation discoverer
    attestation_discovery: GitHubAttestationDiscovery,

    /// Rekor transparency log URL
    rekor_url: String,
}

impl SlsaVerifier {
    /// Create a new SLSA verifier.
    pub fn new() -> Self {
        let tile_cache = RekorTileCache::new(TileCacheConfig::default()).ok();

        Self {
            client: Client::new(),
            trusted_builders: vec![
                "https://github.com/slsa-framework/slsa-github-generator".to_string(),
                "https://github.com/actions/runner".to_string(),
                "https://token.actions.githubusercontent.com".to_string(),
            ],
            tile_cache,
            attestation_discovery: GitHubAttestationDiscovery::new(),
            rekor_url: "https://rekor.sigstore.dev".to_string(),
        }
    }

    /// Create verifier with custom cache configuration
    pub fn with_cache_config(config: TileCacheConfig) -> Result<Self> {
        let tile_cache = RekorTileCache::new(config)?;

        Ok(Self {
            client: Client::new(),
            trusted_builders: vec![
                "https://github.com/slsa-framework/slsa-github-generator".to_string(),
                "https://github.com/actions/runner".to_string(),
                "https://token.actions.githubusercontent.com".to_string(),
            ],
            tile_cache: Some(tile_cache),
            attestation_discovery: GitHubAttestationDiscovery::new(),
            rekor_url: "https://rekor.sigstore.dev".to_string(),
        })
    }

    /// Add a trusted builder identity.
    pub fn add_trusted_builder(&mut self, builder: String) {
        self.trusted_builders.push(builder);
    }

    /// Verify provenance for a crate from crates.io.
    pub async fn verify_crate(&self, name: &str, version: &str) -> Result<Provenance> {
        info!("Verifying provenance for crate {}@{}", name, version);

        // Get crate metadata from crates.io
        let crate_info = self.get_crate_info(name, version).await?;

        // Check verdict cache first
        if let Some(ref cache) = self.tile_cache {
            if let Some(ref checksum) = crate_info.checksum {
                if let Ok(Some(verdict)) = cache.get_cached_verdict(checksum) {
                    debug!("Using cached verdict for {}@{}", name, version);
                    return Ok(self.verdict_to_provenance(&verdict));
                }
            }
        }

        let mut provenance = Provenance::default();

        // Try to discover and verify attestation from GitHub Releases
        if let Some(ref repo_url) = crate_info.repository {
            match self
                .attestation_discovery
                .discover_attestation(repo_url, version)
                .await
            {
                Ok(Some(bundle)) => {
                    provenance = self.verify_attestation_bundle(&bundle, &crate_info)?;
                }
                Ok(None) => {
                    provenance.errors.push(format!(
                        "No attestation found in GitHub releases for {}",
                        repo_url
                    ));
                }
                Err(e) => {
                    provenance
                        .errors
                        .push(format!("Failed to discover attestation: {}", e));
                }
            }
        } else {
            provenance
                .errors
                .push("No repository URL in crate metadata".to_string());
        }

        // Check Rekor transparency log
        if let Some(checksum) = &crate_info.checksum {
            match self.search_rekor_with_cache(checksum).await {
                Ok(entries) if !entries.is_empty() => {
                    provenance.signature_valid = true;
                    if let Some(entry) = entries.first() {
                        provenance.build_time = entry.integrated_time;
                    }
                }
                Ok(_) => {
                    debug!("No Rekor entries found for checksum");
                }
                Err(e) => {
                    provenance
                        .errors
                        .push(format!("Rekor search failed: {}", e));
                }
            }

            // Cache the verdict
            if let Some(ref cache) = self.tile_cache {
                let verdict = self.provenance_to_verdict(&provenance);
                let _ = cache.cache_verdict(checksum, &verdict);
            }
        }

        Ok(provenance)
    }

    /// Verify multiple crates in batch (leveraging tile deduplication)
    pub async fn verify_crates_batch(
        &self,
        crates: &[(&str, &str)],
    ) -> Result<Vec<(String, Provenance)>> {
        let mut results = Vec::new();

        // Collect all checksums and their tile indices
        let mut crate_infos = Vec::new();
        for (name, version) in crates {
            match self.get_crate_info(name, version).await {
                Ok(info) => crate_infos.push((name.to_string(), version.to_string(), info)),
                Err(e) => {
                    let mut prov = Provenance::default();
                    prov.errors.push(format!("Failed to get crate info: {}", e));
                    results.push((format!("{}@{}", name, version), prov));
                }
            }
        }

        // Batch Rekor lookups (tiles are deduplicated automatically)
        for (name, version, info) in crate_infos {
            let provenance = self
                .verify_crate_with_info(&name, &version, &info)
                .await
                .unwrap_or_else(|e| {
                    let mut p = Provenance::default();
                    p.errors.push(e.to_string());
                    p
                });
            results.push((format!("{}@{}", name, version), provenance));
        }

        Ok(results)
    }

    async fn verify_crate_with_info(
        &self,
        name: &str,
        version: &str,
        crate_info: &CrateInfo,
    ) -> Result<Provenance> {
        let mut provenance = Provenance::default();

        // Try to discover and verify attestation
        if let Some(ref repo_url) = crate_info.repository {
            match self
                .attestation_discovery
                .discover_attestation(repo_url, version)
                .await
            {
                Ok(Some(bundle)) => {
                    provenance = self.verify_attestation_bundle(&bundle, crate_info)?;
                }
                Ok(None) => {
                    provenance.errors.push("No attestation found".to_string());
                }
                Err(e) => {
                    provenance
                        .errors
                        .push(format!("Attestation discovery failed: {}", e));
                }
            }
        }

        // Check Rekor
        if let Some(checksum) = &crate_info.checksum {
            if let Ok(entries) = self.search_rekor_with_cache(checksum).await {
                if !entries.is_empty() {
                    provenance.signature_valid = true;
                }
            }
        }

        Ok(provenance)
    }

    /// Get crate info from crates.io.
    async fn get_crate_info(&self, name: &str, version: &str) -> Result<CrateInfo> {
        let url = format!("https://crates.io/api/v1/crates/{}/{}", name, version);

        let response = self
            .client
            .get(&url)
            .header("User-Agent", "sec_auditor/0.1.0")
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(AuditorError::Sigstore(format!(
                "Failed to get crate info: {}",
                response.status()
            )));
        }

        let data: serde_json::Value = response.json().await?;

        let version_info = data
            .get("version")
            .ok_or_else(|| AuditorError::Sigstore("Missing version info".to_string()))?;

        let crate_info = data.get("crate");

        Ok(CrateInfo {
            name: name.to_string(),
            version: version.to_string(),
            checksum: version_info
                .get("checksum")
                .and_then(|v| v.as_str())
                .map(|s| s.to_string()),
            dl_path: version_info
                .get("dl_path")
                .and_then(|v| v.as_str())
                .map(|s| s.to_string()),
            repository: crate_info
                .and_then(|c| c.get("repository"))
                .and_then(|v| v.as_str())
                .map(|s| s.to_string()),
        })
    }

    /// Verify an attestation bundle
    fn verify_attestation_bundle(
        &self,
        bundle: &AttestationBundle,
        crate_info: &CrateInfo,
    ) -> Result<Provenance> {
        let mut provenance = Provenance::default();

        for statement in &bundle.statements {
            // Check predicate type for SLSA provenance
            if statement.predicate_type.contains("slsa.dev/provenance") {
                provenance.slsa_level = provenance.slsa_level.max(1);

                // Extract builder information
                if let Some(builder) = statement.predicate.get("builder") {
                    if let Some(id) = builder.get("id").and_then(|v| v.as_str()) {
                        provenance.builder = Some(id.to_string());

                        // Check if builder is trusted
                        if self.trusted_builders.iter().any(|b| id.contains(b)) {
                            provenance.slsa_level = provenance.slsa_level.max(2);
                        } else {
                            provenance
                                .errors
                                .push(format!("Untrusted builder: {}", id));
                        }
                    }
                }

                // Extract build definition
                if let Some(build_def) = statement.predicate.get("buildDefinition") {
                    // Extract resolved dependencies for source info
                    if let Some(deps) = build_def.get("resolvedDependencies") {
                        if let Some(deps_arr) = deps.as_array() {
                            for dep in deps_arr {
                                if let Some(uri) = dep.get("uri").and_then(|v| v.as_str()) {
                                    if uri.contains("github.com") {
                                        provenance.source_repo = Some(uri.to_string());
                                        if let Some(digest) = dep.get("digest") {
                                            provenance.source_commit = digest
                                                .get("sha1")
                                                .or(digest.get("gitCommit"))
                                                .and_then(|v| v.as_str())
                                                .map(|s| s.to_string());
                                        }
                                    }
                                }
                            }
                        }
                    }
                }

                // Verify subject digest matches crate checksum
                if let Some(checksum) = &crate_info.checksum {
                    let matches = statement.subject.iter().any(|s| {
                        s.digest
                            .get("sha256")
                            .map(|d| d.to_lowercase() == checksum.to_lowercase())
                            .unwrap_or(false)
                    });

                    if matches {
                        provenance.signature_valid = true;
                    } else {
                        provenance
                            .errors
                            .push("Subject digest mismatch".to_string());
                    }
                }
            }
        }

        Ok(provenance)
    }

    /// Search Rekor with tile caching
    async fn search_rekor_with_cache(&self, checksum: &str) -> Result<Vec<RekorEntry>> {
        // Use tile cache if available
        if let Some(ref cache) = self.tile_cache {
            // Get current checkpoint
            let checkpoint = cache.get_checkpoint().await?;
            debug!(
                "Rekor tree size: {}, searching for {}",
                checkpoint.tree_size, checksum
            );
        }

        // Fall back to API search (tiles are for proof verification, not search)
        self.search_rekor(checksum).await
    }

    /// Search Rekor transparency log for an artifact.
    async fn search_rekor(&self, checksum: &str) -> Result<Vec<RekorEntry>> {
        let url = format!("{}/api/v1/index/retrieve", self.rekor_url);

        let request = serde_json::json!({
            "hash": format!("sha256:{}", checksum)
        });

        let response = self.client.post(&url).json(&request).send().await?;

        if !response.status().is_success() {
            if response.status() == reqwest::StatusCode::NOT_FOUND {
                return Ok(Vec::new());
            }
            return Err(AuditorError::Sigstore(format!(
                "Rekor search failed: {}",
                response.status()
            )));
        }

        let uuids: Vec<String> = response.json().await?;

        let mut entries = Vec::new();
        for uuid in uuids.into_iter().take(5) {
            if let Ok(entry) = self.get_rekor_entry(&uuid).await {
                entries.push(entry);
            }
        }

        Ok(entries)
    }

    /// Get a specific Rekor entry.
    async fn get_rekor_entry(&self, uuid: &str) -> Result<RekorEntry> {
        let url = format!("{}/api/v1/log/entries/{}", self.rekor_url, uuid);

        let response = self.client.get(&url).send().await?;

        if !response.status().is_success() {
            return Err(AuditorError::Sigstore(format!(
                "Failed to get Rekor entry: {}",
                response.status()
            )));
        }

        let data: serde_json::Value = response.json().await?;

        // Parse the entry
        if let Some((_, entry_value)) = data.as_object().and_then(|o| o.iter().next()) {
            let integrated_time = entry_value
                .get("integratedTime")
                .and_then(|v| v.as_i64())
                .and_then(|ts| chrono::DateTime::from_timestamp(ts, 0))
                .map(|dt| dt.with_timezone(&chrono::Utc));

            return Ok(RekorEntry {
                uuid: uuid.to_string(),
                integrated_time,
                body: entry_value.get("body").cloned(),
            });
        }

        Err(AuditorError::Sigstore(
            "Invalid Rekor entry format".to_string(),
        ))
    }

    /// Verify a local file against expected checksum.
    pub async fn verify_file_checksum(path: &Path, expected: &str) -> Result<bool> {
        let content = tokio::fs::read(path).await?;
        let mut hasher = Sha256::new();
        hasher.update(&content);
        let result = hasher.finalize();
        let actual = format!("{:x}", result);

        Ok(actual == expected.to_lowercase())
    }

    /// Generate findings for provenance issues.
    pub fn generate_findings(&self, crate_name: &str, provenance: &Provenance) -> Vec<Finding> {
        let mut findings = Vec::new();

        // No attestation
        if provenance.builder.is_none() && !provenance.errors.is_empty() {
            let finding = Finding::sast(
                "provenance/missing-attestation",
                "Missing Provenance Attestation",
                format!(
                    "Crate '{}' has no verifiable provenance attestation: {}",
                    crate_name,
                    provenance.errors.join("; ")
                ),
                Location::new("Cargo.lock".into(), 0, 0),
                Severity::Medium,
            )
            .with_confidence(Confidence::High)
            .with_remediation("Consider using crates that provide SLSA provenance attestations.");

            findings.push(finding);
        }

        // Untrusted builder
        if let Some(ref builder) = provenance.builder {
            if !self.trusted_builders.iter().any(|b| builder.contains(b)) {
                let finding = Finding::sast(
                    "provenance/untrusted-builder",
                    "Untrusted Build System",
                    format!(
                        "Crate '{}' was built by an untrusted builder: {}",
                        crate_name, builder
                    ),
                    Location::new("Cargo.lock".into(), 0, 0),
                    Severity::Low,
                )
                .with_confidence(Confidence::Medium)
                .with_remediation(
                    "Verify the build system is trustworthy or use crates built by trusted systems.",
                );

                findings.push(finding);
            }
        }

        // Low SLSA level
        if provenance.slsa_level < 2 && provenance.builder.is_some() {
            let finding = Finding::sast(
                "provenance/low-slsa-level",
                "Low SLSA Level",
                format!(
                    "Crate '{}' has SLSA level {} (recommended: 2+)",
                    crate_name, provenance.slsa_level
                ),
                Location::new("Cargo.lock".into(), 0, 0),
                Severity::Low,
            )
            .with_confidence(Confidence::High)
            .with_remediation(
                "Prefer crates with SLSA Level 2 or higher for better supply chain security.",
            );

            findings.push(finding);
        }

        // Invalid signature
        if provenance.builder.is_some() && !provenance.signature_valid {
            let finding = Finding::sast(
                "provenance/invalid-signature",
                "Invalid Provenance Signature",
                format!(
                    "Crate '{}' has provenance but signature verification failed",
                    crate_name
                ),
                Location::new("Cargo.lock".into(), 0, 0),
                Severity::High,
            )
            .with_confidence(Confidence::High)
            .with_remediation("This may indicate tampering. Investigate before using this crate.");

            findings.push(finding);
        }

        findings
    }

    fn provenance_to_verdict(&self, provenance: &Provenance) -> VerificationVerdict {
        VerificationVerdict {
            verified: provenance.signature_valid,
            slsa_level: provenance.slsa_level,
            builder: provenance.builder.clone(),
            source_repo: provenance.source_repo.clone(),
            source_commit: provenance.source_commit.clone(),
            verified_at: SystemTime::now()
                .duration_since(UNIX_EPOCH)
                .unwrap_or_default()
                .as_secs(),
            errors: provenance.errors.clone(),
        }
    }

    fn verdict_to_provenance(&self, verdict: &VerificationVerdict) -> Provenance {
        Provenance {
            slsa_level: verdict.slsa_level,
            signature_valid: verdict.verified,
            builder: verdict.builder.clone(),
            source_repo: verdict.source_repo.clone(),
            source_commit: verdict.source_commit.clone(),
            build_time: None, // Not stored in verdict
            errors: verdict.errors.clone(),
        }
    }
}

impl Default for SlsaVerifier {
    fn default() -> Self {
        Self::new()
    }
}

/// Information about a crate.
#[derive(Debug)]
struct CrateInfo {
    name: String,
    version: String,
    checksum: Option<String>,
    dl_path: Option<String>,
    repository: Option<String>,
}

/// Entry from the Rekor transparency log.
#[derive(Debug)]
struct RekorEntry {
    uuid: String,
    integrated_time: Option<chrono::DateTime<chrono::Utc>>,
    body: Option<serde_json::Value>,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_file_checksum() {
        use std::io::Write;
        let mut file = tempfile::NamedTempFile::new().unwrap();
        file.write_all(b"test content").unwrap();

        // SHA256 of "test content"
        let expected = "6ae8a75555209fd6c44157c0aed8016e763ff435a19cf186f76863140143ff72";
        assert!(SlsaVerifier::verify_file_checksum(file.path(), expected)
            .await
            .unwrap());
    }

    #[test]
    fn test_github_url_parsing() {
        let discovery = GitHubAttestationDiscovery::new();

        let (owner, repo) = discovery
            .parse_github_url("https://github.com/tokio-rs/tokio")
            .unwrap();
        assert_eq!(owner, "tokio-rs");
        assert_eq!(repo, "tokio");

        let (owner, repo) = discovery
            .parse_github_url("https://github.com/serde-rs/serde.git")
            .unwrap();
        assert_eq!(owner, "serde-rs");
        assert_eq!(repo, "serde");

        let (owner, repo) = discovery
            .parse_github_url("git@github.com:rust-lang/rust.git")
            .unwrap();
        assert_eq!(owner, "rust-lang");
        assert_eq!(repo, "rust");
    }

    #[test]
    fn test_tile_cache_config_default() {
        let config = TileCacheConfig::default();
        assert_eq!(config.checkpoint_max_age_secs, 3600);
        assert_eq!(config.tile_size, 256);
    }

    #[test]
    fn test_verification_verdict_serialization() {
        let verdict = VerificationVerdict {
            verified: true,
            slsa_level: 2,
            builder: Some("https://github.com/actions/runner".to_string()),
            source_repo: Some("https://github.com/example/repo".to_string()),
            source_commit: Some("abc123".to_string()),
            verified_at: 1234567890,
            errors: vec![],
        };

        let json = serde_json::to_string(&verdict).unwrap();
        let parsed: VerificationVerdict = serde_json::from_str(&json).unwrap();

        assert_eq!(parsed.verified, verdict.verified);
        assert_eq!(parsed.slsa_level, verdict.slsa_level);
        assert_eq!(parsed.builder, verdict.builder);
    }
}

================================================================================
END: src\provenance\slsa.rs
================================================================================

================================================================================
FILE: src\ai\mod.rs
================================================================================
//! AI-driven vulnerability analysis module.
//!
//! This module provides AI-powered analysis capabilities using local LLM inference.
//! Note: Full candle integration requires additional setup and model files.
//! This implementation provides the interface and can use external LLM APIs as fallback.

use crate::error::{AuditorError, Result};
use crate::models::{Confidence, Finding, Language, Location, Severity};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::path::{Path, PathBuf};
use std::process::Command;
use tracing::{debug, warn};

/// AI analyzer for advanced vulnerability detection.
pub struct AiAnalyzer {
    /// HTTP client for API calls
    client: Client,

    /// Whether to use local inference (when available)
    use_local: bool,

    /// API endpoint for external LLM (optional fallback)
    api_endpoint: Option<String>,

    /// API key for external LLM
    api_key: Option<String>,

    /// Maximum tokens for context
    max_context_tokens: usize,
}

impl AiAnalyzer {
    /// Create a new AI analyzer.
    pub fn new() -> Self {
        Self {
            client: Client::new(),
            use_local: false, // Default to API since candle requires additional setup
            api_endpoint: std::env::var("LLM_API_ENDPOINT").ok(),
            api_key: std::env::var("LLM_API_KEY").ok(),
            max_context_tokens: 4096,
        }
    }

    /// Enable local inference (requires candle and model setup).
    pub fn with_local_inference(mut self, enabled: bool) -> Self {
        self.use_local = enabled;
        self
    }

    /// Set the API endpoint for external LLM.
    pub fn with_api_endpoint(mut self, endpoint: String) -> Self {
        self.api_endpoint = Some(endpoint);
        self
    }

    /// Set the API key for external LLM.
    pub fn with_api_key(mut self, key: String) -> Self {
        self.api_key = Some(key);
        self
    }

    /// Analyze a code snippet for vulnerabilities.
    pub async fn analyze_snippet(
        &self,
        code: &str,
        language: Language,
        context: &AnalysisContext,
    ) -> Result<Vec<Finding>> {
        if self.use_local {
            // Local inference not fully implemented - would use candle
            warn!("Local inference not available, falling back to heuristic analysis");
            return self.heuristic_analysis(code, language, context);
        }

        if let (Some(ref endpoint), Some(ref key)) = (&self.api_endpoint, &self.api_key) {
            return self.api_analysis(code, language, context, endpoint, key).await;
        }

        // Fallback to heuristic analysis
        self.heuristic_analysis(code, language, context)
    }

    /// Analyze using external LLM API.
    async fn api_analysis(
        &self,
        code: &str,
        language: Language,
        context: &AnalysisContext,
        endpoint: &str,
        api_key: &str,
    ) -> Result<Vec<Finding>> {
        let prompt = self.build_security_prompt(code, language, context);

        let request = LlmRequest {
            model: "gpt-4".to_string(), // Or configured model
            messages: vec![
                LlmMessage {
                    role: "system".to_string(),
                    content: SECURITY_SYSTEM_PROMPT.to_string(),
                },
                LlmMessage {
                    role: "user".to_string(),
                    content: prompt,
                },
            ],
            max_tokens: 1000,
            temperature: 0.1, // Low temperature for consistent security analysis
        };

        let response = self
            .client
            .post(endpoint)
            .header("Authorization", format!("Bearer {}", api_key))
            .json(&request)
            .send()
            .await?;

        if !response.status().is_success() {
            return Err(AuditorError::Analysis(format!(
                "LLM API error: {}",
                response.status()
            )));
        }

        let llm_response: LlmResponse = response.json().await?;

        // Parse the response into findings
        self.parse_llm_response(&llm_response, context)
    }

    /// Build a security-focused prompt for the LLM with Chain-of-Thought structure.
    fn build_security_prompt(&self, code: &str, language: Language, context: &AnalysisContext) -> String {
        let mut prompt = String::new();

        // Add few-shot examples for Rust to calibrate detection
        if language == Language::Rust {
            prompt.push_str(RUST_FEW_SHOT_EXAMPLES);
            prompt.push_str("\n\n---\n\n### NOW ANALYZE THE FOLLOWING CODE:\n\n");
        }

        prompt.push_str(&format!(
            "**Language**: {}\n",
            language
        ));

        if let Some(ref function_name) = context.function_name {
            prompt.push_str(&format!("**Function**: {}\n", function_name));
        }

        if !context.callers.is_empty() {
            prompt.push_str(&format!("**Called by**: {}\n", context.callers.join(", ")));
        }

        if !context.callees.is_empty() {
            prompt.push_str(&format!("**Calls**: {}\n", context.callees.join(", ")));
        }

        if !context.data_sources.is_empty() {
            prompt.push_str(&format!("**Data Sources**: {}\n", context.data_sources.join(", ")));
        }

        prompt.push_str("\n**Code to Analyze**:\n```");
        prompt.push_str(match language {
            Language::Rust => "rust",
            Language::Python => "python",
            Language::JavaScript | Language::TypeScript => "javascript",
            Language::Go => "go",
            _ => "",
        });
        prompt.push_str("\n");
        prompt.push_str(code);
        prompt.push_str("\n```\n\n");

        prompt.push_str(ANALYSIS_INSTRUCTIONS);

        prompt.push_str("\n\n**IMPORTANT**: Follow the Chain-of-Thought protocol. First analyze surface, invariants, data flow, and compiler checks. Only THEN determine if vulnerabilities exist. Output valid JSON.");

        prompt
    }

    /// Parse LLM response into findings with Chain-of-Thought metadata.
    fn parse_llm_response(&self, response: &LlmResponse, context: &AnalysisContext) -> Result<Vec<Finding>> {
        let content = response
            .choices
            .first()
            .map(|c| &c.message.content)
            .ok_or_else(|| AuditorError::Analysis("Empty LLM response".to_string()))?;

        // Parse structured response
        let mut findings = Vec::new();

        // Extract JSON from response (handle markdown code blocks)
        let json_content = Self::extract_json_from_response(content);

        // Try to parse as JSON first
        if let Ok(parsed) = serde_json::from_str::<LlmAnalysisResult>(&json_content) {
            // Log Chain-of-Thought reasoning for debugging
            if let Some(ref cot) = parsed.chain_of_thought {
                debug!("CoT Surface: {:?}", cot.surface_analysis);
                debug!("CoT Invariant: {:?}", cot.invariant_analysis);
                debug!("CoT DataFlow: {:?}", cot.data_flow);
                debug!("CoT Compiler: {:?}", cot.compiler_check);
            }

            for vuln in parsed.vulnerabilities {
                let severity = match vuln.severity.to_lowercase().as_str() {
                    "critical" => Severity::Critical,
                    "high" => Severity::High,
                    "medium" => Severity::Medium,
                    "low" => Severity::Low,
                    _ => Severity::Unknown,
                };

                let confidence = match vuln.confidence.to_lowercase().as_str() {
                    "high" => Confidence::High,
                    "medium" => Confidence::Medium,
                    _ => Confidence::Low,
                };

                let location = Location::new(
                    context.file_path.clone(),
                    vuln.line.unwrap_or(1),
                    1,
                );

                let mut finding = Finding::sast(
                    format!("ai/{}", vuln.vulnerability_type.to_lowercase().replace(' ', "-").replace(':', "-")),
                    &vuln.vulnerability_type,
                    &vuln.description,
                    location,
                    severity,
                )
                .with_confidence(confidence)
                .with_remediation(&vuln.remediation)
                .with_metadata("detection_method", serde_json::json!("llm_cot"));

                // Add evidence if provided
                if let Some(ref evidence) = vuln.evidence {
                    finding = finding.with_metadata("evidence", serde_json::json!(evidence));
                }

                // Add CWE ID if provided
                if let Some(ref cwe_id) = vuln.cwe_id {
                    finding = finding.with_metadata("cwe_id", serde_json::json!(cwe_id));
                }

                // Add Chain-of-Thought metadata if available
                if let Some(ref cot) = parsed.chain_of_thought {
                    if let Some(ref analysis) = cot.data_flow {
                        finding = finding.with_metadata("cot_data_flow", serde_json::json!(analysis));
                    }
                }

                findings.push(finding);
            }
        } else {
            // Fallback: parse free-form text response
            debug!("Could not parse structured response: {}", content);
        }

        Ok(findings)
    }

    /// Extract JSON from a response that may be wrapped in markdown code blocks.
    fn extract_json_from_response(content: &str) -> String {
        // Try to find JSON in code blocks
        if let Some(start) = content.find("```json") {
            let after_marker = &content[start + 7..];
            if let Some(end) = after_marker.find("```") {
                return after_marker[..end].trim().to_string();
            }
        }

        // Try plain code blocks
        if let Some(start) = content.find("```") {
            let after_marker = &content[start + 3..];
            // Skip language identifier if present
            let json_start = after_marker.find('\n').map(|i| i + 1).unwrap_or(0);
            let after_newline = &after_marker[json_start..];
            if let Some(end) = after_newline.find("```") {
                return after_newline[..end].trim().to_string();
            }
        }

        // Try to find raw JSON object
        if let Some(start) = content.find('{') {
            if let Some(end) = content.rfind('}') {
                if end > start {
                    return content[start..=end].to_string();
                }
            }
        }

        // Return as-is
        content.to_string()
    }

    /// Heuristic analysis without LLM.
    fn heuristic_analysis(
        &self,
        code: &str,
        language: Language,
        context: &AnalysisContext,
    ) -> Result<Vec<Finding>> {
        let mut findings = Vec::new();

        // Pattern-based heuristics for common issues
        let patterns = match language {
            Language::Rust => vec![
                ("std::mem::transmute", "Potential type confusion", Severity::High),
                ("ptr::write_volatile", "Volatile write may have race conditions", Severity::Medium),
                ("from_raw_parts", "Unchecked slice creation from raw parts", Severity::High),
                ("ManuallyDrop", "Manual memory management may leak", Severity::Medium),
                ("UnsafeCell", "Interior mutability may cause data races", Severity::Medium),
            ],
            Language::Python => vec![
                ("os.system", "Command injection risk", Severity::High),
                ("subprocess.call", "Shell command execution", Severity::Medium),
                ("yaml.load", "Unsafe YAML loading (use safe_load)", Severity::High),
                ("marshal.loads", "Unsafe deserialization", Severity::Critical),
                ("input(", "Python 2 input() executes code", Severity::High),
            ],
            Language::JavaScript => vec![
                ("dangerouslySetInnerHTML", "React XSS risk", Severity::High),
                ("new Function(", "Dynamic function creation", Severity::High),
                ("setTimeout(", "Potential code injection if string", Severity::Medium),
                (".createContextualFragment", "DOM-based XSS", Severity::High),
                ("location.href =", "Open redirect potential", Severity::Medium),
            ],
            Language::Go => vec![
                ("cgo", "CGO introduces memory safety risks", Severity::Medium),
                ("reflect.SliceHeader", "Unsafe slice manipulation", Severity::High),
                ("unsafe.Pointer", "Unsafe pointer usage", Severity::High),
                ("//#nosec", "Security check disabled", Severity::Low),
            ],
            _ => vec![],
        };

        for (pattern, description, severity) in patterns {
            if code.contains(pattern) {
                let line = code
                    .lines()
                    .enumerate()
                    .find(|(_, l)| l.contains(pattern))
                    .map(|(i, _)| i + 1)
                    .unwrap_or(1);

                let location = Location::new(context.file_path.clone(), line, 1);

                let finding = Finding::sast(
                    format!("ai-heuristic/{}", pattern.replace("::", "-").replace('.', "-")),
                    format!("Heuristic: {}", pattern),
                    description,
                    location,
                    severity,
                )
                .with_confidence(Confidence::Low)
                .with_metadata("detection_method", serde_json::json!("heuristic"));

                findings.push(finding);
            }
        }

        // Check for common security anti-patterns
        findings.extend(self.check_security_antipatterns(code, language, context));

        Ok(findings)
    }

    /// Check for common security anti-patterns.
    fn check_security_antipatterns(
        &self,
        code: &str,
        language: Language,
        context: &AnalysisContext,
    ) -> Vec<Finding> {
        let mut findings = Vec::new();

        // Check for TODO/FIXME related to security
        for (line_num, line) in code.lines().enumerate() {
            let line_lower = line.to_lowercase();

            if (line_lower.contains("todo") || line_lower.contains("fixme") || line_lower.contains("hack"))
                && (line_lower.contains("security")
                    || line_lower.contains("vuln")
                    || line_lower.contains("auth")
                    || line_lower.contains("crypt")
                    || line_lower.contains("inject")
                    || line_lower.contains("xss"))
            {
                let location = Location::new(context.file_path.clone(), line_num + 1, 1)
                    .with_language(language);

                let finding = Finding::sast(
                    "ai-heuristic/security-todo",
                    "Security-Related TODO",
                    format!("Developer note indicates security concern: {}", line.trim()),
                    location,
                    Severity::Medium,
                )
                .with_confidence(Confidence::Medium)
                .with_remediation("Address the security concern noted in this comment.");

                findings.push(finding);
            }
        }

        // Check for disabled security features
        let disabled_patterns = [
            ("verify=False", "SSL verification disabled"),
            ("check=False", "Subprocess check disabled"),
            ("shell=True", "Shell execution enabled"),
            ("dangerouslySetInnerHTML", "React sanitization bypassed"),
            ("trustAllCerts", "Certificate validation disabled"),
            ("csrf_exempt", "CSRF protection disabled"),
            ("@nosecurity", "Security checks disabled"),
            ("CORS: *", "CORS allows all origins"),
        ];

        for (pattern, description) in disabled_patterns {
            if code.contains(pattern) {
                let line = code
                    .lines()
                    .enumerate()
                    .find(|(_, l)| l.contains(pattern))
                    .map(|(i, _)| i + 1)
                    .unwrap_or(1);

                let location = Location::new(context.file_path.clone(), line, 1)
                    .with_language(language);

                let finding = Finding::sast(
                    "ai-heuristic/disabled-security",
                    "Disabled Security Control",
                    description,
                    location,
                    Severity::High,
                )
                .with_confidence(Confidence::High);

                findings.push(finding);
            }
        }

        findings
    }

    /// Check if AI analysis is available.
    pub fn is_available(&self) -> bool {
        self.use_local || (self.api_endpoint.is_some() && self.api_key.is_some())
    }
}

impl Default for AiAnalyzer {
    fn default() -> Self {
        Self::new()
    }
}

/// Context for AI analysis.
#[derive(Debug, Clone, Default)]
pub struct AnalysisContext {
    /// File path being analyzed
    pub file_path: std::path::PathBuf,

    /// Function or method name
    pub function_name: Option<String>,

    /// Functions that call this code
    pub callers: Vec<String>,

    /// Functions called by this code
    pub callees: Vec<String>,

    /// Data flow information
    pub data_sources: Vec<String>,

    /// Additional context
    pub metadata: std::collections::HashMap<String, String>,
}

/// LLM API request structure.
#[derive(Debug, Serialize)]
struct LlmRequest {
    model: String,
    messages: Vec<LlmMessage>,
    max_tokens: u32,
    temperature: f32,
}

#[derive(Debug, Serialize, Deserialize)]
struct LlmMessage {
    role: String,
    content: String,
}

/// LLM API response structure.
#[derive(Debug, Deserialize)]
struct LlmResponse {
    choices: Vec<LlmChoice>,
}

#[derive(Debug, Deserialize)]
struct LlmChoice {
    message: LlmMessage,
}

/// Structured analysis result from LLM with Chain-of-Thought.
#[derive(Debug, Deserialize)]
struct LlmAnalysisResult {
    /// Chain-of-thought reasoning (optional for backward compatibility)
    #[serde(default)]
    chain_of_thought: Option<ChainOfThought>,
    /// Detected vulnerabilities
    vulnerabilities: Vec<LlmVulnerability>,
}

/// Chain-of-Thought reasoning structure.
#[derive(Debug, Deserialize, Default)]
struct ChainOfThought {
    #[serde(default)]
    surface_analysis: Option<String>,
    #[serde(default)]
    invariant_analysis: Option<String>,
    #[serde(default)]
    data_flow: Option<String>,
    #[serde(default)]
    compiler_check: Option<String>,
}

#[derive(Debug, Deserialize)]
struct LlmVulnerability {
    vulnerability_type: String,
    severity: String,
    confidence: String,
    description: String,
    #[serde(default)]
    line: Option<usize>,
    remediation: String,
    /// Evidence from the code
    #[serde(default)]
    evidence: Option<String>,
    /// CWE identifier
    #[serde(default)]
    cwe_id: Option<String>,
}

/// System prompt for security analysis with Chain-of-Thought reasoning.
///
/// This prompt implements the research-backed "Role + CoT + Protocol" structure
/// that reduces hallucinations and improves precision in vulnerability detection.
const SECURITY_SYSTEM_PROMPT: &str = r#"You are a specialized Security Auditor Agent. Your mission is to identify cryptographic failures, memory safety violations, injection flaws, and logic vulnerabilities with precision and minimal false positives.

### ANALYSIS PROTOCOL (Chain of Thought)

You MUST follow these steps in order before rendering a verdict:

**Step 1: Surface Analysis**
- Identify all public entry points (exported functions, API handlers, main)
- List all `unsafe` blocks, FFI calls, or dangerous function calls
- Note all external inputs (user data, network, files, environment)

**Step 2: Invariant Analysis**
For each `unsafe` block or dangerous operation:
- Explicitly state the safety contract required
- Identify what invariants must hold (pointer validity, bounds, lifetimes)
- Check if surrounding code upholds these invariants

**Step 3: Data Flow Tracing**
- Trace user-controlled inputs from sources to sinks
- Identify all transformations and sanitization steps
- Determine if tainted data reaches sensitive operations unsanitized

**Step 4: Compiler Verification (for Rust)**
- Specify if the potential vulnerability would be caught by the borrow checker
- Mark issues as "Compiler Enforced" if the type system prevents exploitation
- Only report as vulnerability if it bypasses compiler guarantees

**Step 5: Verdict**
- Only after completing Steps 1-4, determine if a vulnerability exists
- Assign confidence based on certainty of the data flow and invariant violations

### OUTPUT FORMAT

Respond STRICTLY in JSON with the following structure:
{
  "chain_of_thought": {
    "surface_analysis": "What entry points and dangerous operations were found",
    "invariant_analysis": "What safety contracts exist and are they upheld",
    "data_flow": "How does untrusted data flow through the code",
    "compiler_check": "What does the compiler guarantee (for Rust)"
  },
  "vulnerabilities": [
    {
      "vulnerability_type": "CWE-XXX: Name",
      "severity": "critical|high|medium|low",
      "confidence": "high|medium|low",
      "description": "Detailed technical description",
      "line": 42,
      "evidence": "The specific code pattern that indicates the vulnerability",
      "remediation": "Specific fix with code example if applicable",
      "cwe_id": "CWE-XXX"
    }
  ]
}

### SEVERITY GUIDELINES

- **Critical**: Remote code execution, authentication bypass, privilege escalation
- **High**: SQL injection, command injection, unsafe deserialization, memory corruption
- **Medium**: XSS, information disclosure, insecure defaults, path traversal
- **Low**: Information leakage, missing headers, minor configuration issues

### CONFIDENCE GUIDELINES

- **High**: Clear data flow from source to sink, missing sanitization is evident
- **Medium**: Likely vulnerability but depends on runtime conditions or external factors
- **Low**: Suspicious pattern but cannot confirm exploitability from static analysis

If no vulnerabilities are found, return:
{
  "chain_of_thought": { ... analysis ... },
  "vulnerabilities": []
}"#;

/// Few-shot examples for Rust vulnerability detection.
/// These calibrate the model's detection threshold and reduce false positives.
const RUST_FEW_SHOT_EXAMPLES: &str = r#"
### FEW-SHOT EXAMPLE 1: Use-After-Free (TRUE POSITIVE)

```rust
use std::cell::RefCell;
use std::rc::Rc;

fn vulnerable_uaf() {
    let data = Rc::new(RefCell::new(vec![1, 2, 3]));
    let reference = data.borrow();
    let ptr = reference.as_ptr();  // Raw pointer to inner data
    drop(reference);
    drop(data);  // Data is freed here
    unsafe {
        // Use-after-free: ptr is now dangling
        println!("{}", *ptr);  // CWE-416
    }
}
```

**Analysis:**
{
  "chain_of_thought": {
    "surface_analysis": "Found unsafe block with raw pointer dereference",
    "invariant_analysis": "Raw pointer requires underlying data to be alive; data is dropped before use",
    "data_flow": "ptr derived from data, data dropped, ptr dereferenced",
    "compiler_check": "Borrow checker cannot track raw pointers - NOT compiler enforced"
  },
  "vulnerabilities": [{
    "vulnerability_type": "CWE-416: Use After Free",
    "severity": "high",
    "confidence": "high",
    "description": "Raw pointer `ptr` is dereferenced after `data` is dropped, causing use-after-free",
    "line": 11,
    "evidence": "println!(\"{}\", *ptr) after drop(data)",
    "remediation": "Keep Rc alive while using raw pointer, or use safe references",
    "cwe_id": "CWE-416"
  }]
}

### FEW-SHOT EXAMPLE 2: Race Condition (TRUE POSITIVE)

```rust
use std::sync::{Arc, Mutex};
use std::thread;

fn vulnerable_race() {
    let counter = Arc::new(Mutex::new(0));
    let mut handles = vec![];

    for _ in 0..10 {
        let counter = Arc::clone(&counter);
        let handle = thread::spawn(move || {
            let mut num = counter.lock().unwrap();
            let current = *num;  // Read value
            // Lock is dropped here due to temporary scope
            drop(num);
            // TOCTOU gap: another thread can modify counter here
            let new_counter = Arc::clone(&counter);
            let mut num = new_counter.lock().unwrap();
            *num = current + 1;  // Write stale value - race condition!
        });
        handles.push(handle);
    }
}
```

**Analysis:**
{
  "chain_of_thought": {
    "surface_analysis": "Mutex with multiple threads, lock acquired and released multiple times",
    "invariant_analysis": "Counter increment requires atomic read-modify-write; lock released between read and write",
    "data_flow": "Value read under lock, lock dropped, value written under new lock",
    "compiler_check": "Rust prevents data races but not logic races - race condition possible"
  },
  "vulnerabilities": [{
    "vulnerability_type": "CWE-362: Race Condition",
    "severity": "medium",
    "confidence": "high",
    "description": "TOCTOU vulnerability: Mutex lock is released between reading and writing counter, allowing stale value writes",
    "line": 14,
    "evidence": "drop(num) followed by separate lock acquisition and write",
    "remediation": "Keep lock held during entire read-modify-write: `*num += 1` without dropping lock",
    "cwe_id": "CWE-362"
  }]
}

### FEW-SHOT EXAMPLE 3: Safe Usage of Unsafe (FALSE POSITIVE - DO NOT REPORT)

```rust
fn safe_slice_from_raw_parts(data: &[u8], offset: usize, len: usize) -> Option<&[u8]> {
    // Bounds checking BEFORE unsafe operation
    if offset.checked_add(len)? > data.len() {
        return None;
    }

    // This unsafe is actually safe because:
    // 1. Bounds are verified above
    // 2. Source slice is borrowed, so data is valid
    // 3. Alignment is correct (u8 has alignment 1)
    unsafe {
        let ptr = data.as_ptr().add(offset);
        Some(std::slice::from_raw_parts(ptr, len))
    }
}
```

**Analysis:**
{
  "chain_of_thought": {
    "surface_analysis": "Found unsafe block with from_raw_parts and pointer arithmetic",
    "invariant_analysis": "from_raw_parts requires valid ptr, correct len, and proper alignment. All invariants upheld: bounds checked, ptr from valid slice, u8 alignment trivially satisfied",
    "data_flow": "Input bounds validated before any unsafe operation",
    "compiler_check": "Lifetime tied to input slice borrow - compiler ensures data validity"
  },
  "vulnerabilities": []
}

This is NOT a vulnerability because the bounds check before the unsafe block ensures all safety invariants are upheld.
"#;

/// Instructions for analysis.
const ANALYSIS_INSTRUCTIONS: &str = r#"
Analyze this code for security vulnerabilities. Consider:

1. **Input Validation**: Is user input properly validated and sanitized?
2. **Authentication**: Are there authentication bypass risks?
3. **Authorization**: Are access controls properly enforced?
4. **Injection**: Can malicious input affect execution (SQL, command, code)?
5. **Data Exposure**: Could sensitive data be leaked?
6. **Cryptography**: Are cryptographic operations secure?
7. **Race Conditions**: Are there TOCTOU or other timing issues?
8. **Error Handling**: Do errors reveal sensitive information?

Provide your analysis as structured JSON with vulnerabilities found.
"#;

// ============================================================================
// Compiler-in-the-Loop Feedback System
// ============================================================================

/// Result of running the compiler on code.
#[derive(Debug, Clone)]
pub struct CompilerFeedback {
    /// Whether the code compiled successfully.
    pub compiles: bool,
    /// Compiler errors if compilation failed.
    pub errors: Vec<CompilerDiagnostic>,
    /// Compiler warnings.
    pub warnings: Vec<CompilerDiagnostic>,
    /// Raw stderr output.
    pub raw_output: String,
    /// Time taken for compilation.
    pub compile_time_ms: u64,
}

/// A single compiler diagnostic (error or warning).
#[derive(Debug, Clone)]
pub struct CompilerDiagnostic {
    /// Diagnostic level (error, warning, note, help).
    pub level: DiagnosticLevel,
    /// Error/warning code (e.g., E0382, W0000).
    pub code: Option<String>,
    /// Primary message.
    pub message: String,
    /// File where the diagnostic occurred.
    pub file: Option<PathBuf>,
    /// Line number.
    pub line: Option<usize>,
    /// Column number.
    pub column: Option<usize>,
    /// Span of the problematic code.
    pub span: Option<String>,
    /// Suggested fix from the compiler.
    pub suggestion: Option<String>,
}

/// Diagnostic severity level.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DiagnosticLevel {
    Error,
    Warning,
    Note,
    Help,
}

/// Compiler feedback loop for validating LLM-generated fixes.
///
/// This implements a "compiler-in-the-loop" pattern where:
/// 1. LLM suggests a vulnerability fix
/// 2. We inject the fix into a temporary crate
/// 3. We run `cargo check` to validate the fix compiles
/// 4. We parse compiler feedback and return it to the LLM for refinement
pub struct CompilerFeedbackLoop {
    /// Directory for temporary crates.
    temp_dir: PathBuf,
    /// Cargo path (default: "cargo").
    cargo_path: String,
    /// Timeout for cargo check in seconds.
    timeout_secs: u64,
    /// Additional Cargo.toml dependencies to include.
    extra_dependencies: HashMap<String, String>,
    /// Edition to use (default: 2021).
    edition: String,
}

impl Default for CompilerFeedbackLoop {
    fn default() -> Self {
        Self::new()
    }
}

impl CompilerFeedbackLoop {
    /// Create a new compiler feedback loop.
    pub fn new() -> Self {
        Self {
            temp_dir: std::env::temp_dir().join("sec_auditor_cfl"),
            cargo_path: "cargo".to_string(),
            timeout_secs: 30,
            extra_dependencies: HashMap::new(),
            edition: "2021".to_string(),
        }
    }

    /// Set the temporary directory for crates.
    pub fn with_temp_dir(mut self, dir: PathBuf) -> Self {
        self.temp_dir = dir;
        self
    }

    /// Set the cargo path.
    pub fn with_cargo_path(mut self, path: String) -> Self {
        self.cargo_path = path;
        self
    }

    /// Set the compilation timeout.
    pub fn with_timeout(mut self, secs: u64) -> Self {
        self.timeout_secs = secs;
        self
    }

    /// Add extra dependencies for the temporary crate.
    pub fn with_dependency(mut self, name: &str, version: &str) -> Self {
        self.extra_dependencies.insert(name.to_string(), version.to_string());
        self
    }

    /// Check if Rust code compiles and return compiler feedback.
    pub fn check_rust_code(&self, code: &str) -> Result<CompilerFeedback> {
        let start = std::time::Instant::now();

        // Create temporary crate directory
        let crate_dir = self.create_temp_crate(code)?;

        // Run cargo check
        let output = self.run_cargo_check(&crate_dir)?;

        let compile_time_ms = start.elapsed().as_millis() as u64;

        // Parse compiler output
        let feedback = self.parse_compiler_output(&output, compile_time_ms);

        // Clean up (best effort)
        let _ = std::fs::remove_dir_all(&crate_dir);

        Ok(feedback)
    }

    /// Check if a code fix would compile when applied to existing code.
    pub fn validate_fix(
        &self,
        original_code: &str,
        fixed_code: &str,
        dependencies: &[(&str, &str)],
    ) -> Result<FixValidation> {
        // First check if original code compiles
        let original_feedback = self.check_rust_code(original_code)?;

        // Add dependencies and check fixed code
        let mut cfl = self.clone();
        for (name, version) in dependencies {
            cfl = cfl.with_dependency(name, version);
        }
        let fixed_feedback = cfl.check_rust_code(fixed_code)?;

        // Compute lengths before moving errors
        let original_error_count = original_feedback.errors.len();
        let fixed_error_count = fixed_feedback.errors.len();

        Ok(FixValidation {
            original_compiles: original_feedback.compiles,
            fixed_compiles: fixed_feedback.compiles,
            original_errors: original_error_count,
            fixed_errors: fixed_error_count,
            original_warnings: original_feedback.warnings.len(),
            fixed_warnings: fixed_feedback.warnings.len(),
            new_errors: fixed_feedback.errors,
            improvement: original_error_count as i32 - fixed_error_count as i32,
        })
    }

    /// Create a temporary crate with the given code.
    fn create_temp_crate(&self, code: &str) -> Result<PathBuf> {
        // Create unique directory
        let crate_name = format!("cfl_{}", uuid::Uuid::new_v4().to_string().replace('-', ""));
        let crate_dir = self.temp_dir.join(&crate_name);

        std::fs::create_dir_all(&crate_dir).map_err(|e| {
            AuditorError::Analysis(format!("Failed to create temp crate dir: {}", e))
        })?;

        // Create Cargo.toml
        let mut deps = String::new();
        for (name, version) in &self.extra_dependencies {
            deps.push_str(&format!("{} = \"{}\"\n", name, version));
        }

        let cargo_toml = format!(
            r#"[package]
name = "{}"
version = "0.1.0"
edition = "{}"

[dependencies]
{}
"#,
            crate_name, self.edition, deps
        );

        std::fs::write(crate_dir.join("Cargo.toml"), cargo_toml).map_err(|e| {
            AuditorError::Analysis(format!("Failed to write Cargo.toml: {}", e))
        })?;

        // Create src directory
        let src_dir = crate_dir.join("src");
        std::fs::create_dir_all(&src_dir).map_err(|e| {
            AuditorError::Analysis(format!("Failed to create src dir: {}", e))
        })?;

        // Write main.rs or lib.rs
        let main_rs = if code.contains("fn main(") {
            code.to_string()
        } else {
            format!("{}\n\nfn main() {{}}", code)
        };

        std::fs::write(src_dir.join("main.rs"), main_rs).map_err(|e| {
            AuditorError::Analysis(format!("Failed to write main.rs: {}", e))
        })?;

        Ok(crate_dir)
    }

    /// Run cargo check on a crate directory.
    fn run_cargo_check(&self, crate_dir: &Path) -> Result<std::process::Output> {
        let output = Command::new(&self.cargo_path)
            .args(["check", "--message-format=json"])
            .current_dir(crate_dir)
            .output()
            .map_err(|e| AuditorError::Analysis(format!("Failed to run cargo check: {}", e)))?;

        Ok(output)
    }

    /// Parse compiler output into structured feedback.
    fn parse_compiler_output(&self, output: &std::process::Output, compile_time_ms: u64) -> CompilerFeedback {
        let stderr = String::from_utf8_lossy(&output.stderr).to_string();
        let stdout = String::from_utf8_lossy(&output.stdout).to_string();

        let mut errors = Vec::new();
        let mut warnings = Vec::new();

        // Parse JSON messages from stdout (cargo check --message-format=json)
        for line in stdout.lines() {
            if let Ok(msg) = serde_json::from_str::<CargoMessage>(line) {
                if let CargoMessage::CompilerMessage { message } = msg {
                    let level = match message.level.as_str() {
                        "error" => DiagnosticLevel::Error,
                        "warning" => DiagnosticLevel::Warning,
                        "note" => DiagnosticLevel::Note,
                        "help" => DiagnosticLevel::Help,
                        _ => continue,
                    };

                    let diag = CompilerDiagnostic {
                        level,
                        code: message.code.map(|c| c.code),
                        message: message.message,
                        file: message.spans.first().and_then(|s| {
                            s.file_name.as_ref().map(PathBuf::from)
                        }),
                        line: message.spans.first().map(|s| s.line_start),
                        column: message.spans.first().map(|s| s.column_start),
                        span: message.spans.first().and_then(|s| s.text.first().map(|t| t.text.clone())),
                        suggestion: message.children.iter()
                            .find(|c| c.level == "help")
                            .map(|c| c.message.clone()),
                    };

                    match level {
                        DiagnosticLevel::Error => errors.push(diag),
                        DiagnosticLevel::Warning => warnings.push(diag),
                        _ => {}
                    }
                }
            }
        }

        CompilerFeedback {
            compiles: output.status.success() && errors.is_empty(),
            errors,
            warnings,
            raw_output: stderr,
            compile_time_ms,
        }
    }

    /// Generate a prompt enhancement with compiler feedback for LLM.
    pub fn format_feedback_for_llm(&self, feedback: &CompilerFeedback) -> String {
        if feedback.compiles {
            return "The code compiles successfully with no errors.".to_string();
        }

        let mut prompt = String::new();
        prompt.push_str("## COMPILER FEEDBACK\n\n");
        prompt.push_str("The code does NOT compile. Here are the errors:\n\n");

        for (i, err) in feedback.errors.iter().enumerate() {
            prompt.push_str(&format!("### Error {} ", i + 1));
            if let Some(ref code) = err.code {
                prompt.push_str(&format!("[{}]", code));
            }
            prompt.push('\n');

            prompt.push_str(&format!("**Message**: {}\n", err.message));

            if let (Some(ref file), Some(line)) = (&err.file, err.line) {
                prompt.push_str(&format!("**Location**: {}:{}\n", file.display(), line));
            }

            if let Some(ref span) = err.span {
                prompt.push_str(&format!("**Code**: `{}`\n", span));
            }

            if let Some(ref suggestion) = err.suggestion {
                prompt.push_str(&format!("**Suggestion**: {}\n", suggestion));
            }

            prompt.push('\n');
        }

        if !feedback.warnings.is_empty() {
            prompt.push_str(&format!("\nThere are also {} warnings.\n", feedback.warnings.len()));
        }

        prompt.push_str("\nPlease revise your fix to address these compiler errors.");

        prompt
    }
}

impl Clone for CompilerFeedbackLoop {
    fn clone(&self) -> Self {
        Self {
            temp_dir: self.temp_dir.clone(),
            cargo_path: self.cargo_path.clone(),
            timeout_secs: self.timeout_secs,
            extra_dependencies: self.extra_dependencies.clone(),
            edition: self.edition.clone(),
        }
    }
}

/// Result of validating a code fix.
#[derive(Debug, Clone)]
pub struct FixValidation {
    /// Whether the original code compiles.
    pub original_compiles: bool,
    /// Whether the fixed code compiles.
    pub fixed_compiles: bool,
    /// Number of errors in original code.
    pub original_errors: usize,
    /// Number of errors in fixed code.
    pub fixed_errors: usize,
    /// Number of warnings in original code.
    pub original_warnings: usize,
    /// Number of warnings in fixed code.
    pub fixed_warnings: usize,
    /// New errors introduced by the fix.
    pub new_errors: Vec<CompilerDiagnostic>,
    /// Improvement score (positive = fewer errors after fix).
    pub improvement: i32,
}

impl FixValidation {
    /// Check if the fix is valid (code compiles or improves).
    pub fn is_valid(&self) -> bool {
        self.fixed_compiles || self.improvement > 0
    }

    /// Check if the fix introduced new errors.
    pub fn introduced_errors(&self) -> bool {
        self.fixed_errors > self.original_errors
    }
}

// Cargo JSON message types
#[derive(Debug, Deserialize)]
#[serde(tag = "reason")]
enum CargoMessage {
    #[serde(rename = "compiler-message")]
    CompilerMessage { message: CompilerMessageInner },
    #[serde(other)]
    Other,
}

#[derive(Debug, Deserialize)]
struct CompilerMessageInner {
    level: String,
    code: Option<CompilerCode>,
    message: String,
    #[serde(default)]
    spans: Vec<CompilerSpan>,
    #[serde(default)]
    children: Vec<CompilerChild>,
}

#[derive(Debug, Deserialize)]
struct CompilerCode {
    code: String,
}

#[derive(Debug, Deserialize)]
struct CompilerSpan {
    #[serde(default)]
    file_name: Option<String>,
    #[serde(default)]
    line_start: usize,
    #[serde(default)]
    column_start: usize,
    #[serde(default)]
    text: Vec<CompilerText>,
}

#[derive(Debug, Deserialize)]
struct CompilerText {
    text: String,
}

#[derive(Debug, Deserialize)]
struct CompilerChild {
    level: String,
    message: String,
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_heuristic_analysis() {
        let analyzer = AiAnalyzer::new();
        let code = r#"
            import os
            os.system(user_input)  # Command injection!
        "#;

        let context = AnalysisContext {
            file_path: "test.py".into(),
            ..Default::default()
        };

        let findings = analyzer
            .heuristic_analysis(code, Language::Python, &context)
            .unwrap();

        assert!(!findings.is_empty());
        assert!(findings.iter().any(|f| f.description.contains("Command")));
    }

    #[test]
    fn test_security_todo_detection() {
        let analyzer = AiAnalyzer::new();
        let code = r#"
            // TODO: Fix this security vulnerability before release
            fn handle_auth() {}
        "#;

        let context = AnalysisContext {
            file_path: "test.rs".into(),
            ..Default::default()
        };

        let findings = analyzer.check_security_antipatterns(code, Language::Rust, &context);

        assert!(!findings.is_empty());
        assert!(findings.iter().any(|f| f.title.contains("TODO")));
    }

    #[test]
    fn test_compiler_feedback_valid_code() {
        let cfl = CompilerFeedbackLoop::new();

        let valid_code = r#"
fn add(a: i32, b: i32) -> i32 {
    a + b
}
"#;

        let feedback = cfl.check_rust_code(valid_code).unwrap();
        assert!(feedback.compiles, "Valid code should compile");
        assert!(feedback.errors.is_empty(), "Valid code should have no errors");
    }

    #[test]
    fn test_compiler_feedback_invalid_code() {
        let cfl = CompilerFeedbackLoop::new();

        let invalid_code = r#"
fn broken() {
    let x: String = 42;  // Type error: expected String, got i32
}
"#;

        let feedback = cfl.check_rust_code(invalid_code).unwrap();
        assert!(!feedback.compiles, "Invalid code should not compile");
        assert!(!feedback.errors.is_empty(), "Invalid code should have errors");
    }

    #[test]
    fn test_compiler_feedback_borrow_error() {
        let cfl = CompilerFeedbackLoop::new();

        let borrow_error_code = r#"
fn borrow_problem() {
    let mut s = String::from("hello");
    let r1 = &s;
    let r2 = &mut s;  // Error: cannot borrow as mutable while immutable borrow exists
    println!("{} {}", r1, r2);
}
"#;

        let feedback = cfl.check_rust_code(borrow_error_code).unwrap();
        assert!(!feedback.compiles, "Code with borrow error should not compile");
        // The error should be about borrowing
        let has_borrow_error = feedback.errors.iter().any(|e| {
            e.message.contains("borrow") || e.code.as_ref().map(|c| c == "E0502").unwrap_or(false)
        });
        assert!(has_borrow_error, "Should detect borrow error");
    }

    #[test]
    fn test_format_feedback_for_llm() {
        let cfl = CompilerFeedbackLoop::new();

        let feedback = CompilerFeedback {
            compiles: false,
            errors: vec![CompilerDiagnostic {
                level: DiagnosticLevel::Error,
                code: Some("E0382".to_string()),
                message: "use of moved value: `s`".to_string(),
                file: Some(PathBuf::from("src/main.rs")),
                line: Some(5),
                column: Some(10),
                span: Some("println!(\"{}\", s);".to_string()),
                suggestion: Some("consider cloning the value".to_string()),
            }],
            warnings: vec![],
            raw_output: String::new(),
            compile_time_ms: 100,
        };

        let prompt = cfl.format_feedback_for_llm(&feedback);

        assert!(prompt.contains("COMPILER FEEDBACK"));
        assert!(prompt.contains("E0382"));
        assert!(prompt.contains("use of moved value"));
        assert!(prompt.contains("consider cloning"));
    }
}

================================================================================
END: src\ai\mod.rs
================================================================================

================================================================================
FILE: src\privacy\mod.rs
================================================================================
//! Privacy-preserving code analysis module.
//!
//! This module provides privacy features for secure code analysis:
//! - Code anonymization pipeline for removing PII from code before LLM analysis
//! - Local-first inference tier using quantized models
//! - Identifier normalization to prevent data leakage
//!
//! # Privacy-First Architecture
//!
//! The privacy module implements a tiered approach:
//!
//! 1. **Code Anonymization**: Before any analysis, code can be anonymized to remove
//!    proprietary identifiers, internal API names, and potential PII.
//!
//! 2. **Local LLM Inference**: Quantized CodeLlama models can run entirely locally,
//!    ensuring code never leaves the user's machine.
//!
//! 3. **Optional Cloud Fallback**: If local confidence is low, users can opt-in to
//!    cloud-based analysis (disabled by default).

mod anonymizer;
mod local_llm;

pub use anonymizer::*;
pub use local_llm::*;

================================================================================
END: src\privacy\mod.rs
================================================================================

================================================================================
FILE: src\privacy\anonymizer.rs
================================================================================
//! Code anonymization pipeline for privacy-preserving analysis.
//!
//! This module normalizes identifiers in source code before sending to LLMs,
//! preventing leakage of proprietary names, internal APIs, or PII.
//!
//! The anonymization is reversible, allowing findings to be mapped back to
//! original code locations.

use crate::models::Language;
use regex::Regex;
use std::collections::HashMap;
use tree_sitter::Parser;

/// Configuration for code anonymization.
#[derive(Debug, Clone)]
pub struct AnonymizationConfig {
    /// Anonymize function/method names.
    pub anonymize_functions: bool,
    /// Anonymize variable names.
    pub anonymize_variables: bool,
    /// Anonymize class/struct/type names.
    pub anonymize_types: bool,
    /// Anonymize string literals (replace with placeholder).
    pub anonymize_strings: bool,
    /// Anonymize numeric literals.
    pub anonymize_numbers: bool,
    /// Anonymize comments.
    pub anonymize_comments: bool,
    /// Preserve language keywords and builtins.
    pub preserve_keywords: bool,
    /// Preserve common library names (e.g., std, tokio, reqwest).
    pub preserve_common_libs: bool,
    /// Minimum identifier length to anonymize (shorter ones are kept).
    pub min_identifier_length: usize,
}

impl Default for AnonymizationConfig {
    fn default() -> Self {
        Self {
            anonymize_functions: true,
            anonymize_variables: true,
            anonymize_types: true,
            anonymize_strings: true,
            anonymize_numbers: false, // Keep numbers by default for security analysis
            anonymize_comments: true,
            preserve_keywords: true,
            preserve_common_libs: true,
            min_identifier_length: 2,
        }
    }
}

/// Result of anonymizing code.
#[derive(Debug, Clone)]
pub struct AnonymizedCode {
    /// The anonymized source code.
    pub code: String,
    /// Mapping from anonymized names to original names.
    pub mapping: IdentifierMapping,
    /// Original code (for reference).
    pub original: String,
    /// Language of the code.
    pub language: Language,
}

impl AnonymizedCode {
    /// Restore an identifier to its original form.
    pub fn restore_identifier(&self, anonymized: &str) -> Option<&str> {
        self.mapping.to_original(anonymized)
    }

    /// Restore line numbers (they're preserved during anonymization).
    pub fn restore_line(&self, line: usize) -> usize {
        line
    }

    /// Restore a finding message by replacing anonymized identifiers.
    pub fn restore_message(&self, message: &str) -> String {
        let mut result = message.to_string();
        for (anon, orig) in &self.mapping.reverse {
            result = result.replace(anon, orig);
        }
        result
    }
}

/// Bidirectional mapping between original and anonymized identifiers.
#[derive(Debug, Clone, Default)]
pub struct IdentifierMapping {
    /// Original -> Anonymized
    forward: HashMap<String, String>,
    /// Anonymized -> Original
    reverse: HashMap<String, String>,
    /// Counters for generating unique names
    counters: IdentifierCounters,
}

#[derive(Debug, Clone, Default)]
struct IdentifierCounters {
    function: usize,
    variable: usize,
    type_name: usize,
    string: usize,
}

impl IdentifierMapping {
    /// Create a new empty mapping.
    pub fn new() -> Self {
        Self::default()
    }

    /// Get or create an anonymized function name.
    pub fn anonymize_function(&mut self, original: &str) -> String {
        if let Some(anon) = self.forward.get(original) {
            return anon.clone();
        }

        self.counters.function += 1;
        let anon = format!("func_{}", self.counters.function);
        self.forward.insert(original.to_string(), anon.clone());
        self.reverse.insert(anon.clone(), original.to_string());
        anon
    }

    /// Get or create an anonymized variable name.
    pub fn anonymize_variable(&mut self, original: &str) -> String {
        if let Some(anon) = self.forward.get(original) {
            return anon.clone();
        }

        self.counters.variable += 1;
        let anon = format!("var_{}", self.counters.variable);
        self.forward.insert(original.to_string(), anon.clone());
        self.reverse.insert(anon.clone(), original.to_string());
        anon
    }

    /// Get or create an anonymized type name.
    pub fn anonymize_type(&mut self, original: &str) -> String {
        if let Some(anon) = self.forward.get(original) {
            return anon.clone();
        }

        self.counters.type_name += 1;
        let anon = format!("Type_{}", self.counters.type_name);
        self.forward.insert(original.to_string(), anon.clone());
        self.reverse.insert(anon.clone(), original.to_string());
        anon
    }

    /// Get or create an anonymized string placeholder.
    pub fn anonymize_string(&mut self, original: &str) -> String {
        if let Some(anon) = self.forward.get(original) {
            return anon.clone();
        }

        self.counters.string += 1;
        let anon = format!("\"STRING_{}\"", self.counters.string);
        self.forward.insert(original.to_string(), anon.clone());
        self.reverse.insert(anon.clone(), original.to_string());
        anon
    }

    /// Look up the original identifier from an anonymized one.
    pub fn to_original(&self, anonymized: &str) -> Option<&str> {
        self.reverse.get(anonymized).map(|s| s.as_str())
    }

    /// Look up the anonymized identifier from an original one.
    pub fn to_anonymized(&self, original: &str) -> Option<&str> {
        self.forward.get(original).map(|s| s.as_str())
    }

    /// Get all mappings.
    pub fn mappings(&self) -> impl Iterator<Item = (&str, &str)> {
        self.forward.iter().map(|(k, v)| (k.as_str(), v.as_str()))
    }
}

/// Code anonymizer using tree-sitter for accurate parsing.
pub struct CodeAnonymizer {
    /// Configuration.
    config: AnonymizationConfig,
    /// Language keywords to preserve.
    keywords: HashMap<Language, Vec<&'static str>>,
    /// Common library names to preserve.
    common_libs: Vec<&'static str>,
}

impl CodeAnonymizer {
    /// Create a new code anonymizer with default configuration.
    pub fn new() -> Self {
        Self::with_config(AnonymizationConfig::default())
    }

    /// Create a new code anonymizer with custom configuration.
    pub fn with_config(config: AnonymizationConfig) -> Self {
        let mut keywords = HashMap::new();

        // Rust keywords
        keywords.insert(
            Language::Rust,
            vec![
                "as", "async", "await", "break", "const", "continue", "crate", "dyn",
                "else", "enum", "extern", "false", "fn", "for", "if", "impl", "in",
                "let", "loop", "match", "mod", "move", "mut", "pub", "ref", "return",
                "self", "Self", "static", "struct", "super", "trait", "true", "type",
                "unsafe", "use", "where", "while", "async", "await", "try",
                // Built-in types
                "bool", "char", "str", "u8", "u16", "u32", "u64", "u128", "usize",
                "i8", "i16", "i32", "i64", "i128", "isize", "f32", "f64",
                "String", "Vec", "Option", "Result", "Box", "Rc", "Arc", "Cell",
                "RefCell", "HashMap", "HashSet", "BTreeMap", "BTreeSet",
                // Common methods
                "new", "default", "clone", "into", "from", "unwrap", "expect",
                "ok", "err", "some", "none", "map", "and_then", "or_else",
            ],
        );

        // Python keywords
        keywords.insert(
            Language::Python,
            vec![
                "False", "None", "True", "and", "as", "assert", "async", "await",
                "break", "class", "continue", "def", "del", "elif", "else", "except",
                "finally", "for", "from", "global", "if", "import", "in", "is",
                "lambda", "nonlocal", "not", "or", "pass", "raise", "return", "try",
                "while", "with", "yield",
                // Built-in functions
                "print", "len", "range", "str", "int", "float", "list", "dict",
                "set", "tuple", "type", "isinstance", "hasattr", "getattr", "setattr",
                "open", "read", "write", "close",
            ],
        );

        // JavaScript keywords
        keywords.insert(
            Language::JavaScript,
            vec![
                "break", "case", "catch", "class", "const", "continue", "debugger",
                "default", "delete", "do", "else", "export", "extends", "false",
                "finally", "for", "function", "if", "import", "in", "instanceof",
                "let", "new", "null", "return", "static", "super", "switch", "this",
                "throw", "true", "try", "typeof", "undefined", "var", "void", "while",
                "with", "yield", "async", "await",
                // Built-in objects
                "Array", "Object", "String", "Number", "Boolean", "Date", "Math",
                "JSON", "Promise", "Map", "Set", "WeakMap", "WeakSet", "console",
            ],
        );

        // Go keywords
        keywords.insert(
            Language::Go,
            vec![
                "break", "case", "chan", "const", "continue", "default", "defer",
                "else", "fallthrough", "for", "func", "go", "goto", "if", "import",
                "interface", "map", "package", "range", "return", "select", "struct",
                "switch", "type", "var",
                // Built-in types
                "bool", "byte", "complex64", "complex128", "error", "float32",
                "float64", "int", "int8", "int16", "int32", "int64", "rune",
                "string", "uint", "uint8", "uint16", "uint32", "uint64", "uintptr",
                // Built-in functions
                "append", "cap", "close", "complex", "copy", "delete", "imag",
                "len", "make", "new", "panic", "print", "println", "real", "recover",
            ],
        );

        let common_libs = vec![
            // Rust crates
            "std", "tokio", "async_std", "reqwest", "serde", "serde_json",
            "anyhow", "thiserror", "tracing", "log", "clap", "chrono",
            "regex", "lazy_static", "once_cell", "parking_lot", "crossbeam",
            // Python packages
            "os", "sys", "json", "re", "datetime", "collections", "itertools",
            "requests", "numpy", "pandas", "flask", "django",
            // JavaScript/Node packages
            "fs", "path", "http", "https", "crypto", "express", "react",
            "axios", "lodash", "moment",
            // Go packages
            "fmt", "os", "io", "net", "http", "json", "context", "sync",
        ];

        Self {
            config,
            keywords,
            common_libs,
        }
    }

    /// Anonymize source code.
    pub fn anonymize(&self, code: &str, language: Language) -> AnonymizedCode {
        let mut mapping = IdentifierMapping::new();
        let anonymized = self.anonymize_with_mapping(code, language, &mut mapping);

        AnonymizedCode {
            code: anonymized,
            mapping,
            original: code.to_string(),
            language,
        }
    }

    /// Anonymize with an existing mapping (for multi-file consistency).
    pub fn anonymize_with_mapping(
        &self,
        code: &str,
        language: Language,
        mapping: &mut IdentifierMapping,
    ) -> String {
        // Try tree-sitter first for accurate parsing
        if let Some(result) = self.anonymize_with_tree_sitter(code, language, mapping) {
            return result;
        }

        // Fall back to regex-based anonymization
        self.anonymize_with_regex(code, language, mapping)
    }

    /// Anonymize using tree-sitter for accurate parsing.
    fn anonymize_with_tree_sitter(
        &self,
        code: &str,
        language: Language,
        mapping: &mut IdentifierMapping,
    ) -> Option<String> {
        let mut parser = Parser::new();

        let ts_language = match language {
            Language::Rust => tree_sitter_rust::LANGUAGE.into(),
            Language::Python => tree_sitter_python::LANGUAGE.into(),
            Language::JavaScript => tree_sitter_javascript::LANGUAGE.into(),
            Language::Go => tree_sitter_go::LANGUAGE.into(),
            _ => return None,
        };

        parser.set_language(&ts_language).ok()?;
        let tree = parser.parse(code, None)?;

        // Collect all replacements
        let mut replacements: Vec<(usize, usize, String)> = Vec::new();

        // Walk the tree and collect identifiers
        let mut cursor = tree.walk();
        self.collect_replacements(
            code,
            language,
            &mut cursor,
            mapping,
            &mut replacements,
        );

        // Sort by position (reverse order for safe replacement)
        replacements.sort_by(|a, b| b.0.cmp(&a.0));

        // Apply replacements
        let mut result = code.to_string();
        for (start, end, replacement) in replacements {
            if start < result.len() && end <= result.len() {
                result.replace_range(start..end, &replacement);
            }
        }

        Some(result)
    }

    fn collect_replacements(
        &self,
        code: &str,
        language: Language,
        cursor: &mut tree_sitter::TreeCursor,
        mapping: &mut IdentifierMapping,
        replacements: &mut Vec<(usize, usize, String)>,
    ) {
        loop {
            let node = cursor.node();
            let node_kind = node.kind();
            let start = node.start_byte();
            let end = node.end_byte();

            if start < code.len() && end <= code.len() {
                let text = &code[start..end];

                // Check if this is an identifier we should anonymize
                let replacement = self.get_replacement(text, node_kind, language, mapping);
                if let Some(rep) = replacement {
                    replacements.push((start, end, rep));
                }
            }

            // Recurse into children
            if cursor.goto_first_child() {
                self.collect_replacements(code, language, cursor, mapping, replacements);
                cursor.goto_parent();
            }

            // Move to next sibling
            if !cursor.goto_next_sibling() {
                break;
            }
        }
    }

    fn get_replacement(
        &self,
        text: &str,
        node_kind: &str,
        language: Language,
        mapping: &mut IdentifierMapping,
    ) -> Option<String> {
        // Skip if too short
        if text.len() < self.config.min_identifier_length {
            return None;
        }

        // Skip keywords
        if self.config.preserve_keywords {
            if let Some(kw_list) = self.keywords.get(&language) {
                if kw_list.contains(&text) {
                    return None;
                }
            }
        }

        // Skip common library names
        if self.config.preserve_common_libs && self.common_libs.contains(&text) {
            return None;
        }

        // Determine identifier type based on node kind
        match node_kind {
            // Function definitions
            "function_item" | "function_definition" | "function_declaration"
            | "method_definition" | "method_declaration" => {
                if self.config.anonymize_functions {
                    return Some(mapping.anonymize_function(text));
                }
            }

            // Identifiers (need context to determine type)
            "identifier" | "name" => {
                // Check if it looks like a type (starts with uppercase)
                if text.chars().next().map(|c| c.is_uppercase()).unwrap_or(false) {
                    if self.config.anonymize_types {
                        return Some(mapping.anonymize_type(text));
                    }
                } else if self.config.anonymize_variables {
                    return Some(mapping.anonymize_variable(text));
                }
            }

            // Type names
            "type_identifier" | "class_name" | "struct_name" | "enum_name"
            | "trait_name" | "interface_name" => {
                if self.config.anonymize_types {
                    return Some(mapping.anonymize_type(text));
                }
            }

            // String literals
            "string" | "string_literal" | "raw_string_literal" | "string_content" => {
                if self.config.anonymize_strings {
                    return Some(mapping.anonymize_string(text));
                }
            }

            // Comments
            "comment" | "line_comment" | "block_comment" | "doc_comment" => {
                if self.config.anonymize_comments {
                    return Some("/* COMMENT */".to_string());
                }
            }

            _ => {}
        }

        None
    }

    /// Fallback regex-based anonymization.
    fn anonymize_with_regex(
        &self,
        code: &str,
        language: Language,
        mapping: &mut IdentifierMapping,
    ) -> String {
        let mut result = code.to_string();

        // Anonymize strings first (to avoid matching identifiers inside strings)
        if self.config.anonymize_strings {
            let string_re = Regex::new(r#""[^"\\]*(?:\\.[^"\\]*)*""#).unwrap();
            result = string_re
                .replace_all(&result, |_: &regex::Captures| {
                    mapping.counters.string += 1;
                    format!("\"STRING_{}\"", mapping.counters.string)
                })
                .to_string();
        }

        // Anonymize comments
        if self.config.anonymize_comments {
            // Line comments
            let line_comment = match language {
                Language::Python => Regex::new(r"#.*$").unwrap(),
                _ => Regex::new(r"//.*$").unwrap(),
            };
            result = line_comment.replace_all(&result, "// COMMENT").to_string();

            // Block comments
            let block_comment = match language {
                Language::Python => Regex::new(r#"'''[\s\S]*?'''"#).unwrap(),
                _ => Regex::new(r"/\*[\s\S]*?\*/").unwrap(),
            };
            result = block_comment.replace_all(&result, "/* COMMENT */").to_string();
        }

        // Anonymize identifiers (simple approach)
        if self.config.anonymize_functions || self.config.anonymize_variables {
            let ident_re = Regex::new(r"\b([a-zA-Z_][a-zA-Z0-9_]*)\b").unwrap();
            let keywords = self.keywords.get(&language).cloned().unwrap_or_default();

            let result_clone = result.clone();
            result = ident_re
                .replace_all(&result_clone, |caps: &regex::Captures| {
                    let ident = &caps[1];

                    // Skip if too short
                    if ident.len() < self.config.min_identifier_length {
                        return ident.to_string();
                    }

                    // Skip keywords
                    if self.config.preserve_keywords && keywords.contains(&ident) {
                        return ident.to_string();
                    }

                    // Skip common libs
                    if self.config.preserve_common_libs && self.common_libs.contains(&ident) {
                        return ident.to_string();
                    }

                    // Anonymize based on case
                    if ident.chars().next().map(|c| c.is_uppercase()).unwrap_or(false) {
                        if self.config.anonymize_types {
                            mapping.anonymize_type(ident)
                        } else {
                            ident.to_string()
                        }
                    } else {
                        mapping.anonymize_variable(ident)
                    }
                })
                .to_string();
        }

        result
    }
}

impl Default for CodeAnonymizer {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_basic_anonymization() {
        let anonymizer = CodeAnonymizer::new();
        let code = r#"
fn calculate_price(item_count: u32, unit_price: f64) -> f64 {
    let total = item_count as f64 * unit_price;
    total
}
"#;

        let result = anonymizer.anonymize(code, Language::Rust);

        // Keywords should be preserved
        assert!(result.code.contains("fn"));
        assert!(result.code.contains("let"));
        assert!(result.code.contains("u32"));
        assert!(result.code.contains("f64"));

        // Original identifiers should be replaced
        assert!(!result.code.contains("calculate_price"));
        assert!(!result.code.contains("item_count"));
        assert!(!result.code.contains("unit_price"));
    }

    #[test]
    fn test_mapping_restoration() {
        let anonymizer = CodeAnonymizer::new();
        let code = "let my_secret_variable = 42;";

        let result = anonymizer.anonymize(code, Language::Rust);

        // Should be able to restore original name
        let anon_name = result.mapping.to_anonymized("my_secret_variable");
        assert!(anon_name.is_some());

        let restored = result.mapping.to_original(anon_name.unwrap());
        assert_eq!(restored, Some("my_secret_variable"));
    }

    #[test]
    fn test_string_anonymization() {
        let anonymizer = CodeAnonymizer::new();
        let code = r#"let secret = "my-api-key-12345";"#;

        let result = anonymizer.anonymize(code, Language::Rust);

        // Original string should be replaced
        assert!(!result.code.contains("my-api-key-12345"));
        assert!(result.code.contains("STRING_"));
    }

    #[test]
    fn test_comment_anonymization() {
        let anonymizer = CodeAnonymizer::new();
        let code = r#"
// This is a secret comment with internal info
fn main() {}
"#;

        let result = anonymizer.anonymize(code, Language::Rust);

        // Original comment content should be removed
        assert!(!result.code.contains("secret comment"));
        assert!(result.code.contains("COMMENT"));
    }

    #[test]
    fn test_preserve_common_libs() {
        let config = AnonymizationConfig {
            preserve_common_libs: true,
            ..Default::default()
        };
        let anonymizer = CodeAnonymizer::with_config(config);

        let code = "use std::collections::HashMap;";
        let result = anonymizer.anonymize(code, Language::Rust);

        // std and HashMap should be preserved
        assert!(result.code.contains("std"));
        assert!(result.code.contains("HashMap"));
    }

    #[test]
    fn test_restore_message() {
        let anonymizer = CodeAnonymizer::new();
        let code = "fn process_user_data(user_id: u32) {}";

        let result = anonymizer.anonymize(code, Language::Rust);

        // Simulate a finding message with anonymized names
        let anon_name = result.mapping.to_anonymized("process_user_data").unwrap();
        let finding_msg = format!("Vulnerability in function {}", anon_name);

        // Restore should bring back original name
        let restored = result.restore_message(&finding_msg);
        assert!(restored.contains("process_user_data"));
    }

    #[test]
    fn test_consistent_mapping() {
        let anonymizer = CodeAnonymizer::new();
        let code = r#"
fn my_func() {
    let x = my_func;
    my_func();
}
"#;

        let result = anonymizer.anonymize(code, Language::Rust);

        // Same identifier should get same anonymized name
        let anon = result.mapping.to_anonymized("my_func").unwrap();
        let occurrences = result.code.matches(anon).count();
        // Should appear multiple times (definition + references)
        assert!(occurrences >= 1);
    }

    #[test]
    fn test_python_anonymization() {
        let anonymizer = CodeAnonymizer::new();
        let code = r#"
def calculate_total(items):
    total = 0
    for item in items:
        total += item.price
    return total
"#;

        let result = anonymizer.anonymize(code, Language::Python);

        // Python keywords should be preserved
        assert!(result.code.contains("def"));
        assert!(result.code.contains("for"));
        assert!(result.code.contains("return"));

        // Custom identifiers should be replaced
        assert!(!result.code.contains("calculate_total"));
    }

    #[test]
    fn test_type_anonymization() {
        let anonymizer = CodeAnonymizer::new();
        let code = "struct CustomerData { name: String }";

        let result = anonymizer.anonymize(code, Language::Rust);

        // Type name should be anonymized
        assert!(!result.code.contains("CustomerData"));
        assert!(result.code.contains("Type_"));
    }
}

================================================================================
END: src\privacy\anonymizer.rs
================================================================================

================================================================================
FILE: src\privacy\local_llm.rs
================================================================================
//! Local-first LLM inference tier for privacy-preserving analysis.
//!
//! This module provides local inference capabilities using quantized models,
//! allowing security analysis without sending code to external APIs.
//!
//! # Architecture
//!
//! The local LLM tier operates in a tiered approach:
//! 1. **Local-first**: Try local inference with quantized CodeLlama
//! 2. **Fallback**: If local fails or confidence is low, can optionally use cloud API
//!
//! # Supported Models
//!
//! - CodeLlama 7B (Q4_K_M quantization) - ~4GB RAM
//! - CodeLlama 13B (Q4_K_M quantization) - ~8GB RAM
//! - CodeLlama 34B (Q4_K_M quantization) - ~20GB RAM (recommended)

use crate::error::{AuditorError, Result};
use crate::models::{Finding, FindingCategory, Language, Location, Severity, Confidence};
use crate::privacy::anonymizer::{AnonymizationConfig, CodeAnonymizer};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;
use std::time::Duration;
use tracing::{debug, info, warn};

/// Configuration for local LLM inference.
#[derive(Debug, Clone)]
pub struct LocalLlmConfig {
    /// Path to the quantized model file (GGUF format).
    pub model_path: Option<PathBuf>,
    /// Model context window size.
    pub context_size: usize,
    /// Number of tokens to generate.
    pub max_tokens: usize,
    /// Temperature for sampling (0.0 = deterministic).
    pub temperature: f32,
    /// Number of threads for inference.
    pub num_threads: usize,
    /// GPU layers to offload (0 = CPU only).
    pub gpu_layers: usize,
    /// Local llama.cpp server endpoint (if using server mode).
    pub server_endpoint: Option<String>,
    /// Timeout for inference requests.
    pub timeout: Duration,
    /// Whether to anonymize code before sending to local model.
    pub anonymize_code: bool,
    /// Minimum confidence threshold for accepting local results.
    pub min_confidence: f32,
    /// Whether to allow fallback to cloud API.
    pub allow_cloud_fallback: bool,
}

impl Default for LocalLlmConfig {
    fn default() -> Self {
        Self {
            model_path: None,
            context_size: 4096,
            max_tokens: 1024,
            temperature: 0.1, // Low temperature for consistent analysis
            num_threads: num_cpus::get(),
            gpu_layers: 0,
            server_endpoint: Some("http://127.0.0.1:8080".to_string()),
            timeout: Duration::from_secs(120),
            anonymize_code: true,
            min_confidence: 0.7,
            allow_cloud_fallback: false, // Privacy-first by default
        }
    }
}

/// Model size/quality tiers.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ModelTier {
    /// 7B parameter model - fastest, lowest quality
    Small,
    /// 13B parameter model - balanced
    Medium,
    /// 34B parameter model - highest quality
    Large,
}

impl ModelTier {
    /// Estimated RAM requirement for Q4_K_M quantization.
    pub fn ram_requirement_gb(&self) -> f32 {
        match self {
            ModelTier::Small => 4.0,
            ModelTier::Medium => 8.0,
            ModelTier::Large => 20.0,
        }
    }

    /// Recommended context size.
    pub fn recommended_context_size(&self) -> usize {
        match self {
            ModelTier::Small => 4096,
            ModelTier::Medium => 8192,
            ModelTier::Large => 16384,
        }
    }

    /// Model identifier for llama.cpp.
    pub fn model_name(&self) -> &'static str {
        match self {
            ModelTier::Small => "codellama-7b-instruct.Q4_K_M.gguf",
            ModelTier::Medium => "codellama-13b-instruct.Q4_K_M.gguf",
            ModelTier::Large => "codellama-34b-instruct.Q4_K_M.gguf",
        }
    }
}

/// Result from local LLM analysis.
#[derive(Debug, Clone)]
pub struct LocalAnalysisResult {
    /// Detected vulnerabilities.
    pub findings: Vec<Finding>,
    /// Raw model response.
    pub raw_response: String,
    /// Confidence score (0.0 - 1.0).
    pub confidence: f32,
    /// Whether the result was from local inference.
    pub is_local: bool,
    /// Inference time in milliseconds.
    pub inference_time_ms: u64,
    /// Tokens processed.
    pub tokens_processed: usize,
}

/// Request format for llama.cpp server.
#[derive(Debug, Serialize)]
struct LlamaServerRequest {
    prompt: String,
    n_predict: usize,
    temperature: f32,
    stop: Vec<String>,
    stream: bool,
}

/// Response format from llama.cpp server.
#[derive(Debug, Deserialize)]
struct LlamaServerResponse {
    content: String,
    #[serde(default)]
    tokens_evaluated: usize,
    #[serde(default)]
    tokens_predicted: usize,
    #[serde(default)]
    timings: Option<LlamaTimings>,
}

#[derive(Debug, Deserialize)]
struct LlamaTimings {
    #[serde(default)]
    predicted_ms: f64,
    #[serde(default)]
    prompt_ms: f64,
}

/// Local LLM inference engine.
pub struct LocalLlmEngine {
    /// Configuration.
    config: LocalLlmConfig,
    /// HTTP client for server mode.
    client: Client,
    /// Code anonymizer.
    anonymizer: CodeAnonymizer,
    /// Security analysis prompt template.
    prompt_template: String,
}

impl LocalLlmEngine {
    /// Create a new local LLM engine with default configuration.
    pub fn new() -> Self {
        Self::with_config(LocalLlmConfig::default())
    }

    /// Create with custom configuration.
    pub fn with_config(config: LocalLlmConfig) -> Self {
        let client = Client::builder()
            .timeout(config.timeout)
            .build()
            .expect("Failed to create HTTP client");

        let anonymizer_config = AnonymizationConfig {
            anonymize_strings: config.anonymize_code,
            anonymize_comments: config.anonymize_code,
            ..Default::default()
        };
        let anonymizer = CodeAnonymizer::with_config(anonymizer_config);

        let prompt_template = Self::build_prompt_template();

        Self {
            config,
            client,
            anonymizer,
            prompt_template,
        }
    }

    /// Build the security analysis prompt template.
    fn build_prompt_template() -> String {
        r#"[INST] You are a security vulnerability analyzer. Analyze the following code for security issues.

For each vulnerability found, respond with a JSON object in this exact format:
```json
{
  "vulnerabilities": [
    {
      "type": "vulnerability type (e.g., SQL Injection, XSS, Buffer Overflow)",
      "severity": "critical|high|medium|low",
      "line": line_number,
      "description": "brief description of the issue",
      "fix": "suggested remediation"
    }
  ],
  "confidence": 0.0-1.0
}
```

If no vulnerabilities are found, respond with:
```json
{"vulnerabilities": [], "confidence": 0.9}
```

Language: {language}
Code:
```{lang_ext}
{code}
```

Analyze for: SQL injection, command injection, XSS, buffer overflows, use-after-free,
race conditions, path traversal, insecure deserialization, and cryptographic issues.
[/INST]"#
            .to_string()
    }

    /// Check if local inference is available.
    pub async fn is_available(&self) -> bool {
        if let Some(ref endpoint) = self.config.server_endpoint {
            // Check if llama.cpp server is running
            let health_url = format!("{}/health", endpoint);
            match self.client.get(&health_url).send().await {
                Ok(resp) => resp.status().is_success(),
                Err(_) => false,
            }
        } else if let Some(ref model_path) = self.config.model_path {
            // Check if model file exists
            model_path.exists()
        } else {
            false
        }
    }

    /// Analyze code for vulnerabilities using local LLM.
    pub async fn analyze(
        &self,
        code: &str,
        language: Language,
    ) -> Result<LocalAnalysisResult> {
        let start = std::time::Instant::now();

        // Optionally anonymize code
        let (analysis_code, anonymized) = if self.config.anonymize_code {
            let anon = self.anonymizer.anonymize(code, language);
            (anon.code.clone(), Some(anon))
        } else {
            (code.to_string(), None)
        };

        // Build prompt
        let prompt = self.build_prompt(&analysis_code, language);

        // Run inference
        let response = self.run_inference(&prompt).await?;

        let inference_time = start.elapsed().as_millis() as u64;

        // Parse response
        let mut result = self.parse_response(&response.content, language)?;
        result.inference_time_ms = inference_time;
        result.tokens_processed = response.tokens_evaluated + response.tokens_predicted;
        result.is_local = true;
        result.raw_response = response.content;

        // De-anonymize findings if needed
        if let Some(anon) = anonymized {
            for finding in &mut result.findings {
                finding.description = anon.restore_message(&finding.description);
                finding.title = anon.restore_message(&finding.title);
            }
        }

        // Check confidence threshold
        if result.confidence < self.config.min_confidence && self.config.allow_cloud_fallback {
            warn!(
                "Local inference confidence ({:.2}) below threshold ({:.2})",
                result.confidence, self.config.min_confidence
            );
            // Caller can decide whether to use cloud fallback
        }

        Ok(result)
    }

    /// Build the analysis prompt.
    fn build_prompt(&self, code: &str, language: Language) -> String {
        let lang_ext = match language {
            Language::Rust => "rust",
            Language::Python => "python",
            Language::JavaScript => "javascript",
            Language::Go => "go",
            Language::Java => "java",
            Language::C => "c",
            Language::Cpp => "cpp",
            _ => "text",
        };

        self.prompt_template
            .replace("{language}", &format!("{:?}", language))
            .replace("{lang_ext}", lang_ext)
            .replace("{code}", code)
    }

    /// Run inference via llama.cpp server.
    async fn run_inference(&self, prompt: &str) -> Result<LlamaServerResponse> {
        let endpoint = self
            .config
            .server_endpoint
            .as_ref()
            .ok_or_else(|| AuditorError::Config("No server endpoint configured".into()))?;

        let request = LlamaServerRequest {
            prompt: prompt.to_string(),
            n_predict: self.config.max_tokens,
            temperature: self.config.temperature,
            stop: vec!["[/INST]".to_string(), "```\n\n".to_string()],
            stream: false,
        };

        let url = format!("{}/completion", endpoint);
        debug!("Sending request to local LLM server: {}", url);

        let response = self
            .client
            .post(&url)
            .json(&request)
            .send()
            .await
            .map_err(|e| AuditorError::Analysis(format!("Local LLM request failed: {}", e)))?;

        if !response.status().is_success() {
            let status = response.status();
            let body = response.text().await.unwrap_or_default();
            return Err(AuditorError::Analysis(format!(
                "Local LLM server error: {} - {}",
                status, body
            )));
        }

        let result: LlamaServerResponse = response
            .json()
            .await
            .map_err(|e| AuditorError::Parse(format!("Failed to parse LLM response: {}", e)))?;

        Ok(result)
    }

    /// Parse the LLM response into findings.
    fn parse_response(
        &self,
        response: &str,
        language: Language,
    ) -> Result<LocalAnalysisResult> {
        // Extract JSON from response
        let json_str = self.extract_json(response);

        let parsed: LlmAnalysisResponse = serde_json::from_str(&json_str).unwrap_or_else(|e| {
            debug!("Failed to parse LLM JSON response: {}", e);
            LlmAnalysisResponse {
                vulnerabilities: vec![],
                confidence: 0.5,
            }
        });

        let findings: Vec<Finding> = parsed
            .vulnerabilities
            .into_iter()
            .map(|v| self.vuln_to_finding(v, language))
            .collect();

        Ok(LocalAnalysisResult {
            findings,
            raw_response: String::new(),
            confidence: parsed.confidence,
            is_local: true,
            inference_time_ms: 0,
            tokens_processed: 0,
        })
    }

    /// Extract JSON from potentially wrapped response.
    fn extract_json(&self, response: &str) -> String {
        // Try to find JSON in code blocks
        if let Some(start) = response.find("```json") {
            let after_start = &response[start + 7..];
            if let Some(end) = after_start.find("```") {
                return after_start[..end].trim().to_string();
            }
        }

        // Try to find raw JSON object
        if let Some(start) = response.find('{') {
            if let Some(end) = response.rfind('}') {
                return response[start..=end].to_string();
            }
        }

        // Return empty response
        r#"{"vulnerabilities": [], "confidence": 0.5}"#.to_string()
    }

    /// Convert LLM vulnerability to Finding.
    fn vuln_to_finding(&self, vuln: LlmVulnerability, language: Language) -> Finding {
        let severity = match vuln.severity.to_lowercase().as_str() {
            "critical" => Severity::Critical,
            "high" => Severity::High,
            "medium" => Severity::Medium,
            "low" => Severity::Low,
            _ => Severity::Medium,
        };

        let location = Location::new(
            PathBuf::new(), // Caller should set the actual path
            vuln.line.unwrap_or(1),
            1,
        )
        .with_language(language);

        Finding {
            id: uuid::Uuid::new_v4().to_string(),
            category: FindingCategory::Ai,
            severity,
            title: vuln.vuln_type,
            description: vuln.description,
            location,
            snippet: None,
            vulnerability: None,
            confidence: Confidence::Medium,
            rule_id: "ai/local-llm".to_string(),
            remediation: vuln.fix,
            metadata: std::collections::HashMap::new(),
            discovered_at: chrono::Utc::now(),
        }
    }

    /// Analyze multiple code snippets in batch.
    pub async fn analyze_batch(
        &self,
        snippets: Vec<(String, Language)>,
    ) -> Vec<Result<LocalAnalysisResult>> {
        let mut results = Vec::with_capacity(snippets.len());

        for (code, language) in snippets {
            results.push(self.analyze(&code, language).await);
        }

        results
    }

    /// Get model information.
    pub async fn get_model_info(&self) -> Result<ModelInfo> {
        let endpoint = self
            .config
            .server_endpoint
            .as_ref()
            .ok_or_else(|| AuditorError::Config("No server endpoint configured".into()))?;

        let url = format!("{}/props", endpoint);

        let response = self.client.get(&url).send().await.map_err(|e| {
            AuditorError::Analysis(format!("Failed to get model info: {}", e))
        })?;

        if !response.status().is_success() {
            return Err(AuditorError::Analysis(
                "Failed to get model info".into(),
            ));
        }

        let props: serde_json::Value = response.json().await.map_err(|e| {
            AuditorError::Parse(format!("Failed to parse model info: {}", e))
        })?;

        Ok(ModelInfo {
            model_name: props
                .get("model")
                .and_then(|v| v.as_str())
                .unwrap_or("unknown")
                .to_string(),
            context_size: props
                .get("n_ctx")
                .and_then(|v| v.as_u64())
                .unwrap_or(0) as usize,
            vocab_size: props
                .get("n_vocab")
                .and_then(|v| v.as_u64())
                .unwrap_or(0) as usize,
        })
    }
}

impl Default for LocalLlmEngine {
    fn default() -> Self {
        Self::new()
    }
}

/// LLM response structure.
#[derive(Debug, Deserialize)]
struct LlmAnalysisResponse {
    vulnerabilities: Vec<LlmVulnerability>,
    confidence: f32,
}

/// Vulnerability from LLM response.
#[derive(Debug, Deserialize)]
struct LlmVulnerability {
    #[serde(rename = "type")]
    vuln_type: String,
    severity: String,
    line: Option<usize>,
    description: String,
    #[serde(default)]
    fix: Option<String>,
}

/// Information about the loaded model.
#[derive(Debug, Clone)]
pub struct ModelInfo {
    pub model_name: String,
    pub context_size: usize,
    pub vocab_size: usize,
}

/// Tiered inference strategy.
pub struct TieredInference {
    /// Local LLM engine.
    local: LocalLlmEngine,
    /// Whether to use local-only mode.
    local_only: bool,
    /// Cloud API endpoint (if fallback allowed).
    cloud_endpoint: Option<String>,
    /// Cloud API key.
    cloud_api_key: Option<String>,
}

impl TieredInference {
    /// Create a new tiered inference system (local-only by default).
    pub fn new() -> Self {
        Self {
            local: LocalLlmEngine::new(),
            local_only: true,
            cloud_endpoint: None,
            cloud_api_key: None,
        }
    }

    /// Create with cloud fallback enabled.
    pub fn with_cloud_fallback(cloud_endpoint: String, api_key: String) -> Self {
        let mut config = LocalLlmConfig::default();
        config.allow_cloud_fallback = true;

        Self {
            local: LocalLlmEngine::with_config(config),
            local_only: false,
            cloud_endpoint: Some(cloud_endpoint),
            cloud_api_key: Some(api_key),
        }
    }

    /// Analyze code using tiered approach.
    pub async fn analyze(
        &self,
        code: &str,
        language: Language,
    ) -> Result<LocalAnalysisResult> {
        // Try local first
        if self.local.is_available().await {
            match self.local.analyze(code, language).await {
                Ok(result) if result.confidence >= self.local.config.min_confidence => {
                    info!(
                        "Local inference succeeded with confidence {:.2}",
                        result.confidence
                    );
                    return Ok(result);
                }
                Ok(result) => {
                    warn!(
                        "Local inference confidence ({:.2}) below threshold",
                        result.confidence
                    );
                    if self.local_only {
                        return Ok(result); // Return low-confidence result if local-only
                    }
                    // Fall through to cloud
                }
                Err(e) => {
                    warn!("Local inference failed: {}", e);
                    if self.local_only {
                        return Err(e);
                    }
                    // Fall through to cloud
                }
            }
        } else if self.local_only {
            return Err(AuditorError::Config(
                "Local LLM not available and cloud fallback disabled".into(),
            ));
        }

        // Fallback to cloud if allowed
        if !self.local_only {
            self.analyze_with_cloud(code, language).await
        } else {
            Err(AuditorError::Config(
                "No inference backend available".into(),
            ))
        }
    }

    /// Analyze using cloud API (only used as fallback).
    async fn analyze_with_cloud(
        &self,
        _code: &str,
        _language: Language,
    ) -> Result<LocalAnalysisResult> {
        // This would implement cloud API fallback
        // For privacy reasons, this is disabled by default
        Err(AuditorError::Config(
            "Cloud fallback not implemented - use local inference".into(),
        ))
    }
}

impl Default for TieredInference {
    fn default() -> Self {
        Self::new()
    }
}

/// Builder for configuring local LLM.
pub struct LocalLlmConfigBuilder {
    config: LocalLlmConfig,
}

impl LocalLlmConfigBuilder {
    /// Start building configuration.
    pub fn new() -> Self {
        Self {
            config: LocalLlmConfig::default(),
        }
    }

    /// Set model path.
    pub fn model_path(mut self, path: PathBuf) -> Self {
        self.config.model_path = Some(path);
        self
    }

    /// Set server endpoint.
    pub fn server_endpoint(mut self, endpoint: &str) -> Self {
        self.config.server_endpoint = Some(endpoint.to_string());
        self
    }

    /// Set context size.
    pub fn context_size(mut self, size: usize) -> Self {
        self.config.context_size = size;
        self
    }

    /// Set max tokens.
    pub fn max_tokens(mut self, tokens: usize) -> Self {
        self.config.max_tokens = tokens;
        self
    }

    /// Set temperature.
    pub fn temperature(mut self, temp: f32) -> Self {
        self.config.temperature = temp;
        self
    }

    /// Set GPU layers.
    pub fn gpu_layers(mut self, layers: usize) -> Self {
        self.config.gpu_layers = layers;
        self
    }

    /// Enable/disable code anonymization.
    pub fn anonymize_code(mut self, enable: bool) -> Self {
        self.config.anonymize_code = enable;
        self
    }

    /// Set minimum confidence threshold.
    pub fn min_confidence(mut self, confidence: f32) -> Self {
        self.config.min_confidence = confidence;
        self
    }

    /// Allow cloud fallback.
    pub fn allow_cloud_fallback(mut self, allow: bool) -> Self {
        self.config.allow_cloud_fallback = allow;
        self
    }

    /// Build the configuration.
    pub fn build(self) -> LocalLlmConfig {
        self.config
    }
}

impl Default for LocalLlmConfigBuilder {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_config_builder() {
        let config = LocalLlmConfigBuilder::new()
            .server_endpoint("http://localhost:8080")
            .context_size(8192)
            .max_tokens(2048)
            .temperature(0.2)
            .anonymize_code(true)
            .min_confidence(0.8)
            .build();

        assert_eq!(config.server_endpoint, Some("http://localhost:8080".to_string()));
        assert_eq!(config.context_size, 8192);
        assert_eq!(config.max_tokens, 2048);
        assert!((config.temperature - 0.2).abs() < f32::EPSILON);
        assert!(config.anonymize_code);
        assert!((config.min_confidence - 0.8).abs() < f32::EPSILON);
    }

    #[test]
    fn test_model_tier_info() {
        assert_eq!(ModelTier::Small.ram_requirement_gb(), 4.0);
        assert_eq!(ModelTier::Medium.ram_requirement_gb(), 8.0);
        assert_eq!(ModelTier::Large.ram_requirement_gb(), 20.0);

        assert_eq!(ModelTier::Small.recommended_context_size(), 4096);
        assert_eq!(ModelTier::Large.recommended_context_size(), 16384);
    }

    #[test]
    fn test_default_config() {
        let config = LocalLlmConfig::default();

        assert!(config.anonymize_code);
        assert!(!config.allow_cloud_fallback);
        assert!((config.temperature - 0.1).abs() < f32::EPSILON);
        assert_eq!(config.context_size, 4096);
    }

    #[test]
    fn test_json_extraction() {
        let engine = LocalLlmEngine::new();

        // Test code block extraction
        let response = r#"Here is the analysis:
```json
{"vulnerabilities": [], "confidence": 0.9}
```
End of analysis."#;
        let json = engine.extract_json(response);
        assert!(json.contains("vulnerabilities"));
        assert!(json.contains("0.9"));

        // Test raw JSON extraction
        let response2 = r#"Analysis: {"vulnerabilities": [{"type": "XSS"}], "confidence": 0.8}"#;
        let json2 = engine.extract_json(response2);
        assert!(json2.contains("XSS"));
    }

    #[test]
    fn test_prompt_building() {
        let engine = LocalLlmEngine::new();
        let prompt = engine.build_prompt("fn main() {}", Language::Rust);

        assert!(prompt.contains("[INST]"));
        assert!(prompt.contains("fn main() {}"));
        assert!(prompt.contains("rust"));
        assert!(prompt.contains("SQL injection"));
    }

    #[test]
    fn test_parse_empty_response() {
        let engine = LocalLlmEngine::new();

        let result = engine.parse_response(
            r#"{"vulnerabilities": [], "confidence": 0.95}"#,
            Language::Rust,
        );

        assert!(result.is_ok());
        let analysis = result.unwrap();
        assert!(analysis.findings.is_empty());
        assert!((analysis.confidence - 0.95).abs() < 0.01);
    }

    #[test]
    fn test_parse_vulnerability_response() {
        let engine = LocalLlmEngine::new();

        let response = r#"{
            "vulnerabilities": [
                {
                    "type": "SQL Injection",
                    "severity": "high",
                    "line": 42,
                    "description": "User input directly concatenated into SQL query"
                }
            ],
            "confidence": 0.85
        }"#;

        let result = engine.parse_response(response, Language::Python);
        assert!(result.is_ok());

        let analysis = result.unwrap();
        assert_eq!(analysis.findings.len(), 1);
        assert_eq!(analysis.findings[0].severity, Severity::High);
        assert!((analysis.confidence - 0.85).abs() < 0.01);
    }

    #[test]
    fn test_tiered_inference_local_only() {
        let inference = TieredInference::new();
        assert!(inference.local_only);
        assert!(inference.cloud_endpoint.is_none());
    }

    #[test]
    fn test_severity_mapping() {
        let engine = LocalLlmEngine::new();

        let critical = LlmVulnerability {
            vuln_type: "Test".to_string(),
            severity: "critical".to_string(),
            line: None,
            description: "Test".to_string(),
            fix: None,
        };
        let finding = engine.vuln_to_finding(critical, Language::Rust);
        assert_eq!(finding.severity, Severity::Critical);

        let low = LlmVulnerability {
            vuln_type: "Test".to_string(),
            severity: "LOW".to_string(), // Test case insensitivity
            line: None,
            description: "Test".to_string(),
            fix: None,
        };
        let finding = engine.vuln_to_finding(low, Language::Rust);
        assert_eq!(finding.severity, Severity::Low);
    }
}

================================================================================
END: src\privacy\local_llm.rs
================================================================================

================================================================================
FILE: src\crosslang\mod.rs
================================================================================
//! Cross-language vulnerability pattern infrastructure.
//!
//! This module provides an Abstract Pattern Intermediate Representation (APIR)
//! that enables defining security vulnerability patterns once and applying them
//! across multiple programming languages.
//!
//! # Architecture
//!
//! ```text
//! +------------------+     +------------------+     +------------------+
//! | Pattern DSL      | --> | APIR (Abstract)  | --> | Language Mapping |
//! | (Human-readable) |     | (Canonical)      |     | (Concrete AST)   |
//! +------------------+     +------------------+     +------------------+
//!                                  |
//!                                  v
//!                          +------------------+
//!                          | Tree-sitter      |
//!                          | Pattern Matching |
//!                          +------------------+
//! ```
//!
//! # Pattern Categories
//!
//! - **Injection**: SQL, Command, XSS, LDAP, XPath
//! - **Cryptographic**: Weak algorithms, hardcoded keys, insecure RNG
//! - **Memory Safety**: Buffer overflows, use-after-free, null dereference
//! - **Authentication**: Hardcoded credentials, weak session management
//! - **Authorization**: IDOR, privilege escalation
//! - **Data Exposure**: PII leakage, verbose errors

mod apir;
mod lang_mapping;

pub use apir::*;
pub use lang_mapping::*;

================================================================================
END: src\crosslang\mod.rs
================================================================================

================================================================================
FILE: src\crosslang\apir.rs
================================================================================
//! Abstract Pattern Intermediate Representation (APIR).
//!
//! APIR provides a language-agnostic way to define security vulnerability patterns.
//! Patterns are defined in an abstract form and then compiled to language-specific
//! tree-sitter queries.

use std::collections::HashMap;

/// A complete vulnerability pattern definition.
#[derive(Debug, Clone)]
pub struct VulnerabilityPattern {
    /// Unique identifier for the pattern.
    pub id: String,
    /// Human-readable name.
    pub name: String,
    /// Detailed description.
    pub description: String,
    /// CWE identifier(s).
    pub cwes: Vec<String>,
    /// Severity level.
    pub severity: PatternSeverity,
    /// Pattern category.
    pub category: PatternCategory,
    /// The abstract pattern definition.
    pub pattern: AbstractPattern,
    /// Languages this pattern applies to (empty = all supported).
    pub languages: Vec<String>,
    /// Remediation guidance.
    pub remediation: String,
    /// Example vulnerable code snippets.
    pub examples: Vec<PatternExample>,
    /// Tags for filtering.
    pub tags: Vec<String>,
}

impl VulnerabilityPattern {
    /// Create a new pattern builder.
    pub fn builder(id: impl Into<String>) -> VulnerabilityPatternBuilder {
        VulnerabilityPatternBuilder::new(id)
    }
}

/// Builder for vulnerability patterns.
pub struct VulnerabilityPatternBuilder {
    pattern: VulnerabilityPattern,
}

impl VulnerabilityPatternBuilder {
    pub fn new(id: impl Into<String>) -> Self {
        Self {
            pattern: VulnerabilityPattern {
                id: id.into(),
                name: String::new(),
                description: String::new(),
                cwes: vec![],
                severity: PatternSeverity::Medium,
                category: PatternCategory::Other,
                pattern: AbstractPattern::Empty,
                languages: vec![],
                remediation: String::new(),
                examples: vec![],
                tags: vec![],
            },
        }
    }

    pub fn name(mut self, name: impl Into<String>) -> Self {
        self.pattern.name = name.into();
        self
    }

    pub fn description(mut self, desc: impl Into<String>) -> Self {
        self.pattern.description = desc.into();
        self
    }

    pub fn cwe(mut self, cwe: impl Into<String>) -> Self {
        self.pattern.cwes.push(cwe.into());
        self
    }

    pub fn severity(mut self, severity: PatternSeverity) -> Self {
        self.pattern.severity = severity;
        self
    }

    pub fn category(mut self, category: PatternCategory) -> Self {
        self.pattern.category = category;
        self
    }

    pub fn pattern(mut self, pattern: AbstractPattern) -> Self {
        self.pattern.pattern = pattern;
        self
    }

    pub fn language(mut self, lang: impl Into<String>) -> Self {
        self.pattern.languages.push(lang.into());
        self
    }

    pub fn remediation(mut self, remediation: impl Into<String>) -> Self {
        self.pattern.remediation = remediation.into();
        self
    }

    pub fn example(mut self, example: PatternExample) -> Self {
        self.pattern.examples.push(example);
        self
    }

    pub fn tag(mut self, tag: impl Into<String>) -> Self {
        self.pattern.tags.push(tag.into());
        self
    }

    pub fn build(self) -> VulnerabilityPattern {
        self.pattern
    }
}

/// Severity levels for patterns.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum PatternSeverity {
    Critical,
    High,
    Medium,
    Low,
    Info,
}

/// Categories of vulnerability patterns.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum PatternCategory {
    /// SQL, Command, XSS, etc.
    Injection,
    /// Weak crypto, hardcoded keys
    Cryptography,
    /// Buffer overflow, UAF
    MemorySafety,
    /// Hardcoded creds, weak sessions
    Authentication,
    /// IDOR, privilege escalation
    Authorization,
    /// PII leakage
    DataExposure,
    /// Race conditions
    Concurrency,
    /// Path traversal
    PathTraversal,
    /// Insecure deserialization
    Deserialization,
    /// Configuration issues
    Configuration,
    /// Other/uncategorized
    Other,
}

/// Example code for a pattern (vulnerable or safe).
#[derive(Debug, Clone)]
pub struct PatternExample {
    /// Language of the example.
    pub language: String,
    /// The code snippet.
    pub code: String,
    /// Whether this is vulnerable or safe code.
    pub vulnerable: bool,
    /// Explanation of why it's vulnerable/safe.
    pub explanation: String,
}

/// Abstract pattern definition - the core of APIR.
/// Note: Serialization is handled manually to avoid recursion limit issues.
#[derive(Debug, Clone)]
pub enum AbstractPattern {
    /// No pattern (placeholder).
    Empty,

    /// Match a specific AST node type.
    Node(NodePattern),

    /// Match a function/method call.
    FunctionCall(FunctionCallPattern),

    /// Match a binary operation.
    BinaryOp(BinaryOpPattern),

    /// Match data flow from source to sink.
    DataFlow(DataFlowPattern),

    /// Match a sequence of patterns.
    Sequence(Vec<AbstractPattern>),

    /// Match any of the patterns.
    AnyOf(Vec<AbstractPattern>),

    /// Match all patterns.
    AllOf(Vec<AbstractPattern>),

    /// Negation - match if pattern does NOT match.
    Not(Box<AbstractPattern>),

    /// Match a string literal.
    StringLiteral(StringPattern),

    /// Match a variable/identifier.
    Identifier(IdentifierPattern),

    /// Match an assignment.
    Assignment(AssignmentPattern),

    /// Match a conditional.
    Conditional(ConditionalPattern),

    /// Match a loop construct.
    Loop(LoopPattern),

    /// Custom tree-sitter query (escape hatch).
    Custom(CustomPattern),
}

impl AbstractPattern {
    /// Create a function call pattern.
    pub fn function_call(name: impl Into<String>) -> Self {
        AbstractPattern::FunctionCall(FunctionCallPattern {
            name: NameMatcher::Exact(name.into()),
            receiver: None,
            args: vec![],
            modifiers: vec![],
        })
    }

    /// Create a data flow pattern.
    pub fn data_flow(source: DataFlowEndpoint, sink: DataFlowEndpoint) -> Self {
        AbstractPattern::DataFlow(DataFlowPattern {
            source,
            sink,
            sanitizers: vec![],
            propagators: vec![],
        })
    }

    /// Create an "any of" pattern.
    pub fn any_of(patterns: Vec<AbstractPattern>) -> Self {
        AbstractPattern::AnyOf(patterns)
    }

    /// Create an "all of" pattern.
    pub fn all_of(patterns: Vec<AbstractPattern>) -> Self {
        AbstractPattern::AllOf(patterns)
    }

    /// Negate this pattern.
    pub fn not(self) -> Self {
        AbstractPattern::Not(Box::new(self))
    }
}

/// Pattern for matching AST nodes by type.
#[derive(Debug, Clone)]
pub struct NodePattern {
    /// Abstract node type.
    pub node_type: AbstractNodeType,
    /// Child patterns.
    pub children: Vec<AbstractPattern>,
    /// Capture name for this node.
    pub capture: Option<String>,
}

/// Abstract node types that map to concrete AST nodes per language.
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum AbstractNodeType {
    Function,
    Method,
    Class,
    Struct,
    Module,
    Parameter,
    Argument,
    Variable,
    Constant,
    Assignment,
    BinaryExpression,
    UnaryExpression,
    CallExpression,
    MemberAccess,
    IndexAccess,
    IfStatement,
    ForLoop,
    WhileLoop,
    Return,
    Import,
    StringLiteral,
    NumberLiteral,
    BooleanLiteral,
    ArrayLiteral,
    ObjectLiteral,
    TryBlock,
    CatchBlock,
    Custom(String),
}

/// Pattern for matching function/method calls.
#[derive(Debug, Clone)]
pub struct FunctionCallPattern {
    /// Function name matcher.
    pub name: NameMatcher,
    /// Optional receiver (for method calls).
    pub receiver: Option<Box<AbstractPattern>>,
    /// Argument patterns.
    pub args: Vec<ArgumentPattern>,
    /// Required modifiers (async, static, etc.).
    pub modifiers: Vec<String>,
}

/// Pattern for matching binary operations.
#[derive(Debug, Clone)]
pub struct BinaryOpPattern {
    /// Operator to match.
    pub operator: BinaryOperator,
    /// Left operand pattern.
    pub left: Box<AbstractPattern>,
    /// Right operand pattern.
    pub right: Box<AbstractPattern>,
}

/// Abstract binary operators.
#[derive(Debug, Clone)]
pub enum BinaryOperator {
    Add,
    Subtract,
    Multiply,
    Divide,
    Modulo,
    Equal,
    NotEqual,
    LessThan,
    LessEqual,
    GreaterThan,
    GreaterEqual,
    And,
    Or,
    BitwiseAnd,
    BitwiseOr,
    BitwiseXor,
    Concatenation,
    Assignment,
    Custom(String),
}

/// Data flow pattern for taint analysis.
#[derive(Debug, Clone)]
pub struct DataFlowPattern {
    /// Where tainted data originates.
    pub source: DataFlowEndpoint,
    /// Where tainted data must not reach.
    pub sink: DataFlowEndpoint,
    /// Functions that sanitize the data.
    pub sanitizers: Vec<DataFlowEndpoint>,
    /// Functions that propagate taint.
    pub propagators: Vec<DataFlowEndpoint>,
}

/// Endpoint in a data flow (source, sink, or sanitizer).
#[derive(Debug, Clone)]
pub struct DataFlowEndpoint {
    /// Category of the endpoint.
    pub category: EndpointCategory,
    /// Specific function/method names.
    pub names: Vec<NameMatcher>,
    /// Argument positions that are tainted/sinks.
    pub arg_positions: Vec<usize>,
    /// Return value is tainted/sink.
    pub returns: bool,
}

impl DataFlowEndpoint {
    /// Create a user input source.
    pub fn user_input() -> Self {
        Self {
            category: EndpointCategory::UserInput,
            names: vec![],
            arg_positions: vec![],
            returns: true,
        }
    }

    /// Create a database query sink.
    pub fn database_query() -> Self {
        Self {
            category: EndpointCategory::DatabaseQuery,
            names: vec![],
            arg_positions: vec![0],
            returns: false,
        }
    }

    /// Create a command execution sink.
    pub fn command_exec() -> Self {
        Self {
            category: EndpointCategory::CommandExecution,
            names: vec![],
            arg_positions: vec![0],
            returns: false,
        }
    }

    /// Create an HTML output sink.
    pub fn html_output() -> Self {
        Self {
            category: EndpointCategory::HtmlOutput,
            names: vec![],
            arg_positions: vec![0],
            returns: false,
        }
    }
}

/// Categories of data flow endpoints.
#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum EndpointCategory {
    /// User-controlled input.
    UserInput,
    /// Environment variables.
    Environment,
    /// File system operations.
    FileSystem,
    /// Network requests.
    Network,
    /// Database queries.
    DatabaseQuery,
    /// Command execution.
    CommandExecution,
    /// HTML/DOM output.
    HtmlOutput,
    /// Logging.
    Logging,
    /// Cryptographic operations.
    Cryptographic,
    /// Memory operations.
    MemoryOperation,
    /// Custom category.
    Custom(String),
}

/// Name matching strategies.
#[derive(Debug, Clone)]
pub enum NameMatcher {
    /// Exact match.
    Exact(String),
    /// Prefix match.
    Prefix(String),
    /// Suffix match.
    Suffix(String),
    /// Contains substring.
    Contains(String),
    /// Regex match.
    Regex(String),
    /// Any of these names.
    AnyOf(Vec<String>),
}

impl NameMatcher {
    /// Check if a name matches this pattern.
    pub fn matches(&self, name: &str) -> bool {
        match self {
            NameMatcher::Exact(s) => name == s,
            NameMatcher::Prefix(p) => name.starts_with(p),
            NameMatcher::Suffix(s) => name.ends_with(s),
            NameMatcher::Contains(c) => name.contains(c),
            NameMatcher::Regex(r) => {
                regex::Regex::new(r).map(|re| re.is_match(name)).unwrap_or(false)
            }
            NameMatcher::AnyOf(names) => names.iter().any(|n| n == name),
        }
    }
}

/// Pattern for matching function arguments.
#[derive(Debug, Clone)]
pub struct ArgumentPattern {
    /// Position (0-indexed).
    pub position: usize,
    /// Pattern to match the argument.
    pub pattern: Box<AbstractPattern>,
    /// Is this argument required?
    pub required: bool,
}

/// Pattern for matching string literals.
#[derive(Debug, Clone)]
pub struct StringPattern {
    /// Matcher for the string content.
    pub content: NameMatcher,
    /// Include raw/template strings.
    pub include_raw: bool,
    /// Include interpolated strings.
    pub include_interpolated: bool,
}

/// Pattern for matching identifiers.
#[derive(Debug, Clone)]
pub struct IdentifierPattern {
    /// Name matcher.
    pub name: NameMatcher,
    /// Scope constraints.
    pub scope: Option<ScopeConstraint>,
}

/// Scope constraints for identifier matching.
#[derive(Debug, Clone)]
pub enum ScopeConstraint {
    /// Must be local variable.
    Local,
    /// Must be parameter.
    Parameter,
    /// Must be global/module-level.
    Global,
    /// Must be class/struct member.
    Member,
    /// Any scope.
    Any,
}

/// Pattern for matching assignments.
#[derive(Debug, Clone)]
pub struct AssignmentPattern {
    /// Left-hand side (target).
    pub target: Box<AbstractPattern>,
    /// Right-hand side (value).
    pub value: Box<AbstractPattern>,
    /// Assignment type (=, +=, etc.).
    pub assignment_type: Option<String>,
}

/// Pattern for matching conditionals.
#[derive(Debug, Clone)]
pub struct ConditionalPattern {
    /// Condition expression pattern.
    pub condition: Box<AbstractPattern>,
    /// Then branch pattern.
    pub then_branch: Option<Box<AbstractPattern>>,
    /// Else branch pattern.
    pub else_branch: Option<Box<AbstractPattern>>,
}

/// Pattern for matching loops.
#[derive(Debug, Clone)]
pub struct LoopPattern {
    /// Loop type.
    pub loop_type: LoopType,
    /// Condition/iteration pattern.
    pub iterator: Option<Box<AbstractPattern>>,
    /// Body pattern.
    pub body: Option<Box<AbstractPattern>>,
}

/// Types of loops.
#[derive(Debug, Clone)]
pub enum LoopType {
    For,
    ForEach,
    While,
    DoWhile,
    Loop,
    Any,
}

/// Custom pattern using raw tree-sitter query.
#[derive(Debug, Clone)]
pub struct CustomPattern {
    /// Language-specific queries.
    pub queries: HashMap<String, String>,
    /// Default query (if language not in map).
    pub default: Option<String>,
}

/// Registry of vulnerability patterns.
#[derive(Debug, Clone, Default)]
pub struct PatternRegistry {
    patterns: HashMap<String, VulnerabilityPattern>,
    by_category: HashMap<PatternCategory, Vec<String>>,
    by_language: HashMap<String, Vec<String>>,
}

impl PatternRegistry {
    /// Create a new empty registry.
    pub fn new() -> Self {
        Self::default()
    }

    /// Create a registry with built-in patterns.
    pub fn with_builtins() -> Self {
        let mut registry = Self::new();
        register_builtin_patterns(&mut registry);
        registry
    }

    /// Register a pattern.
    pub fn register(&mut self, pattern: VulnerabilityPattern) {
        let id = pattern.id.clone();

        // Index by category
        self.by_category
            .entry(pattern.category)
            .or_default()
            .push(id.clone());

        // Index by language
        if pattern.languages.is_empty() {
            // Applies to all languages
            for lang in &["rust", "python", "javascript", "go", "java", "c", "cpp"] {
                self.by_language
                    .entry(lang.to_string())
                    .or_default()
                    .push(id.clone());
            }
        } else {
            for lang in &pattern.languages {
                self.by_language
                    .entry(lang.clone())
                    .or_default()
                    .push(id.clone());
            }
        }

        self.patterns.insert(id, pattern);
    }

    /// Get a pattern by ID.
    pub fn get(&self, id: &str) -> Option<&VulnerabilityPattern> {
        self.patterns.get(id)
    }

    /// Get all patterns.
    pub fn all(&self) -> impl Iterator<Item = &VulnerabilityPattern> {
        self.patterns.values()
    }

    /// Get patterns by category.
    pub fn by_category(&self, category: PatternCategory) -> Vec<&VulnerabilityPattern> {
        self.by_category
            .get(&category)
            .map(|ids| ids.iter().filter_map(|id| self.patterns.get(id)).collect())
            .unwrap_or_default()
    }

    /// Get patterns for a specific language.
    pub fn for_language(&self, language: &str) -> Vec<&VulnerabilityPattern> {
        self.by_language
            .get(language)
            .map(|ids| ids.iter().filter_map(|id| self.patterns.get(id)).collect())
            .unwrap_or_default()
    }

    /// Get pattern count.
    pub fn len(&self) -> usize {
        self.patterns.len()
    }

    /// Check if registry is empty.
    pub fn is_empty(&self) -> bool {
        self.patterns.is_empty()
    }
}

/// Register built-in vulnerability patterns.
fn register_builtin_patterns(registry: &mut PatternRegistry) {
    // SQL Injection
    registry.register(
        VulnerabilityPattern::builder("sql-injection")
            .name("SQL Injection")
            .description("User input concatenated into SQL query without proper sanitization")
            .cwe("CWE-89")
            .severity(PatternSeverity::Critical)
            .category(PatternCategory::Injection)
            .pattern(AbstractPattern::data_flow(
                DataFlowEndpoint::user_input(),
                DataFlowEndpoint::database_query(),
            ))
            .remediation("Use parameterized queries or prepared statements")
            .tag("owasp-top-10")
            .tag("injection")
            .build(),
    );

    // Command Injection
    registry.register(
        VulnerabilityPattern::builder("command-injection")
            .name("Command Injection")
            .description("User input passed to command execution without sanitization")
            .cwe("CWE-78")
            .severity(PatternSeverity::Critical)
            .category(PatternCategory::Injection)
            .pattern(AbstractPattern::data_flow(
                DataFlowEndpoint::user_input(),
                DataFlowEndpoint::command_exec(),
            ))
            .remediation("Use safe APIs that don't invoke shell, or sanitize input")
            .tag("owasp-top-10")
            .tag("injection")
            .build(),
    );

    // XSS
    registry.register(
        VulnerabilityPattern::builder("xss")
            .name("Cross-Site Scripting (XSS)")
            .description("User input rendered in HTML without encoding")
            .cwe("CWE-79")
            .severity(PatternSeverity::High)
            .category(PatternCategory::Injection)
            .pattern(AbstractPattern::data_flow(
                DataFlowEndpoint::user_input(),
                DataFlowEndpoint::html_output(),
            ))
            .remediation("HTML encode all user input before rendering")
            .tag("owasp-top-10")
            .tag("xss")
            .build(),
    );

    // Hardcoded Credentials
    registry.register(
        VulnerabilityPattern::builder("hardcoded-credentials")
            .name("Hardcoded Credentials")
            .description("Credentials or secrets hardcoded in source code")
            .cwe("CWE-798")
            .severity(PatternSeverity::High)
            .category(PatternCategory::Authentication)
            .pattern(AbstractPattern::AllOf(vec![
                AbstractPattern::Node(NodePattern {
                    node_type: AbstractNodeType::Assignment,
                    children: vec![],
                    capture: Some("assignment".to_string()),
                }),
                AbstractPattern::AnyOf(vec![
                    AbstractPattern::Identifier(IdentifierPattern {
                        name: NameMatcher::Contains("password".to_string()),
                        scope: None,
                    }),
                    AbstractPattern::Identifier(IdentifierPattern {
                        name: NameMatcher::Contains("secret".to_string()),
                        scope: None,
                    }),
                    AbstractPattern::Identifier(IdentifierPattern {
                        name: NameMatcher::Contains("api_key".to_string()),
                        scope: None,
                    }),
                ]),
            ]))
            .remediation("Use environment variables or secure secret management")
            .tag("secrets")
            .build(),
    );

    // Weak Cryptography
    registry.register(
        VulnerabilityPattern::builder("weak-crypto")
            .name("Weak Cryptographic Algorithm")
            .description("Use of deprecated or weak cryptographic algorithms")
            .cwe("CWE-327")
            .severity(PatternSeverity::Medium)
            .category(PatternCategory::Cryptography)
            .pattern(AbstractPattern::function_call(""))
            .remediation("Use strong, modern cryptographic algorithms")
            .tag("crypto")
            .build(),
    );

    // Path Traversal
    registry.register(
        VulnerabilityPattern::builder("path-traversal")
            .name("Path Traversal")
            .description("User input used in file path without sanitization")
            .cwe("CWE-22")
            .severity(PatternSeverity::High)
            .category(PatternCategory::PathTraversal)
            .pattern(AbstractPattern::data_flow(
                DataFlowEndpoint::user_input(),
                DataFlowEndpoint {
                    category: EndpointCategory::FileSystem,
                    names: vec![],
                    arg_positions: vec![0],
                    returns: false,
                },
            ))
            .remediation("Validate and canonicalize file paths before use")
            .tag("owasp-top-10")
            .tag("path-traversal")
            .build(),
    );

    // Insecure Deserialization
    registry.register(
        VulnerabilityPattern::builder("insecure-deserialization")
            .name("Insecure Deserialization")
            .description("Deserializing untrusted data without validation")
            .cwe("CWE-502")
            .severity(PatternSeverity::High)
            .category(PatternCategory::Deserialization)
            .pattern(AbstractPattern::data_flow(
                DataFlowEndpoint::user_input(),
                DataFlowEndpoint {
                    category: EndpointCategory::Custom("deserialization".to_string()),
                    names: vec![],
                    arg_positions: vec![0],
                    returns: false,
                },
            ))
            .remediation("Validate deserialized data or use safe serialization formats")
            .tag("owasp-top-10")
            .build(),
    );
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_pattern_builder() {
        let pattern = VulnerabilityPattern::builder("test-pattern")
            .name("Test Pattern")
            .description("A test pattern")
            .cwe("CWE-123")
            .severity(PatternSeverity::High)
            .category(PatternCategory::Injection)
            .tag("test")
            .build();

        assert_eq!(pattern.id, "test-pattern");
        assert_eq!(pattern.name, "Test Pattern");
        assert_eq!(pattern.cwes, vec!["CWE-123"]);
        assert_eq!(pattern.severity, PatternSeverity::High);
        assert_eq!(pattern.category, PatternCategory::Injection);
    }

    #[test]
    fn test_name_matcher() {
        assert!(NameMatcher::Exact("execute".to_string()).matches("execute"));
        assert!(!NameMatcher::Exact("execute".to_string()).matches("exec"));

        assert!(NameMatcher::Prefix("get".to_string()).matches("getUser"));
        assert!(!NameMatcher::Prefix("get".to_string()).matches("setUser"));

        assert!(NameMatcher::Suffix("Async".to_string()).matches("fetchAsync"));
        assert!(!NameMatcher::Suffix("Async".to_string()).matches("fetchSync"));

        assert!(NameMatcher::Contains("Sql".to_string()).matches("executeSqlQuery"));

        assert!(NameMatcher::AnyOf(vec!["exec".to_string(), "eval".to_string()]).matches("exec"));
        assert!(!NameMatcher::AnyOf(vec!["exec".to_string(), "eval".to_string()]).matches("run"));
    }

    #[test]
    fn test_pattern_registry() {
        let registry = PatternRegistry::with_builtins();

        assert!(!registry.is_empty());
        assert!(registry.get("sql-injection").is_some());
        assert!(registry.get("command-injection").is_some());

        let injection_patterns = registry.by_category(PatternCategory::Injection);
        assert!(!injection_patterns.is_empty());
    }

    #[test]
    fn test_data_flow_pattern() {
        let pattern = AbstractPattern::data_flow(
            DataFlowEndpoint::user_input(),
            DataFlowEndpoint::database_query(),
        );

        if let AbstractPattern::DataFlow(df) = pattern {
            assert_eq!(df.source.category, EndpointCategory::UserInput);
            assert_eq!(df.sink.category, EndpointCategory::DatabaseQuery);
        } else {
            panic!("Expected DataFlow pattern");
        }
    }

    #[test]
    fn test_composite_patterns() {
        let any_of = AbstractPattern::any_of(vec![
            AbstractPattern::function_call("exec"),
            AbstractPattern::function_call("eval"),
        ]);

        if let AbstractPattern::AnyOf(patterns) = any_of {
            assert_eq!(patterns.len(), 2);
        } else {
            panic!("Expected AnyOf pattern");
        }

        let negated = AbstractPattern::function_call("safe").not();

        if let AbstractPattern::Not(_) = negated {
            // OK
        } else {
            panic!("Expected Not pattern");
        }
    }
}

================================================================================
END: src\crosslang\apir.rs
================================================================================

================================================================================
FILE: src\crosslang\lang_mapping.rs
================================================================================
//! Language mapping tables for cross-language vulnerability patterns.
//!
//! This module maps abstract APIR concepts to concrete tree-sitter node types
//! and function names for each supported programming language.

use crate::crosslang::apir::{
    AbstractNodeType, AbstractPattern, EndpointCategory, NameMatcher, VulnerabilityPattern,
};
use crate::models::Language;
use std::collections::HashMap;

/// Language-specific mapping configuration.
#[derive(Debug, Clone)]
pub struct LanguageMapping {
    /// Language identifier.
    pub language: Language,
    /// Node type mappings.
    pub node_types: NodeTypeMapping,
    /// Source endpoint mappings (taint sources).
    pub sources: EndpointMapping,
    /// Sink endpoint mappings (dangerous operations).
    pub sinks: EndpointMapping,
    /// Sanitizer function mappings.
    pub sanitizers: EndpointMapping,
}

impl LanguageMapping {
    /// Get mapping for a specific language.
    pub fn for_language(lang: Language) -> Self {
        match lang {
            Language::Rust => rust_mapping(),
            Language::Python => python_mapping(),
            Language::JavaScript => javascript_mapping(),
            Language::Go => go_mapping(),
            Language::Java => java_mapping(),
            Language::C | Language::Cpp => c_mapping(),
            _ => default_mapping(lang),
        }
    }
}

/// Mapping from abstract node types to concrete tree-sitter node types.
#[derive(Debug, Clone, Default)]
pub struct NodeTypeMapping {
    mappings: HashMap<AbstractNodeType, Vec<String>>,
}

impl NodeTypeMapping {
    /// Create a new node type mapping.
    pub fn new() -> Self {
        Self::default()
    }

    /// Add a mapping.
    pub fn add(&mut self, abstract_type: AbstractNodeType, concrete_types: Vec<&str>) {
        self.mappings.insert(
            abstract_type,
            concrete_types.iter().map(|s| s.to_string()).collect(),
        );
    }

    /// Get concrete types for an abstract type.
    pub fn get(&self, abstract_type: &AbstractNodeType) -> Vec<&str> {
        self.mappings
            .get(abstract_type)
            .map(|v| v.iter().map(|s| s.as_str()).collect())
            .unwrap_or_default()
    }
}

/// Mapping for data flow endpoints (sources, sinks, sanitizers).
#[derive(Debug, Clone, Default)]
pub struct EndpointMapping {
    /// Function/method names by category.
    by_category: HashMap<EndpointCategory, Vec<EndpointInfo>>,
}

impl EndpointMapping {
    /// Create a new endpoint mapping.
    pub fn new() -> Self {
        Self::default()
    }

    /// Add an endpoint.
    pub fn add(&mut self, category: EndpointCategory, info: EndpointInfo) {
        self.by_category.entry(category).or_default().push(info);
    }

    /// Get endpoints for a category.
    pub fn get(&self, category: &EndpointCategory) -> &[EndpointInfo] {
        self.by_category
            .get(category)
            .map(|v| v.as_slice())
            .unwrap_or(&[])
    }

    /// Get all endpoints.
    pub fn all(&self) -> impl Iterator<Item = (&EndpointCategory, &Vec<EndpointInfo>)> {
        self.by_category.iter()
    }
}

/// Information about a specific endpoint (function/method).
#[derive(Debug, Clone)]
pub struct EndpointInfo {
    /// Function/method name pattern.
    pub name: NameMatcher,
    /// Optional receiver type (for method calls).
    pub receiver: Option<String>,
    /// Which argument positions are tainted/sinks.
    pub arg_positions: Vec<usize>,
    /// Whether return value is tainted.
    pub returns_tainted: bool,
    /// Required module/import.
    pub module: Option<String>,
}

impl EndpointInfo {
    /// Create a simple function endpoint.
    pub fn function(name: &str) -> Self {
        Self {
            name: NameMatcher::Exact(name.to_string()),
            receiver: None,
            arg_positions: vec![0],
            returns_tainted: true,
            module: None,
        }
    }

    /// Create a method endpoint.
    pub fn method(receiver: &str, name: &str) -> Self {
        Self {
            name: NameMatcher::Exact(name.to_string()),
            receiver: Some(receiver.to_string()),
            arg_positions: vec![0],
            returns_tainted: true,
            module: None,
        }
    }

    /// Set argument positions.
    pub fn with_args(mut self, positions: Vec<usize>) -> Self {
        self.arg_positions = positions;
        self
    }

    /// Set module requirement.
    pub fn with_module(mut self, module: &str) -> Self {
        self.module = Some(module.to_string());
        self
    }

    /// Set return taint.
    pub fn returns(mut self, tainted: bool) -> Self {
        self.returns_tainted = tainted;
        self
    }
}

/// Create Rust language mapping.
fn rust_mapping() -> LanguageMapping {
    let mut node_types = NodeTypeMapping::new();

    // Node type mappings for Rust
    node_types.add(AbstractNodeType::Function, vec!["function_item"]);
    node_types.add(AbstractNodeType::Method, vec!["function_item"]); // In impl block
    node_types.add(AbstractNodeType::Class, vec!["struct_item", "enum_item"]);
    node_types.add(AbstractNodeType::Struct, vec!["struct_item"]);
    node_types.add(AbstractNodeType::Module, vec!["mod_item"]);
    node_types.add(AbstractNodeType::Parameter, vec!["parameter"]);
    node_types.add(AbstractNodeType::Variable, vec!["identifier"]);
    node_types.add(AbstractNodeType::Assignment, vec!["let_declaration", "assignment_expression"]);
    node_types.add(AbstractNodeType::CallExpression, vec!["call_expression"]);
    node_types.add(AbstractNodeType::MemberAccess, vec!["field_expression"]);
    node_types.add(AbstractNodeType::IfStatement, vec!["if_expression"]);
    node_types.add(AbstractNodeType::ForLoop, vec!["for_expression"]);
    node_types.add(AbstractNodeType::WhileLoop, vec!["while_expression"]);
    node_types.add(AbstractNodeType::Return, vec!["return_expression"]);
    node_types.add(AbstractNodeType::StringLiteral, vec!["string_literal", "raw_string_literal"]);
    node_types.add(AbstractNodeType::TryBlock, vec!["try_expression"]);

    let mut sources = EndpointMapping::new();

    // User input sources in Rust
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::function("read_line").with_module("std::io"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::function("args").with_module("std::env"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("Request", "body"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("Request", "query"),
    );

    // Environment sources
    sources.add(
        EndpointCategory::Environment,
        EndpointInfo::function("var").with_module("std::env"),
    );
    sources.add(
        EndpointCategory::Environment,
        EndpointInfo::function("var_os").with_module("std::env"),
    );

    let mut sinks = EndpointMapping::new();

    // SQL injection sinks
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("Connection", "execute").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("Connection", "query").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::function("query!").with_args(vec![0]).with_module("sqlx"),
    );

    // Command injection sinks
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::method("Command", "new").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::method("Command", "arg").with_args(vec![0]),
    );

    // File system sinks
    sinks.add(
        EndpointCategory::FileSystem,
        EndpointInfo::function("read_to_string").with_args(vec![0]).with_module("std::fs"),
    );
    sinks.add(
        EndpointCategory::FileSystem,
        EndpointInfo::function("write").with_args(vec![0]).with_module("std::fs"),
    );
    sinks.add(
        EndpointCategory::FileSystem,
        EndpointInfo::method("File", "open").with_args(vec![0]),
    );

    let mut sanitizers = EndpointMapping::new();

    // SQL sanitizers (parameterized queries)
    sanitizers.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::function("bind").returns(true),
    );

    // General sanitizers
    sanitizers.add(
        EndpointCategory::Custom("html".to_string()),
        EndpointInfo::function("escape").with_module("html_escape"),
    );

    LanguageMapping {
        language: Language::Rust,
        node_types,
        sources,
        sinks,
        sanitizers,
    }
}

/// Create Python language mapping.
fn python_mapping() -> LanguageMapping {
    let mut node_types = NodeTypeMapping::new();

    node_types.add(AbstractNodeType::Function, vec!["function_definition"]);
    node_types.add(AbstractNodeType::Method, vec!["function_definition"]);
    node_types.add(AbstractNodeType::Class, vec!["class_definition"]);
    node_types.add(AbstractNodeType::Module, vec!["module"]);
    node_types.add(AbstractNodeType::Parameter, vec!["parameter", "default_parameter"]);
    node_types.add(AbstractNodeType::Variable, vec!["identifier"]);
    node_types.add(AbstractNodeType::Assignment, vec!["assignment", "augmented_assignment"]);
    node_types.add(AbstractNodeType::CallExpression, vec!["call"]);
    node_types.add(AbstractNodeType::MemberAccess, vec!["attribute"]);
    node_types.add(AbstractNodeType::IfStatement, vec!["if_statement"]);
    node_types.add(AbstractNodeType::ForLoop, vec!["for_statement"]);
    node_types.add(AbstractNodeType::WhileLoop, vec!["while_statement"]);
    node_types.add(AbstractNodeType::Return, vec!["return_statement"]);
    node_types.add(AbstractNodeType::StringLiteral, vec!["string"]);
    node_types.add(AbstractNodeType::TryBlock, vec!["try_statement"]);
    node_types.add(AbstractNodeType::Import, vec!["import_statement", "import_from_statement"]);

    let mut sources = EndpointMapping::new();

    // User input sources
    sources.add(EndpointCategory::UserInput, EndpointInfo::function("input"));
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("request", "args").with_module("flask"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("request", "form").with_module("flask"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("request", "GET").with_module("django"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("request", "POST").with_module("django"),
    );
    sources.add(EndpointCategory::UserInput, EndpointInfo::function("argv").with_module("sys"));

    // Environment
    sources.add(
        EndpointCategory::Environment,
        EndpointInfo::method("os.environ", "get"),
    );
    sources.add(
        EndpointCategory::Environment,
        EndpointInfo::function("getenv").with_module("os"),
    );

    let mut sinks = EndpointMapping::new();

    // SQL injection sinks
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("cursor", "execute").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("cursor", "executemany").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::function("raw").with_args(vec![0]).with_module("django.db"),
    );

    // Command injection sinks
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("system").with_args(vec![0]).with_module("os"),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("popen").with_args(vec![0]).with_module("os"),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("call").with_args(vec![0]).with_module("subprocess"),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("run").with_args(vec![0]).with_module("subprocess"),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("Popen").with_args(vec![0]).with_module("subprocess"),
    );

    // Code execution sinks
    sinks.add(
        EndpointCategory::Custom("code_execution".to_string()),
        EndpointInfo::function("eval").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::Custom("code_execution".to_string()),
        EndpointInfo::function("exec").with_args(vec![0]),
    );

    // File system sinks
    sinks.add(
        EndpointCategory::FileSystem,
        EndpointInfo::function("open").with_args(vec![0]),
    );

    // Deserialization sinks
    sinks.add(
        EndpointCategory::Custom("deserialization".to_string()),
        EndpointInfo::function("loads").with_args(vec![0]).with_module("pickle"),
    );
    sinks.add(
        EndpointCategory::Custom("deserialization".to_string()),
        EndpointInfo::function("load").with_args(vec![0]).with_module("pickle"),
    );
    sinks.add(
        EndpointCategory::Custom("deserialization".to_string()),
        EndpointInfo::function("safe_load").with_args(vec![0]).with_module("yaml"),
    );

    let mut sanitizers = EndpointMapping::new();

    // SQL sanitizers
    sanitizers.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::function("escape_string"),
    );

    // HTML sanitizers
    sanitizers.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::function("escape").with_module("html"),
    );
    sanitizers.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::function("escape").with_module("markupsafe"),
    );

    // Path sanitizers
    sanitizers.add(
        EndpointCategory::FileSystem,
        EndpointInfo::function("realpath").with_module("os.path"),
    );
    sanitizers.add(
        EndpointCategory::FileSystem,
        EndpointInfo::function("normpath").with_module("os.path"),
    );

    LanguageMapping {
        language: Language::Python,
        node_types,
        sources,
        sinks,
        sanitizers,
    }
}

/// Create JavaScript language mapping.
fn javascript_mapping() -> LanguageMapping {
    let mut node_types = NodeTypeMapping::new();

    node_types.add(AbstractNodeType::Function, vec!["function_declaration", "function", "arrow_function"]);
    node_types.add(AbstractNodeType::Method, vec!["method_definition"]);
    node_types.add(AbstractNodeType::Class, vec!["class_declaration", "class"]);
    node_types.add(AbstractNodeType::Variable, vec!["identifier"]);
    node_types.add(AbstractNodeType::Assignment, vec!["assignment_expression", "variable_declarator"]);
    node_types.add(AbstractNodeType::CallExpression, vec!["call_expression"]);
    node_types.add(AbstractNodeType::MemberAccess, vec!["member_expression"]);
    node_types.add(AbstractNodeType::IfStatement, vec!["if_statement"]);
    node_types.add(AbstractNodeType::ForLoop, vec!["for_statement", "for_in_statement", "for_of_statement"]);
    node_types.add(AbstractNodeType::WhileLoop, vec!["while_statement"]);
    node_types.add(AbstractNodeType::Return, vec!["return_statement"]);
    node_types.add(AbstractNodeType::StringLiteral, vec!["string", "template_string"]);
    node_types.add(AbstractNodeType::TryBlock, vec!["try_statement"]);
    node_types.add(AbstractNodeType::Import, vec!["import_statement"]);

    let mut sources = EndpointMapping::new();

    // Browser sources
    sources.add(EndpointCategory::UserInput, EndpointInfo::method("document", "getElementById"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::method("document", "querySelector"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::method("window", "location"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::method("document", "cookie"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::method("URLSearchParams", "get"));

    // Node.js sources
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("req", "params"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("req", "query"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("req", "body"),
    );

    // Environment
    sources.add(
        EndpointCategory::Environment,
        EndpointInfo::method("process.env", ""),
    );

    let mut sinks = EndpointMapping::new();

    // DOM XSS sinks
    sinks.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::method("element", "innerHTML").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::method("element", "outerHTML").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::method("document", "write").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::method("document", "writeln").with_args(vec![0]),
    );

    // Code execution
    sinks.add(
        EndpointCategory::Custom("code_execution".to_string()),
        EndpointInfo::function("eval").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::Custom("code_execution".to_string()),
        EndpointInfo::function("Function").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::Custom("code_execution".to_string()),
        EndpointInfo::function("setTimeout").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::Custom("code_execution".to_string()),
        EndpointInfo::function("setInterval").with_args(vec![0]),
    );

    // Node.js command execution
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("exec").with_args(vec![0]).with_module("child_process"),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("execSync").with_args(vec![0]).with_module("child_process"),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("spawn").with_args(vec![0]).with_module("child_process"),
    );

    // SQL (Node.js)
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("connection", "query").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::function("raw").with_args(vec![0]),
    );

    let mut sanitizers = EndpointMapping::new();

    // HTML sanitizers
    sanitizers.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::function("encodeURIComponent"),
    );
    sanitizers.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::function("sanitize").with_module("DOMPurify"),
    );
    sanitizers.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::method("element", "textContent"),
    );

    LanguageMapping {
        language: Language::JavaScript,
        node_types,
        sources,
        sinks,
        sanitizers,
    }
}

/// Create Go language mapping.
fn go_mapping() -> LanguageMapping {
    let mut node_types = NodeTypeMapping::new();

    node_types.add(AbstractNodeType::Function, vec!["function_declaration"]);
    node_types.add(AbstractNodeType::Method, vec!["method_declaration"]);
    node_types.add(AbstractNodeType::Struct, vec!["struct_type"]);
    node_types.add(AbstractNodeType::Variable, vec!["identifier"]);
    node_types.add(AbstractNodeType::Assignment, vec!["short_var_declaration", "assignment_statement"]);
    node_types.add(AbstractNodeType::CallExpression, vec!["call_expression"]);
    node_types.add(AbstractNodeType::MemberAccess, vec!["selector_expression"]);
    node_types.add(AbstractNodeType::IfStatement, vec!["if_statement"]);
    node_types.add(AbstractNodeType::ForLoop, vec!["for_statement"]);
    node_types.add(AbstractNodeType::Return, vec!["return_statement"]);
    node_types.add(AbstractNodeType::StringLiteral, vec!["interpreted_string_literal", "raw_string_literal"]);
    node_types.add(AbstractNodeType::Import, vec!["import_declaration"]);

    let mut sources = EndpointMapping::new();

    // HTTP sources
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("Request", "FormValue"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("Request", "URL.Query"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("Request", "Body"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::function("Args").with_module("os"),
    );

    // Environment
    sources.add(
        EndpointCategory::Environment,
        EndpointInfo::function("Getenv").with_module("os"),
    );

    let mut sinks = EndpointMapping::new();

    // SQL
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("DB", "Exec").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("DB", "Query").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("DB", "QueryRow").with_args(vec![0]),
    );

    // Command execution
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("Command").with_args(vec![0]).with_module("os/exec"),
    );

    // HTML
    sinks.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::method("ResponseWriter", "Write").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::function("Fprint").with_args(vec![1]).with_module("fmt"),
    );

    let mut sanitizers = EndpointMapping::new();

    // HTML
    sanitizers.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::function("EscapeString").with_module("html"),
    );

    // Path
    sanitizers.add(
        EndpointCategory::FileSystem,
        EndpointInfo::function("Clean").with_module("path/filepath"),
    );

    LanguageMapping {
        language: Language::Go,
        node_types,
        sources,
        sinks,
        sanitizers,
    }
}

/// Create Java language mapping.
fn java_mapping() -> LanguageMapping {
    let mut node_types = NodeTypeMapping::new();

    node_types.add(AbstractNodeType::Function, vec!["method_declaration"]);
    node_types.add(AbstractNodeType::Method, vec!["method_declaration"]);
    node_types.add(AbstractNodeType::Class, vec!["class_declaration"]);
    node_types.add(AbstractNodeType::Variable, vec!["identifier"]);
    node_types.add(AbstractNodeType::Assignment, vec!["assignment_expression"]);
    node_types.add(AbstractNodeType::CallExpression, vec!["method_invocation"]);
    node_types.add(AbstractNodeType::MemberAccess, vec!["field_access"]);
    node_types.add(AbstractNodeType::IfStatement, vec!["if_statement"]);
    node_types.add(AbstractNodeType::ForLoop, vec!["for_statement", "enhanced_for_statement"]);
    node_types.add(AbstractNodeType::WhileLoop, vec!["while_statement"]);
    node_types.add(AbstractNodeType::Return, vec!["return_statement"]);
    node_types.add(AbstractNodeType::StringLiteral, vec!["string_literal"]);
    node_types.add(AbstractNodeType::TryBlock, vec!["try_statement"]);
    node_types.add(AbstractNodeType::Import, vec!["import_declaration"]);

    let mut sources = EndpointMapping::new();

    // HTTP sources
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("HttpServletRequest", "getParameter"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("HttpServletRequest", "getHeader"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("HttpServletRequest", "getCookies"),
    );
    sources.add(
        EndpointCategory::UserInput,
        EndpointInfo::method("HttpServletRequest", "getInputStream"),
    );

    let mut sinks = EndpointMapping::new();

    // SQL
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("Statement", "executeQuery").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("Statement", "execute").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("Connection", "prepareStatement").with_args(vec![0]),
    );

    // Command
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::method("Runtime", "exec").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::method("ProcessBuilder", "command").with_args(vec![0]),
    );

    // Deserialization
    sinks.add(
        EndpointCategory::Custom("deserialization".to_string()),
        EndpointInfo::method("ObjectInputStream", "readObject"),
    );

    let mut sanitizers = EndpointMapping::new();

    // OWASP ESAPI
    sanitizers.add(
        EndpointCategory::HtmlOutput,
        EndpointInfo::method("ESAPI.encoder()", "encodeForHTML"),
    );
    sanitizers.add(
        EndpointCategory::DatabaseQuery,
        EndpointInfo::method("ESAPI.encoder()", "encodeForSQL"),
    );

    LanguageMapping {
        language: Language::Java,
        node_types,
        sources,
        sinks,
        sanitizers,
    }
}

/// Create C/C++ language mapping.
fn c_mapping() -> LanguageMapping {
    let mut node_types = NodeTypeMapping::new();

    node_types.add(AbstractNodeType::Function, vec!["function_definition"]);
    node_types.add(AbstractNodeType::Struct, vec!["struct_specifier"]);
    node_types.add(AbstractNodeType::Variable, vec!["identifier"]);
    node_types.add(AbstractNodeType::Assignment, vec!["assignment_expression"]);
    node_types.add(AbstractNodeType::CallExpression, vec!["call_expression"]);
    node_types.add(AbstractNodeType::MemberAccess, vec!["field_expression"]);
    node_types.add(AbstractNodeType::IfStatement, vec!["if_statement"]);
    node_types.add(AbstractNodeType::ForLoop, vec!["for_statement"]);
    node_types.add(AbstractNodeType::WhileLoop, vec!["while_statement"]);
    node_types.add(AbstractNodeType::Return, vec!["return_statement"]);
    node_types.add(AbstractNodeType::StringLiteral, vec!["string_literal"]);

    let mut sources = EndpointMapping::new();

    sources.add(EndpointCategory::UserInput, EndpointInfo::function("scanf"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::function("gets"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::function("fgets"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::function("getenv"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::function("read"));
    sources.add(EndpointCategory::UserInput, EndpointInfo::function("recv"));

    let mut sinks = EndpointMapping::new();

    // Buffer overflow sinks
    sinks.add(
        EndpointCategory::MemoryOperation,
        EndpointInfo::function("strcpy").with_args(vec![1]),
    );
    sinks.add(
        EndpointCategory::MemoryOperation,
        EndpointInfo::function("strcat").with_args(vec![1]),
    );
    sinks.add(
        EndpointCategory::MemoryOperation,
        EndpointInfo::function("sprintf").with_args(vec![1]),
    );
    sinks.add(
        EndpointCategory::MemoryOperation,
        EndpointInfo::function("gets").with_args(vec![0]),
    );

    // Format string sinks
    sinks.add(
        EndpointCategory::Custom("format_string".to_string()),
        EndpointInfo::function("printf").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::Custom("format_string".to_string()),
        EndpointInfo::function("fprintf").with_args(vec![1]),
    );

    // Command execution
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("system").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("popen").with_args(vec![0]),
    );
    sinks.add(
        EndpointCategory::CommandExecution,
        EndpointInfo::function("execve").with_args(vec![0]),
    );

    let mut sanitizers = EndpointMapping::new();

    // Safe alternatives
    sanitizers.add(
        EndpointCategory::MemoryOperation,
        EndpointInfo::function("strncpy"),
    );
    sanitizers.add(
        EndpointCategory::MemoryOperation,
        EndpointInfo::function("strncat"),
    );
    sanitizers.add(
        EndpointCategory::MemoryOperation,
        EndpointInfo::function("snprintf"),
    );

    LanguageMapping {
        language: Language::C,
        node_types,
        sources,
        sinks,
        sanitizers,
    }
}

/// Default/fallback mapping for unsupported languages.
fn default_mapping(lang: Language) -> LanguageMapping {
    LanguageMapping {
        language: lang,
        node_types: NodeTypeMapping::new(),
        sources: EndpointMapping::new(),
        sinks: EndpointMapping::new(),
        sanitizers: EndpointMapping::new(),
    }
}

// ============================================================================
// SQL Injection Pattern Registry
// ============================================================================

/// Comprehensive SQL injection pattern registry for cross-language detection.
#[derive(Debug, Clone, Default)]
pub struct SqlInjectionRegistry {
    /// Database-specific sinks by language.
    pub sinks: HashMap<Language, Vec<SqlSink>>,
    /// ORM-specific patterns by language.
    pub orm_patterns: HashMap<Language, Vec<OrmPattern>>,
    /// SQL sanitizers by language.
    pub sanitizers: HashMap<Language, Vec<SqlSanitizer>>,
    /// String concatenation patterns (dangerous in SQL context).
    pub concat_patterns: HashMap<Language, Vec<String>>,
}

/// Information about a SQL execution sink.
#[derive(Debug, Clone)]
pub struct SqlSink {
    /// Database driver/library name.
    pub driver: String,
    /// Function or method name.
    pub function: String,
    /// Receiver type for method calls.
    pub receiver: Option<String>,
    /// Argument positions that are vulnerable to injection.
    pub vulnerable_args: Vec<usize>,
    /// Whether this is a prepared statement factory (potentially safe).
    pub is_prepared_factory: bool,
    /// Risk level (raw query is higher than parameterized).
    pub risk: SqlRisk,
}

/// Risk level for SQL operations.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum SqlRisk {
    /// Direct raw query execution - highest risk.
    RawQuery,
    /// Query building without obvious parameterization.
    QueryBuilder,
    /// Prepared statement creation - safe if used correctly.
    PreparedStatement,
    /// ORM method that could be misused.
    OrmMethod,
}

/// ORM-specific SQL injection pattern.
#[derive(Debug, Clone)]
pub struct OrmPattern {
    /// ORM name (e.g., "SQLAlchemy", "ActiveRecord", "Hibernate").
    pub orm: String,
    /// Method that could be vulnerable.
    pub method: String,
    /// Pattern to detect unsafe usage (e.g., string interpolation in args).
    pub unsafe_pattern: String,
    /// Safe alternative pattern.
    pub safe_alternative: String,
}

/// SQL sanitization method.
#[derive(Debug, Clone)]
pub struct SqlSanitizer {
    /// Library/module name.
    pub library: String,
    /// Function name.
    pub function: String,
    /// Receiver type for method calls.
    pub receiver: Option<String>,
    /// Description of what this sanitizer does.
    pub description: String,
}

impl SqlInjectionRegistry {
    /// Create a new registry with all SQL injection patterns.
    pub fn new() -> Self {
        let mut registry = Self::default();

        // Register patterns for each language
        registry.register_rust_sql();
        registry.register_python_sql();
        registry.register_javascript_sql();
        registry.register_go_sql();
        registry.register_java_sql();
        registry.register_c_sql();

        registry
    }

    /// Get SQL sinks for a specific language.
    pub fn sinks_for(&self, lang: Language) -> &[SqlSink] {
        self.sinks.get(&lang).map(|v| v.as_slice()).unwrap_or(&[])
    }

    /// Get ORM patterns for a specific language.
    pub fn orm_patterns_for(&self, lang: Language) -> &[OrmPattern] {
        self.orm_patterns.get(&lang).map(|v| v.as_slice()).unwrap_or(&[])
    }

    /// Get SQL sanitizers for a specific language.
    pub fn sanitizers_for(&self, lang: Language) -> &[SqlSanitizer] {
        self.sanitizers.get(&lang).map(|v| v.as_slice()).unwrap_or(&[])
    }

    /// Register Rust SQL injection patterns.
    fn register_rust_sql(&mut self) {
        let sinks = vec![
            // sqlx
            SqlSink {
                driver: "sqlx".to_string(),
                function: "query".to_string(),
                receiver: None,
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "sqlx".to_string(),
                function: "query_as".to_string(),
                receiver: None,
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "sqlx".to_string(),
                function: "query_scalar".to_string(),
                receiver: None,
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // diesel (raw_sql is the only danger)
            SqlSink {
                driver: "diesel".to_string(),
                function: "sql".to_string(),
                receiver: None,
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "diesel".to_string(),
                function: "sql_query".to_string(),
                receiver: None,
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // rusqlite
            SqlSink {
                driver: "rusqlite".to_string(),
                function: "execute".to_string(),
                receiver: Some("Connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "rusqlite".to_string(),
                function: "query".to_string(),
                receiver: Some("Connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "rusqlite".to_string(),
                function: "prepare".to_string(),
                receiver: Some("Connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
            // tokio-postgres
            SqlSink {
                driver: "tokio-postgres".to_string(),
                function: "query".to_string(),
                receiver: Some("Client".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "tokio-postgres".to_string(),
                function: "execute".to_string(),
                receiver: Some("Client".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // mysql
            SqlSink {
                driver: "mysql".to_string(),
                function: "query".to_string(),
                receiver: Some("Conn".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "mysql".to_string(),
                function: "exec".to_string(),
                receiver: Some("Conn".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // sea-orm raw queries
            SqlSink {
                driver: "sea-orm".to_string(),
                function: "query_all".to_string(),
                receiver: Some("DatabaseConnection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
        ];

        let orm_patterns = vec![
            OrmPattern {
                orm: "diesel".to_string(),
                method: "filter".to_string(),
                unsafe_pattern: r#"sql::<.*>\(.*format!.*\)"#.to_string(),
                safe_alternative: "Use diesel's DSL instead of raw SQL in filter".to_string(),
            },
            OrmPattern {
                orm: "sea-orm".to_string(),
                method: "from_raw_sql".to_string(),
                unsafe_pattern: r#"from_raw_sql.*\+|format!"#.to_string(),
                safe_alternative: "Use SeaORM's query builder with bind parameters".to_string(),
            },
        ];

        let sanitizers = vec![
            SqlSanitizer {
                library: "sqlx".to_string(),
                function: "bind".to_string(),
                receiver: Some("Query".to_string()),
                description: "Parameterized query binding".to_string(),
            },
            SqlSanitizer {
                library: "rusqlite".to_string(),
                function: "params!".to_string(),
                receiver: None,
                description: "Rusqlite parameter macro".to_string(),
            },
        ];

        self.sinks.insert(Language::Rust, sinks);
        self.orm_patterns.insert(Language::Rust, orm_patterns);
        self.sanitizers.insert(Language::Rust, sanitizers);
        self.concat_patterns.insert(Language::Rust, vec![
            r#"format!\s*\(\s*"[^"]*SELECT"#.to_string(),
            r#"format!\s*\(\s*"[^"]*INSERT"#.to_string(),
            r#"format!\s*\(\s*"[^"]*UPDATE"#.to_string(),
            r#"format!\s*\(\s*"[^"]*DELETE"#.to_string(),
            r#"\+\s*".*WHERE"#.to_string(),
        ]);
    }

    /// Register Python SQL injection patterns.
    fn register_python_sql(&mut self) {
        let sinks = vec![
            // sqlite3
            SqlSink {
                driver: "sqlite3".to_string(),
                function: "execute".to_string(),
                receiver: Some("cursor".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "sqlite3".to_string(),
                function: "executemany".to_string(),
                receiver: Some("cursor".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "sqlite3".to_string(),
                function: "executescript".to_string(),
                receiver: Some("cursor".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // psycopg2
            SqlSink {
                driver: "psycopg2".to_string(),
                function: "execute".to_string(),
                receiver: Some("cursor".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "psycopg2".to_string(),
                function: "mogrify".to_string(),
                receiver: Some("cursor".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // mysql-connector-python
            SqlSink {
                driver: "mysql.connector".to_string(),
                function: "execute".to_string(),
                receiver: Some("cursor".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // PyMySQL
            SqlSink {
                driver: "pymysql".to_string(),
                function: "execute".to_string(),
                receiver: Some("cursor".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // asyncpg
            SqlSink {
                driver: "asyncpg".to_string(),
                function: "execute".to_string(),
                receiver: Some("connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "asyncpg".to_string(),
                function: "fetch".to_string(),
                receiver: Some("connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // Django
            SqlSink {
                driver: "django".to_string(),
                function: "raw".to_string(),
                receiver: Some("Manager".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "django".to_string(),
                function: "extra".to_string(),
                receiver: Some("QuerySet".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::OrmMethod,
            },
            // SQLAlchemy
            SqlSink {
                driver: "sqlalchemy".to_string(),
                function: "text".to_string(),
                receiver: None,
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "sqlalchemy".to_string(),
                function: "execute".to_string(),
                receiver: Some("engine".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
        ];

        let orm_patterns = vec![
            OrmPattern {
                orm: "Django".to_string(),
                method: "extra".to_string(),
                unsafe_pattern: r#"extra\s*\(.*where\s*=\s*\[.*%s"#.to_string(),
                safe_alternative: "Use Django's ORM methods or parameterized raw()".to_string(),
            },
            OrmPattern {
                orm: "Django".to_string(),
                method: "raw".to_string(),
                unsafe_pattern: r#"raw\s*\(.*%.*%"#.to_string(),
                safe_alternative: "Use %s placeholders with params argument".to_string(),
            },
            OrmPattern {
                orm: "SQLAlchemy".to_string(),
                method: "filter".to_string(),
                unsafe_pattern: r#"filter\s*\(.*text\s*\(.*\+|%|f""#.to_string(),
                safe_alternative: "Use SQLAlchemy's column comparison or bindparams".to_string(),
            },
            OrmPattern {
                orm: "SQLAlchemy".to_string(),
                method: "execute".to_string(),
                unsafe_pattern: r#"execute\s*\(.*f"|.*\+.*\+|.*%s.*%"#.to_string(),
                safe_alternative: "Use text() with bindparams".to_string(),
            },
        ];

        let sanitizers = vec![
            SqlSanitizer {
                library: "psycopg2".to_string(),
                function: "sql.Identifier".to_string(),
                receiver: None,
                description: "Safe identifier quoting for psycopg2".to_string(),
            },
            SqlSanitizer {
                library: "psycopg2".to_string(),
                function: "sql.Literal".to_string(),
                receiver: None,
                description: "Safe literal quoting for psycopg2".to_string(),
            },
            SqlSanitizer {
                library: "mysql.connector".to_string(),
                function: "escape_string".to_string(),
                receiver: Some("connection".to_string()),
                description: "MySQL string escaping".to_string(),
            },
            SqlSanitizer {
                library: "sqlalchemy".to_string(),
                function: "bindparam".to_string(),
                receiver: None,
                description: "SQLAlchemy bind parameter".to_string(),
            },
        ];

        self.sinks.insert(Language::Python, sinks);
        self.orm_patterns.insert(Language::Python, orm_patterns);
        self.sanitizers.insert(Language::Python, sanitizers);
        self.concat_patterns.insert(Language::Python, vec![
            r#"f"[^"]*SELECT.*\{"#.to_string(),
            r#"f"[^"]*INSERT.*\{"#.to_string(),
            r#"f"[^"]*UPDATE.*\{"#.to_string(),
            r#"f"[^"]*DELETE.*\{"#.to_string(),
            r#"".*SELECT.*"\s*%"#.to_string(),
            r#"".*WHERE.*"\s*\+"#.to_string(),
            r#"\.format\s*\(.*\).*SELECT"#.to_string(),
        ]);
    }

    /// Register JavaScript SQL injection patterns.
    fn register_javascript_sql(&mut self) {
        let sinks = vec![
            // mysql/mysql2
            SqlSink {
                driver: "mysql".to_string(),
                function: "query".to_string(),
                receiver: Some("connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "mysql2".to_string(),
                function: "query".to_string(),
                receiver: Some("connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "mysql2".to_string(),
                function: "execute".to_string(),
                receiver: Some("connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // pg (node-postgres)
            SqlSink {
                driver: "pg".to_string(),
                function: "query".to_string(),
                receiver: Some("client".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "pg".to_string(),
                function: "query".to_string(),
                receiver: Some("pool".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // better-sqlite3
            SqlSink {
                driver: "better-sqlite3".to_string(),
                function: "prepare".to_string(),
                receiver: Some("db".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
            SqlSink {
                driver: "better-sqlite3".to_string(),
                function: "exec".to_string(),
                receiver: Some("db".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // Sequelize
            SqlSink {
                driver: "sequelize".to_string(),
                function: "query".to_string(),
                receiver: Some("sequelize".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // Knex
            SqlSink {
                driver: "knex".to_string(),
                function: "raw".to_string(),
                receiver: Some("knex".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "knex".to_string(),
                function: "whereRaw".to_string(),
                receiver: Some("query".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::QueryBuilder,
            },
            SqlSink {
                driver: "knex".to_string(),
                function: "havingRaw".to_string(),
                receiver: Some("query".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::QueryBuilder,
            },
            SqlSink {
                driver: "knex".to_string(),
                function: "orderByRaw".to_string(),
                receiver: Some("query".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::QueryBuilder,
            },
            // Prisma (raw queries)
            SqlSink {
                driver: "prisma".to_string(),
                function: "$queryRaw".to_string(),
                receiver: Some("prisma".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "prisma".to_string(),
                function: "$executeRaw".to_string(),
                receiver: Some("prisma".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // TypeORM
            SqlSink {
                driver: "typeorm".to_string(),
                function: "query".to_string(),
                receiver: Some("connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "typeorm".to_string(),
                function: "query".to_string(),
                receiver: Some("manager".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
        ];

        let orm_patterns = vec![
            OrmPattern {
                orm: "Sequelize".to_string(),
                method: "query".to_string(),
                unsafe_pattern: r#"query\s*\(\s*`[^`]*\$\{"#.to_string(),
                safe_alternative: "Use replacements option with ? placeholders".to_string(),
            },
            OrmPattern {
                orm: "Knex".to_string(),
                method: "whereRaw".to_string(),
                unsafe_pattern: r#"whereRaw\s*\(\s*`[^`]*\$\{"#.to_string(),
                safe_alternative: "Use ? placeholders with bindings array".to_string(),
            },
            OrmPattern {
                orm: "TypeORM".to_string(),
                method: "where".to_string(),
                unsafe_pattern: r#"where\s*\(\s*`[^`]*\$\{"#.to_string(),
                safe_alternative: "Use query builder parameters with :param syntax".to_string(),
            },
        ];

        let sanitizers = vec![
            SqlSanitizer {
                library: "mysql".to_string(),
                function: "escape".to_string(),
                receiver: Some("connection".to_string()),
                description: "MySQL string escaping".to_string(),
            },
            SqlSanitizer {
                library: "mysql".to_string(),
                function: "escapeId".to_string(),
                receiver: Some("connection".to_string()),
                description: "MySQL identifier escaping".to_string(),
            },
            SqlSanitizer {
                library: "pg".to_string(),
                function: "escapeLiteral".to_string(),
                receiver: Some("client".to_string()),
                description: "PostgreSQL literal escaping".to_string(),
            },
            SqlSanitizer {
                library: "pg".to_string(),
                function: "escapeIdentifier".to_string(),
                receiver: Some("client".to_string()),
                description: "PostgreSQL identifier escaping".to_string(),
            },
            SqlSanitizer {
                library: "sqlstring".to_string(),
                function: "escape".to_string(),
                receiver: None,
                description: "SQL string escaping library".to_string(),
            },
        ];

        self.sinks.insert(Language::JavaScript, sinks);
        self.orm_patterns.insert(Language::JavaScript, orm_patterns);
        self.sanitizers.insert(Language::JavaScript, sanitizers);
        self.concat_patterns.insert(Language::JavaScript, vec![
            r#"`[^`]*SELECT[^`]*\$\{"#.to_string(),
            r#"`[^`]*INSERT[^`]*\$\{"#.to_string(),
            r#"`[^`]*UPDATE[^`]*\$\{"#.to_string(),
            r#"`[^`]*DELETE[^`]*\$\{"#.to_string(),
            r#"".*SELECT.*"\s*\+"#.to_string(),
            r#"'.*SELECT.*'\s*\+"#.to_string(),
        ]);
    }

    /// Register Go SQL injection patterns.
    fn register_go_sql(&mut self) {
        let sinks = vec![
            // database/sql
            SqlSink {
                driver: "database/sql".to_string(),
                function: "Exec".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "database/sql".to_string(),
                function: "Query".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "database/sql".to_string(),
                function: "QueryRow".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "database/sql".to_string(),
                function: "Prepare".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
            // GORM
            SqlSink {
                driver: "gorm".to_string(),
                function: "Raw".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "gorm".to_string(),
                function: "Exec".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "gorm".to_string(),
                function: "Where".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::OrmMethod,
            },
            // sqlx (Go)
            SqlSink {
                driver: "sqlx".to_string(),
                function: "Get".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "sqlx".to_string(),
                function: "Select".to_string(),
                receiver: Some("DB".to_string()),
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // pgx
            SqlSink {
                driver: "pgx".to_string(),
                function: "Exec".to_string(),
                receiver: Some("Conn".to_string()),
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "pgx".to_string(),
                function: "Query".to_string(),
                receiver: Some("Conn".to_string()),
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
        ];

        let orm_patterns = vec![
            OrmPattern {
                orm: "GORM".to_string(),
                method: "Where".to_string(),
                unsafe_pattern: r#"Where\s*\(\s*fmt\.Sprintf"#.to_string(),
                safe_alternative: "Use ? placeholders with separate arguments".to_string(),
            },
            OrmPattern {
                orm: "GORM".to_string(),
                method: "Raw".to_string(),
                unsafe_pattern: r#"Raw\s*\(\s*.*\+"#.to_string(),
                safe_alternative: "Use ? placeholders with variadic args".to_string(),
            },
        ];

        let sanitizers = vec![
            SqlSanitizer {
                library: "database/sql".to_string(),
                function: "Prepare".to_string(),
                receiver: Some("DB".to_string()),
                description: "Prepared statement (safe when used correctly)".to_string(),
            },
            SqlSanitizer {
                library: "squirrel".to_string(),
                function: "Eq".to_string(),
                receiver: None,
                description: "Squirrel query builder equality condition".to_string(),
            },
        ];

        self.sinks.insert(Language::Go, sinks);
        self.orm_patterns.insert(Language::Go, orm_patterns);
        self.sanitizers.insert(Language::Go, sanitizers);
        self.concat_patterns.insert(Language::Go, vec![
            r#"fmt\.Sprintf\s*\(\s*"[^"]*SELECT"#.to_string(),
            r#"fmt\.Sprintf\s*\(\s*"[^"]*INSERT"#.to_string(),
            r#"fmt\.Sprintf\s*\(\s*"[^"]*UPDATE"#.to_string(),
            r#"fmt\.Sprintf\s*\(\s*"[^"]*DELETE"#.to_string(),
            r#"".*SELECT.*"\s*\+"#.to_string(),
        ]);
    }

    /// Register Java SQL injection patterns.
    fn register_java_sql(&mut self) {
        let sinks = vec![
            // JDBC Statement
            SqlSink {
                driver: "jdbc".to_string(),
                function: "executeQuery".to_string(),
                receiver: Some("Statement".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "jdbc".to_string(),
                function: "execute".to_string(),
                receiver: Some("Statement".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "jdbc".to_string(),
                function: "executeUpdate".to_string(),
                receiver: Some("Statement".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "jdbc".to_string(),
                function: "addBatch".to_string(),
                receiver: Some("Statement".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // PreparedStatement creation
            SqlSink {
                driver: "jdbc".to_string(),
                function: "prepareStatement".to_string(),
                receiver: Some("Connection".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
            // Hibernate HQL
            SqlSink {
                driver: "hibernate".to_string(),
                function: "createQuery".to_string(),
                receiver: Some("Session".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::OrmMethod,
            },
            SqlSink {
                driver: "hibernate".to_string(),
                function: "createSQLQuery".to_string(),
                receiver: Some("Session".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "hibernate".to_string(),
                function: "createNativeQuery".to_string(),
                receiver: Some("EntityManager".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // JPA
            SqlSink {
                driver: "jpa".to_string(),
                function: "createQuery".to_string(),
                receiver: Some("EntityManager".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::OrmMethod,
            },
            SqlSink {
                driver: "jpa".to_string(),
                function: "createNativeQuery".to_string(),
                receiver: Some("EntityManager".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // Spring JDBC
            SqlSink {
                driver: "spring-jdbc".to_string(),
                function: "query".to_string(),
                receiver: Some("JdbcTemplate".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "spring-jdbc".to_string(),
                function: "queryForObject".to_string(),
                receiver: Some("JdbcTemplate".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "spring-jdbc".to_string(),
                function: "execute".to_string(),
                receiver: Some("JdbcTemplate".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "spring-jdbc".to_string(),
                function: "update".to_string(),
                receiver: Some("JdbcTemplate".to_string()),
                vulnerable_args: vec![0],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            // MyBatis
            SqlSink {
                driver: "mybatis".to_string(),
                function: "${".to_string(),  // String interpolation in XML
                receiver: None,
                vulnerable_args: vec![],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
        ];

        let orm_patterns = vec![
            OrmPattern {
                orm: "Hibernate".to_string(),
                method: "createQuery".to_string(),
                unsafe_pattern: r#"createQuery\s*\(\s*"[^"]*"\s*\+"#.to_string(),
                safe_alternative: "Use named parameters with setParameter()".to_string(),
            },
            OrmPattern {
                orm: "JPA".to_string(),
                method: "createQuery".to_string(),
                unsafe_pattern: r#"createQuery\s*\(\s*.*\+"#.to_string(),
                safe_alternative: "Use JPQL named parameters".to_string(),
            },
            OrmPattern {
                orm: "Spring".to_string(),
                method: "query".to_string(),
                unsafe_pattern: r#"query\s*\(\s*"[^"]*"\s*\+|String\.format"#.to_string(),
                safe_alternative: "Use ? placeholders with PreparedStatementSetter".to_string(),
            },
            OrmPattern {
                orm: "MyBatis".to_string(),
                method: "select".to_string(),
                unsafe_pattern: r#"\$\{[^}]+\}"#.to_string(),
                safe_alternative: "Use #{} instead of ${} for parameters".to_string(),
            },
        ];

        let sanitizers = vec![
            SqlSanitizer {
                library: "owasp-esapi".to_string(),
                function: "encodeForSQL".to_string(),
                receiver: Some("Encoder".to_string()),
                description: "OWASP ESAPI SQL encoding".to_string(),
            },
            SqlSanitizer {
                library: "jdbc".to_string(),
                function: "setString".to_string(),
                receiver: Some("PreparedStatement".to_string()),
                description: "PreparedStatement parameter binding".to_string(),
            },
            SqlSanitizer {
                library: "hibernate".to_string(),
                function: "setParameter".to_string(),
                receiver: Some("Query".to_string()),
                description: "Hibernate query parameter binding".to_string(),
            },
        ];

        self.sinks.insert(Language::Java, sinks);
        self.orm_patterns.insert(Language::Java, orm_patterns);
        self.sanitizers.insert(Language::Java, sanitizers);
        self.concat_patterns.insert(Language::Java, vec![
            r#""[^"]*SELECT[^"]*"\s*\+"#.to_string(),
            r#""[^"]*INSERT[^"]*"\s*\+"#.to_string(),
            r#""[^"]*UPDATE[^"]*"\s*\+"#.to_string(),
            r#""[^"]*DELETE[^"]*"\s*\+"#.to_string(),
            r#"String\.format\s*\(\s*"[^"]*SELECT"#.to_string(),
        ]);
    }

    /// Register C/C++ SQL injection patterns.
    fn register_c_sql(&mut self) {
        let sinks = vec![
            // MySQL C API
            SqlSink {
                driver: "mysql".to_string(),
                function: "mysql_query".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "mysql".to_string(),
                function: "mysql_real_query".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "mysql".to_string(),
                function: "mysql_stmt_prepare".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
            // SQLite C API
            SqlSink {
                driver: "sqlite3".to_string(),
                function: "sqlite3_exec".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "sqlite3".to_string(),
                function: "sqlite3_prepare".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
            SqlSink {
                driver: "sqlite3".to_string(),
                function: "sqlite3_prepare_v2".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
            // PostgreSQL libpq
            SqlSink {
                driver: "libpq".to_string(),
                function: "PQexec".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "libpq".to_string(),
                function: "PQexecParams".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "libpq".to_string(),
                function: "PQprepare".to_string(),
                receiver: None,
                vulnerable_args: vec![2],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
            // ODBC
            SqlSink {
                driver: "odbc".to_string(),
                function: "SQLExecDirect".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: false,
                risk: SqlRisk::RawQuery,
            },
            SqlSink {
                driver: "odbc".to_string(),
                function: "SQLPrepare".to_string(),
                receiver: None,
                vulnerable_args: vec![1],
                is_prepared_factory: true,
                risk: SqlRisk::PreparedStatement,
            },
        ];

        let sanitizers = vec![
            SqlSanitizer {
                library: "mysql".to_string(),
                function: "mysql_real_escape_string".to_string(),
                receiver: None,
                description: "MySQL string escaping".to_string(),
            },
            SqlSanitizer {
                library: "sqlite3".to_string(),
                function: "sqlite3_mprintf".to_string(),
                receiver: None,
                description: "SQLite safe formatting with %q".to_string(),
            },
            SqlSanitizer {
                library: "libpq".to_string(),
                function: "PQescapeLiteral".to_string(),
                receiver: None,
                description: "PostgreSQL literal escaping".to_string(),
            },
            SqlSanitizer {
                library: "libpq".to_string(),
                function: "PQescapeIdentifier".to_string(),
                receiver: None,
                description: "PostgreSQL identifier escaping".to_string(),
            },
        ];

        self.sinks.insert(Language::C, sinks);
        self.orm_patterns.insert(Language::C, vec![]);  // No ORMs in C
        self.sanitizers.insert(Language::C, sanitizers);
        self.concat_patterns.insert(Language::C, vec![
            r#"sprintf\s*\([^,]+,\s*"[^"]*SELECT"#.to_string(),
            r#"sprintf\s*\([^,]+,\s*"[^"]*INSERT"#.to_string(),
            r#"sprintf\s*\([^,]+,\s*"[^"]*UPDATE"#.to_string(),
            r#"sprintf\s*\([^,]+,\s*"[^"]*DELETE"#.to_string(),
            r#"snprintf\s*\([^,]+,[^,]+,\s*"[^"]*SELECT"#.to_string(),
            r#"strcat\s*\([^,]+,\s*"[^"]*WHERE"#.to_string(),
        ]);
    }

    /// Generate tree-sitter query for SQL injection detection in a specific language.
    pub fn generate_query(&self, lang: Language) -> String {
        let sinks = self.sinks_for(lang);

        let mut queries = Vec::new();

        for sink in sinks {
            if sink.is_prepared_factory {
                continue; // Skip prepared statements for now
            }

            let query = match lang {
                Language::Rust => {
                    if sink.receiver.is_some() {
                        format!(
                            r#"(call_expression
  function: (field_expression
    value: (_)
    field: (field_identifier) @method_name)
  arguments: (arguments
    (string_literal) @query_string)
  (#eq? @method_name "{}")) @sql_sink"#,
                            sink.function
                        )
                    } else {
                        format!(
                            r#"(call_expression
  function: (identifier) @func_name
  arguments: (arguments
    (string_literal) @query_string)
  (#eq? @func_name "{}")) @sql_sink"#,
                            sink.function
                        )
                    }
                }
                Language::Python => {
                    format!(
                        r#"(call
  function: (attribute
    attribute: (identifier) @method_name)
  arguments: (argument_list
    (string) @query_string)
  (#eq? @method_name "{}")) @sql_sink"#,
                        sink.function
                    )
                }
                Language::JavaScript => {
                    format!(
                        r#"(call_expression
  function: (member_expression
    property: (property_identifier) @method_name)
  arguments: (arguments
    (string) @query_string)
  (#eq? @method_name "{}")) @sql_sink"#,
                        sink.function
                    )
                }
                Language::Go => {
                    format!(
                        r#"(call_expression
  function: (selector_expression
    field: (field_identifier) @method_name)
  arguments: (argument_list
    (interpreted_string_literal) @query_string)
  (#eq? @method_name "{}")) @sql_sink"#,
                        sink.function
                    )
                }
                Language::Java => {
                    format!(
                        r#"(method_invocation
  name: (identifier) @method_name
  arguments: (argument_list
    (string_literal) @query_string)
  (#eq? @method_name "{}")) @sql_sink"#,
                        sink.function
                    )
                }
                Language::C | Language::Cpp => {
                    format!(
                        r#"(call_expression
  function: (identifier) @func_name
  arguments: (argument_list
    (string_literal) @query_string)
  (#eq? @func_name "{}")) @sql_sink"#,
                        sink.function
                    )
                }
                _ => continue,
            };

            queries.push(query);
        }

        queries.join("\n\n")
    }
}

/// Pattern compiler that converts APIR patterns to tree-sitter queries.
pub struct PatternCompiler {
    mappings: HashMap<Language, LanguageMapping>,
}

impl PatternCompiler {
    /// Create a new pattern compiler with default mappings.
    pub fn new() -> Self {
        let mut mappings = HashMap::new();

        for lang in &[
            Language::Rust,
            Language::Python,
            Language::JavaScript,
            Language::Go,
            Language::Java,
            Language::C,
        ] {
            mappings.insert(*lang, LanguageMapping::for_language(*lang));
        }

        Self { mappings }
    }

    /// Get the mapping for a language.
    pub fn mapping_for(&self, lang: Language) -> Option<&LanguageMapping> {
        self.mappings.get(&lang)
    }

    /// Compile an abstract pattern to a tree-sitter query for a specific language.
    pub fn compile(
        &self,
        pattern: &VulnerabilityPattern,
        language: Language,
    ) -> Result<String, String> {
        let mapping = self
            .mappings
            .get(&language)
            .ok_or_else(|| format!("No mapping for language {:?}", language))?;

        self.compile_pattern(&pattern.pattern, mapping)
    }

    fn compile_pattern(
        &self,
        pattern: &AbstractPattern,
        mapping: &LanguageMapping,
    ) -> Result<String, String> {
        match pattern {
            AbstractPattern::Empty => Ok(String::new()),

            AbstractPattern::Node(node) => {
                let concrete_types = mapping.node_types.get(&node.node_type);
                if concrete_types.is_empty() {
                    return Err(format!("No mapping for node type {:?}", node.node_type));
                }

                let type_str = concrete_types.first().unwrap();
                let capture = node
                    .capture
                    .as_ref()
                    .map(|c| format!(" @{}", c))
                    .unwrap_or_default();

                Ok(format!("({}{})", type_str, capture))
            }

            AbstractPattern::FunctionCall(_fc) => {
                let call_types = mapping.node_types.get(&AbstractNodeType::CallExpression);
                let call_type = call_types.first().copied().unwrap_or("call_expression");

                Ok(format!(
                    "({} function: (identifier) @func_name)",
                    call_type
                ))
            }

            AbstractPattern::AnyOf(patterns) => {
                let compiled: Result<Vec<_>, _> = patterns
                    .iter()
                    .map(|p| self.compile_pattern(p, mapping))
                    .collect();

                let compiled = compiled?;
                Ok(format!("[{}]", compiled.join(" ")))
            }

            AbstractPattern::AllOf(patterns) => {
                let compiled: Result<Vec<_>, _> = patterns
                    .iter()
                    .map(|p| self.compile_pattern(p, mapping))
                    .collect();

                let compiled = compiled?;
                Ok(compiled.join("\n"))
            }

            AbstractPattern::DataFlow(_) => {
                // Data flow requires taint analysis, not simple pattern matching
                Err("DataFlow patterns require taint analysis infrastructure".to_string())
            }

            _ => Err(format!("Pattern type {:?} not yet implemented", pattern)),
        }
    }
}

impl Default for PatternCompiler {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_rust_mapping() {
        let mapping = LanguageMapping::for_language(Language::Rust);

        let func_types = mapping.node_types.get(&AbstractNodeType::Function);
        assert!(func_types.contains(&"function_item"));

        let sources = mapping.sources.get(&EndpointCategory::UserInput);
        assert!(!sources.is_empty());
    }

    #[test]
    fn test_python_mapping() {
        let mapping = LanguageMapping::for_language(Language::Python);

        let call_types = mapping.node_types.get(&AbstractNodeType::CallExpression);
        assert!(call_types.contains(&"call"));

        let sinks = mapping.sinks.get(&EndpointCategory::CommandExecution);
        assert!(!sinks.is_empty());

        // Check for os.system
        let has_system = sinks.iter().any(|e| {
            if let NameMatcher::Exact(name) = &e.name {
                name == "system"
            } else {
                false
            }
        });
        assert!(has_system);
    }

    #[test]
    fn test_javascript_mapping() {
        let mapping = LanguageMapping::for_language(Language::JavaScript);

        let sinks = mapping.sinks.get(&EndpointCategory::HtmlOutput);
        assert!(!sinks.is_empty());

        // Check for innerHTML
        let has_inner_html = sinks.iter().any(|e| {
            if let NameMatcher::Exact(name) = &e.name {
                name == "innerHTML"
            } else {
                false
            }
        });
        assert!(has_inner_html);
    }

    #[test]
    fn test_endpoint_info_builder() {
        let endpoint = EndpointInfo::method("Connection", "query")
            .with_args(vec![0, 1])
            .with_module("mysql")
            .returns(false);

        assert_eq!(endpoint.receiver, Some("Connection".to_string()));
        assert_eq!(endpoint.arg_positions, vec![0, 1]);
        assert_eq!(endpoint.module, Some("mysql".to_string()));
        assert!(!endpoint.returns_tainted);
    }

    #[test]
    fn test_pattern_compiler() {
        let compiler = PatternCompiler::new();

        assert!(compiler.mapping_for(Language::Rust).is_some());
        assert!(compiler.mapping_for(Language::Python).is_some());
    }

    #[test]
    fn test_c_buffer_overflow_sinks() {
        let mapping = LanguageMapping::for_language(Language::C);

        let memory_sinks = mapping.sinks.get(&EndpointCategory::MemoryOperation);
        assert!(!memory_sinks.is_empty());

        // Check for strcpy
        let has_strcpy = memory_sinks.iter().any(|e| {
            if let NameMatcher::Exact(name) = &e.name {
                name == "strcpy"
            } else {
                false
            }
        });
        assert!(has_strcpy);
    }

    #[test]
    fn test_sql_injection_registry_rust() {
        let registry = SqlInjectionRegistry::new();

        let rust_sinks = registry.sinks_for(Language::Rust);
        assert!(!rust_sinks.is_empty());

        // Check for sqlx query
        let has_sqlx = rust_sinks.iter().any(|s| s.driver == "sqlx" && s.function == "query");
        assert!(has_sqlx);

        // Check for diesel raw SQL
        let has_diesel = rust_sinks.iter().any(|s| s.driver == "diesel" && s.function == "sql");
        assert!(has_diesel);

        // Check for rusqlite
        let has_rusqlite = rust_sinks.iter().any(|s| s.driver == "rusqlite" && s.function == "execute");
        assert!(has_rusqlite);
    }

    #[test]
    fn test_sql_injection_registry_python() {
        let registry = SqlInjectionRegistry::new();

        let python_sinks = registry.sinks_for(Language::Python);
        assert!(!python_sinks.is_empty());

        // Check for cursor.execute
        let has_execute = python_sinks.iter().any(|s| s.function == "execute");
        assert!(has_execute);

        // Check for Django raw
        let has_django = python_sinks.iter().any(|s| s.driver == "django" && s.function == "raw");
        assert!(has_django);

        // Check ORM patterns
        let orm_patterns = registry.orm_patterns_for(Language::Python);
        assert!(!orm_patterns.is_empty());

        let has_sqlalchemy = orm_patterns.iter().any(|p| p.orm == "SQLAlchemy");
        assert!(has_sqlalchemy);
    }

    #[test]
    fn test_sql_injection_registry_javascript() {
        let registry = SqlInjectionRegistry::new();

        let js_sinks = registry.sinks_for(Language::JavaScript);
        assert!(!js_sinks.is_empty());

        // Check for knex.raw
        let has_knex = js_sinks.iter().any(|s| s.driver == "knex" && s.function == "raw");
        assert!(has_knex);

        // Check for Prisma
        let has_prisma = js_sinks.iter().any(|s| s.driver == "prisma");
        assert!(has_prisma);

        // Check sanitizers
        let sanitizers = registry.sanitizers_for(Language::JavaScript);
        assert!(!sanitizers.is_empty());
    }

    #[test]
    fn test_sql_injection_registry_go() {
        let registry = SqlInjectionRegistry::new();

        let go_sinks = registry.sinks_for(Language::Go);
        assert!(!go_sinks.is_empty());

        // Check for database/sql
        let has_db_sql = go_sinks.iter().any(|s| s.driver == "database/sql" && s.function == "Query");
        assert!(has_db_sql);

        // Check for GORM
        let has_gorm = go_sinks.iter().any(|s| s.driver == "gorm" && s.function == "Raw");
        assert!(has_gorm);
    }

    #[test]
    fn test_sql_injection_registry_java() {
        let registry = SqlInjectionRegistry::new();

        let java_sinks = registry.sinks_for(Language::Java);
        assert!(!java_sinks.is_empty());

        // Check for JDBC
        let has_jdbc = java_sinks.iter().any(|s| s.driver == "jdbc" && s.function == "executeQuery");
        assert!(has_jdbc);

        // Check for Hibernate
        let has_hibernate = java_sinks.iter().any(|s| s.driver == "hibernate");
        assert!(has_hibernate);

        // Check for Spring
        let has_spring = java_sinks.iter().any(|s| s.driver == "spring-jdbc");
        assert!(has_spring);
    }

    #[test]
    fn test_sql_injection_registry_c() {
        let registry = SqlInjectionRegistry::new();

        let c_sinks = registry.sinks_for(Language::C);
        assert!(!c_sinks.is_empty());

        // Check for MySQL C API
        let has_mysql = c_sinks.iter().any(|s| s.function == "mysql_query");
        assert!(has_mysql);

        // Check for SQLite C API
        let has_sqlite = c_sinks.iter().any(|s| s.function == "sqlite3_exec");
        assert!(has_sqlite);

        // Check for PostgreSQL libpq
        let has_libpq = c_sinks.iter().any(|s| s.function == "PQexec");
        assert!(has_libpq);
    }

    #[test]
    fn test_sql_risk_classification() {
        let registry = SqlInjectionRegistry::new();

        let rust_sinks = registry.sinks_for(Language::Rust);

        // Raw queries should have RawQuery risk
        let raw_query_sink = rust_sinks.iter().find(|s| s.function == "query" && s.driver == "sqlx");
        assert!(raw_query_sink.is_some());
        assert_eq!(raw_query_sink.unwrap().risk, SqlRisk::RawQuery);

        // Prepared statement factories should have PreparedStatement risk
        let prepared_sink = rust_sinks.iter().find(|s| s.function == "prepare" && s.driver == "rusqlite");
        assert!(prepared_sink.is_some());
        assert_eq!(prepared_sink.unwrap().risk, SqlRisk::PreparedStatement);
    }

    #[test]
    fn test_sql_query_generation() {
        let registry = SqlInjectionRegistry::new();

        let rust_query = registry.generate_query(Language::Rust);
        assert!(!rust_query.is_empty());
        assert!(rust_query.contains("@sql_sink"));

        let python_query = registry.generate_query(Language::Python);
        assert!(!python_query.is_empty());
        assert!(python_query.contains("execute"));
    }
}

================================================================================
END: src\crosslang\lang_mapping.rs
================================================================================

################################################################################
# END OF CODEBASE DUMP
# Total files: 32
################################################################################
