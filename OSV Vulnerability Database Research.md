# **Engineering High-Fidelity Software Composition Analysis: A Comparative Study of Vulnerability Databases and Parsing Strategies**

## **1\. Executive Summary**

The discipline of Software Composition Analysis (SCA) is undergoing a fundamental paradigm shift. For two decades, the industry operated under a centralized model where the National Vulnerability Database (NVD) served as the singular source of truth for software security flaws. This model, predicated on the manual enrichment of Common Vulnerabilities and Exposures (CVEs) with Common Platform Enumeration (CPE) identifiers, has effectively collapsed under the weight of modern software velocity. The explosion of open-source ecosystems—npm, PyPI, Crates.io, Go, and Maven—has outpaced the capacity of centralized human analysts to curate vulnerability data in real-time.

This report presents a comprehensive architectural analysis for upgrading the src/analyzer/sca.rs engine from a prototype dependency checker to a production-grade vulnerability assessment system. The research is driven by a critical operational reality: the NVD is currently experiencing a catastrophic backlog in vulnerability enrichment, with thousands of CVEs lingering in an "Awaiting Analysis" state for weeks or months.1 During this "blind spot," legacy SCA tools that rely on NVD data structures are failing to detect active threats.

To address the research questions posed regarding coverage, lag, and accuracy, this document establishes that the Open Source Vulnerabilities (OSV) database must replace the NVD as the primary ingestion source for open-source dependencies. OSV’s distributed, automation-first model aligns with the precise Package URL (PURL) standards used by modern package managers, eliminating the ambiguous fuzzy matching required by NVD’s CPE system.3 However, OSV is not a panacea; it lacks the historical depth of NVD for legacy commercial software and often omits severity scores (CVSS) found in centralized databases. Therefore, this report advocates for a **federated querying architecture**, where OSV serves as the primary index for identification and NVD/GitHub/Snyk are queried purely for metadata enrichment (scoring, patch links).

A significant portion of this analysis is dedicated to the mechanics of version parsing. The current implementation identified in src/analyzer/sca.rs—specifically the use of string trimming to handle semantic versioning—is diagnosed as a critical failure point. Semantic versioning is not a universal constant but a collection of ecosystem-specific dialects. The report details the necessary engineering to implement discrete parsers for Node.js (node-semver), Python (pep440\_rs), and Rust (semver), ensuring that the SCA engine respects the nuanced rules of each package manager.5

Finally, the report outlines a strategy for leveraging Google's deps.dev API as a high-performance "super-node" for the SCA engine. By utilizing the GetVersionBatch endpoint, the engine can offload the computationally expensive task of transitive graph resolution and advisory lookup, effectively transforming the local SCA tool into a lightweight client for a global security intelligence graph.7

## **2\. The Crisis in Vulnerability Management: The Collapse of the Centralized Model**

To understand why a shift to OSV is necessary, one must first appreciate the systemic failure of the incumbent model. The vulnerability management ecosystem is currently bifurcated between the "Legacy Centralized" model (NVD) and the "Modern Distributed" model (OSV/GitHub).

### **2.1 The National Vulnerability Database (NVD) and the Backlog of 2024-2025**

For years, the NVD was the gold standard. Its workflow was linear: a researcher discovered a bug, a CVE ID was reserved by a CVE Numbering Authority (CNA), and eventually, the data flowed to NIST for analysis. This analysis involved two critical steps: determining the CVSS severity score and mapping the software to a CPE string.

However, starting in early 2024 and continuing through 2025, this pipeline fractured. A dramatic increase in the volume of CVE submissions—up 32% year-over-year—collided with resource constraints and contracting issues at NIST.1 The result was the formation of a massive backlog. As of mid-2025, thousands of vulnerabilities remain in a state of limbo. While the CVE ID exists, the NVD record lacks the "Affected Product" (CPE) data.

The implications for SCA tools are severe. Legacy scanners often operate by querying the NVD for a specific CPE (e.g., cpe:2.3:a:apache:log4j:\*:\*:\*). If the NVD analysts have not yet attached that CPE string to the new CVE record, the scanner returns a result of "0 Vulnerabilities Found," even if the vulnerability is critical and widely known in the community. This phenomenon, known as the **Enrichment Lag**, has shifted from a delay of days to a delay of months. During this window, organizations relying solely on NVD are exposed to "n-day" vulnerabilities that are public but undetected by their tooling.1

### **2.2 The Structural Incompatibility of CPE**

Beyond the operational backlog, the NVD suffers from a fundamental data structure problem: the Common Platform Enumeration (CPE). CPE was designed in an era of monolithic software (operating systems, hardware appliances). It attempts to categorize all software into a strict Vendor:Product:Version taxonomy.

Modern software development, however, relies on package managers (npm, Cargo, Pip, Maven) which use a different identity model. A package is identified by its registry and name (e.g., registry.npmjs.org/@angular/core). There is no algorithmic mapping between these two worlds.

* **The Mapping Gap:** Does the npm package lodash map to cpe:2.3:a:lodash:lodash or cpe:2.3:a:jdalton:lodash? If the mapping logic in the SCA tool guesses differently than the human analyst at NIST, the match fails.9  
* **False Positives:** CPEs often lack granularity. A vulnerability in cpe:2.3:a:apache:struts might be flagged against any artifact named "struts," leading to false positives on unrelated libraries.  
* **False Negatives:** The NVD often groups all versions of a library under a single CPE, or uses broad ranges. This lacks the precision to handle complex modern versioning behaviors like backported patches or pre-release exclusions.10

### **2.3 The Rise of Open Source Vulnerabilities (OSV)**

In response to these failures, the Open Source Security Foundation (OpenSSF) and Google introduced OSV. OSV is not just a database; it is a schema designed specifically for the way modern software is built.

* **PURL-Native:** OSV records use the Package URL (PURL) standard or ecosystem-specific naming (e.g., ecosystem: npm, name: axios). This enables **deterministic matching**. If the SCA tool sees axios in package-lock.json, it looks up npm/axios in OSV. There is no guessing of vendors.4  
* **Distributed Ingestion:** OSV acts as an aggregator. It does not rely on a single team of analysts. Instead, it ingests advisories from the experts of each ecosystem:  
  * **Rust:** The RustSec Advisory Database.  
  * **Python:** The PyPI Advisory Database.  
  * **GitHub:** The GitHub Advisory Database (covering npm, Maven, NuGet, Ruby).  
  * **Linux:** Debian, Alpine, and Rocky Linux security trackers.  
* **Automation:** Because OSV ingests machine-readable data from these upstream sources, the latency is near-zero. When a maintainer publishes a security advisory on GitHub, it flows into the OSV database almost immediately. This bypasses the NVD backlog entirely.12

## **3\. Comparative Analysis of Data Sources**

The accuracy of the sca.rs engine will depend heavily on the primary data source selected. The following analysis compares the major players across critical dimensions.

### **3.1 Coverage: Breadth vs. Depth**

The research question "How does OSV compare to NVD, Snyk, and GitHub?" yields a nuanced answer: OSV is superior for open-source library coverage, while NVD remains essential for legacy/commercial software.

| Metric | NVD (National Vulnerability Database) | OSV (Open Source Vulnerabilities) | Snyk (Commercial) | GitHub Advisory DB |
| :---- | :---- | :---- | :---- | :---- |
| **Primary Scope** | Commercial Software, OS, Firmware, Open Source | Open Source Packages (npm, Cargo, PyPI, Go) | Open Source \+ Containers \+ IaC | Hosted Open Source Projects |
| **Identifier** | CVE (Common Vulnerabilities & Exposures) | OSV ID (with aliases to CVE/GHSA) | Snyk ID (SNYK-\*) | GHSA (GitHub Security Advisory) |
| **Mapping Mechanism** | CPE (Fuzzy, ambiguous) | PURL / Name (Precise) | PURL / Proprietary | PURL / Name |
| **Latency (Time-to-Index)** | **High** (Months due to backlog) | **Low** (Minutes to Days) | **Low** (Hours) | **Low** (Hours) |
| **Malicious Pkg Support** | Poor (Rarely assigns CVEs to malware) | Growing (via PyPI/Malware lists) | **Excellent** (Dedicated research team) | Good (npm/Malware advisories) |
| **Data Quality** | High consistency (when analyzed) | Variable (depends on upstream source) | High (curated) | High (maintainer driven) |

Analysis of the "Unmapped" Gap:  
A critical study 9 revealed that only 11.3% of NVD records could be mapped to open-source artifacts with high precision using standard techniques. Approximately 88.7% of NVD data remains effectively "unmapped" or ambiguous when trying to correlate it automatically to package manager manifests. This statistical reality confirms that building an SCA tool solely on NVD/CPE data is structurally flawed for modern development stacks.  
**Ecosystem Specificity:**

* **Rust:** OSV ingests RustSec. NVD often lags on Rust crates unless they are high-profile (like tokio or openssl).  
* **JavaScript/npm:** GitHub Advisory DB is the primary source. OSV mirrors this. NVD is often weeks behind on npm ecosystem vulnerabilities.3  
* **Python:** OSV ingests the PyPI Advisory Database.

### **3.2 Latency and Time-to-Detection**

The "lag" between a vulnerability being exploitable and it appearing in the database is the window of maximum risk.

* **NVD Lag:** As established, the current backlog means lag can extend to 90+ days for full enrichment.1  
* **OSV Lag:** OSV's automated pipelines mean that as soon as a fix is committed and a GHSA is minted, the OSV record is live. For automated discovery (like Google's OSS-Fuzz), the lag is effectively negative relative to public disclosure—the vulnerability is found, fixed, and recorded in OSV often before a CVE is even requested.13  
* **Snyk/Commercial:** These databases employ human researchers to monitor mailing lists, commit logs, and social media. They often create a proprietary record (e.g., SNYK-JS-LODASH-123) days before a CVE is assigned. This offers the lowest latency but comes at a financial cost.3

### **3.3 False Positive/Negative Rates**

* **False Positives in NVD:** Stem from the "CPE Guessing Game." If the SCA tool guesses that commons-text maps to a generic Apache CPE, it might alert on vulnerabilities that only apply to commons-lang.  
* **False Negatives in NVD:** Stem from "Version Range Blindness." NVD records often say "versions up to 2.4". If 2.2 was patched independently (backport), NVD often fails to reflect this, or conversely, if the data is missing entirely due to the backlog.  
* **OSV Accuracy:** OSV reduces false positives by supporting **Git Commit Hashing**. It can identify that a specific build is vulnerable because it includes a vulnerable commit hash, regardless of the version string. This is unique to the OSV schema.3

## **4\. The Engineering of Version Parsing: Addressing sca.rs**

The user's code snippet identifies a critical area of concern:

Rust

// From sca.rs:264 \- Fragile version parsing  
let clean\_version \= version\_str.trim\_start\_matches(|c| c \== '^' |

| c \== '\~' ||...);

This logic is not merely "fragile"; it is semantically incorrect for almost every major package ecosystem. By stripping the operators, the code fundamentally alters the meaning of the version constraint, leading to catastrophic false positives and negatives.

### **4.1 The Fallacy of Generic SemVer**

The assumption that "Semantic Versioning" (SemVer) is a universal standard is false. While SemVer 2.0.0 exists as a specification, package managers implement "dialects" that differ in critical ways. A parser written for standard SemVer will fail when analyzing Python or Maven packages.

### **4.2 Ecosystem-Specific Parsing Analysis**

To fix sca.rs, the engine must detect the ecosystem (e.g., by the presence of Cargo.lock, package-lock.json, requirements.txt) and instantiate the correct parser.

#### **4.2.1 The npm (Node.js) Dialect**

**The Problem:** npm uses a complex range syntax including ^ (compatible with), \~ (approximate), \- (ranges), and || (unions).

* *Scenario:* A vulnerability affects \< 1.4.0. The user has ^1.2.0 in package.json.  
  * ^1.2.0 resolves to 1.2.0 \<= v \< 2.0.0.  
  * The user effectively has the *potential* to install 1.9.9 (safe) or 1.3.0 (vulnerable).  
  * **Stripping Logic Failure:** Stripping ^ leaves 1.2.0. Comparing 1.2.0 \< 1.4.0 yields TRUE (Vulnerable). But the user might actually have 1.5.0 installed. This is a false positive.  
* **The Solution:** Use the **node-semver** Rust crate.5 This crate is a port of the official JavaScript library used by npm. It correctly handles the satisfies logic for complex ranges.  
  Rust  
  // Correct Rust Implementation  
  use node\_semver::{Range, Version};

  let affected\_range: Range \= "\< 1.4.0".parse().expect("Valid range");  
  let installed\_version: Version \= "1.5.0".parse().expect("Valid version");

  if affected\_range.satisfies(\&installed\_version) {  
      println\!("Vulnerable\!");  
  } else {  
      println\!("Safe.");  
  }

#### **4.2.2 The Python (PEP 440\) Dialect**

**The Problem:** Python does not use SemVer. It uses PEP 440, which includes concepts that do not exist in SemVer:

* **Epochs:** 1\!1.0.0 (Epoch 1, version 1.0.0) is greater than 2.0.0 (Epoch 0).  
* **Post-releases:** 1.0.0.post1.  
* **Dev-releases:** 1.0.0.dev1.  
* **Arbitrary Equality:** \===1.0.0.  
* **Stripping Logic Failure:** A SemVer parser will often fail to parse 1\!1.0.0 entirely, crashing the scanner or skipping the package.  
* **The Solution:** Use the **pep440\_rs** Rust crate.6 This is the same library used by the high-performance uv package manager. It implements the complete PEP 440 specification including epoch sorting and pre-release handling.

#### **4.2.3 The Rust (Cargo) Dialect**

**The Problem:** Cargo follows SemVer 2.0.0 but with a specific caveat for 0.x.y versions.

* In standard SemVer, ^0.2.3 might technically update to 0.3.0 (depending on interpretation). In Cargo, 0.x updates are considered breaking. ^0.2.3 means \>=0.2.3, \<0.3.0.  
* **The Solution:** Use the official **semver** crate.17 It is maintained by the Rust project and guarantees alignment with Cargo's logic.

#### **4.2.4 The Maven (Java) Dialect**

**The Problem:** Maven uses a mathematical interval syntax.

* (1.0, 2.0): Exclusive (greater than 1.0, less than 2.0).  
* \[1.0, 2.0\]: Inclusive (greater than or equal to 1.0, less than or equal to 2.0).  
* **The Solution:** A dedicated Maven version parser is required. The **mvn\_version** crate 18 or a custom implementation of the ComparableVersion class from Apache Maven is needed. This logic is strictly mathematical and easier to implement than npm's fuzzy logic, but it is incompatible with SemVer parsers.

#### **4.2.5 The Go Dialect**

**The Problem:** Go uses "Pseudo-versions" for untagged commits, e.g., v0.0.0-20191109021931-daa7c04131f5.

* Comparisons require parsing the timestamp segment (20191109021931) to determine ordering. A lexical string comparison fails here.  
* **The Solution:** The logic must split the pseudo-version and compare the timestamp component if the base version is identical. The semver crate does not support this out of the box without customization or using a Go-specific wrapper.19

### **4.3 Recommended Refactoring for sca.rs**

The sca.rs file should define a trait VersionMatcher with implementations for each ecosystem.

| Ecosystem | Recommended Rust Crate | Parsing Logic |
| :---- | :---- | :---- |
| **npm** | node-semver | Range::satisfies(Version) |
| **Python** | pep440\_rs | VersionSpecifier::contains(Version) |
| **Rust** | semver | VersionReq::matches(Version) |
| **Maven** | mvn\_version / Custom | Interval Set Inclusion |
| **Go** | Custom / semver \+ Time | Pseudo-version decomposition |

Handling Ambiguity in Reports:  
When an OSV report specifies a range (e.g., events: \[{introduced: "1.0"}, {fixed: "1.2"}\]), the SCA engine must interpret this as an interval \`

* **Graph Resolution:** Unlike a local scanner which must compute the dependency tree (a difficult task requiring ecosystem toolchains), deps.dev allows querying the *dependencies* of a package. This effectively allows "Cloud-side Dependency Resolution."

**Implementation Logic for sca.rs:**

1. **Parse Lockfile:** Extract all package versions from Cargo.lock or package-lock.json. Convert them to PURLs.  
2. **Batch Query:** Chunk the PURLs (e.g., groups of 500\) and send to https://api.deps.dev/v3alpha/versions/batch.  
3. **Process Response:**  
   * Iterate through the results.  
   * If advisoryKeys is not empty, the package has direct vulnerabilities.  
   * Collect the OSV IDs.  
4. **Fetch Details:** For each unique OSV ID found, fetch the full advisory detail (either from deps.dev/v3alpha/advisories/{id} or a local OSV cache) to get the description and remediation steps.

### **5.2 Cross-Referencing for Enrichment**

While OSV provides the *detection*, it often lacks the *context* that enterprise users demand (specifically CVSS scores and CWE classifications). This is where cross-referencing comes in.

* **The Linkage:** Most OSV records contain an aliases array.  
  JSON  
  "id": "GHSA-xxxx-xxxx-xxxx",  
  "aliases": \["CVE-2023-12345"\]

* **The Federated Workflow:**  
  1. Detect vulnerability via OSV (High Precision).  
  2. Extract CVE-2023-12345 from the alias field.  
  3. Query the NVD API (or a local NVD mirror) for CVE-2023-12345.  
  4. Extract the CVSS V3 score (e.g., 9.8 Critical) and CWE ID (e.g., CWE-79 XSS) from the NVD record.  
  5. **Merge Data:** Present a unified report to the user: "Vulnerability GHSA-xxxx (CVE-2023-12345) detected. Severity: Critical (9.8). Fixed in version 1.2.4."

This hybrid approach leverages OSV's superior mapping accuracy while retaining the standardized scoring metrics of the NVD.3

### **5.3 Handling Rate Limits and Offline Operation**

Relying on public APIs introduces rate limit risks.

* **deps.dev Limits:** While generous, the API will return 429s under heavy load. The SCA engine must implement **exponential backoff** and strict concurrency limits.21  
* **Offline Mode (Recommended for Enterprise):** For high-volume environments (e.g., scanning every CI build), the "Online API" approach is fragile.  
  * **Architecture:** The SCA engine should support an "Offline Mode" where it downloads the OSV database dumps (JSON files stored in GCS buckets) at startup or on a schedule.  
  * **In-Memory Index:** Load the OSV data into an optimized in-memory structure (e.g., a Radix Tree mapping PURLs to Advisories).  
  * **Performance:** This allows identifying vulnerabilities in microseconds without network calls, eliminating the rate limit problem entirely.22

## **6\. Detailed Research Question Answers**

Q1: How does OSV compare to NVD, Snyk, GitHub Advisory Database in coverage?  
OSV effectively supersedes NVD for open-source library coverage. It aggregates GitHub, RustSec, PyPI, and Linux sources, covering the "long tail" of libraries that NVD misses (88% of open source vulnerabilities are effectively unmapped in NVD). However, Snyk and other commercial DBs still maintain an edge in detecting malware and providing curated remediation advice for complex enterprise scenarios. For a general-purpose SCA tool, OSV provides the highest "free" coverage available.9  
Q2: What's the typical lag between CVE publication and OSV entry?  
NVD lag is currently catastrophic (30-90+ days for enrichment). OSV lag is minimal (minutes to days). Because OSV ingests directly from the source (GitHub, Maintainers), it often contains actionable data weeks before the NVD assigns a CPE. This "negative lag" is the primary driver for switching to OSV.1  
Q3: Can we cross-reference multiple databases to improve coverage?  
Yes. The optimal strategy is to use OSV as the "Index" (to find the vulnerability) and NVD as the "Dictionary" (to define its severity). The aliases field in OSV records serves as the foreign key linking these datasets.  
Q4: How do we handle version range ambiguity in vulnerability reports?  
Ambiguity is largely a product of poor parsing. By abandoning generic string manipulation and adopting ecosystem-specific parsers (node-semver, pep440\_rs), "ambiguity" resolves into deterministic boolean logic. The only remaining ambiguity is when a report lacks clear fixed versions; in such cases, the tool should err on the side of caution (flagging as potential) or utilize "reachability analysis" if the language supports it (analyzing call graphs to see if the vulnerable function is used).24

## **7\. Strategic Recommendations for sca.rs Refactoring**

To transform the current codebase into a robust scanning engine, the following roadmap is recommended:

1. **Refactor Version Parsing:** Immediately replace the trim\_start\_matches logic. Introduce the node-semver, pep440\_rs, and semver crates. Create a VersionParser factory that switches implementation based on the lockfile type detected.  
2. **Implement PURL:** Standardize all internal package representation to Package URLs. This simplifies lookups across OSV and deps.dev.  
3. **Integrate deps.dev Batching:** Replace singular HTTP queries with a batching mechanism targeting the v3alpha API. This will increase scanning speed by orders of magnitude.  
4. **Hybrid Data Model:** Modify the internal Vulnerability struct to hold both OSV-derived data (ID, Fixed Version) and NVD-derived data (CVSS Score). Implement a "hydrator" step that fetches NVD data only when a vulnerability is confirmed via OSV.  
5. **Lockfile First:** Prioritize scanning Cargo.lock, package-lock.json, etc., over manifests. This ensures the scanner is checking the exact versions deployed, removing the ambiguity of dependency resolution ranges.

By executing this roadmap, the SCA engine will evolve from a fragile prototype into a high-precision security tool capable of operating effectively in the fragmented landscape of modern software development.

### **Table 1: Ecosystem Parser Implementation Matrix**

| Ecosystem | Lockfile | Versioning Standard | Rust Crate Recommendation | Notes |
| :---- | :---- | :---- | :---- | :---- |
| **Node.js** | package-lock.json | SemVer (npm flavor) | node-semver | Essential for handling \~, ^, \` |
| **Rust** | Cargo.lock | SemVer (Cargo flavor) | semver | Standard crate; handle 0.x breaking changes correctly. |
| **Python** | requirements.txt / poetry.lock | PEP 440 | pep440\_rs | Must handle epochs (1\!) and post-releases. |
| **Java** | pom.xml (Effective POM) | Maven Versioning | mvn\_version | Use set notation parsing logic. |
| **Go** | go.sum | SemVer \+ Pseudo-versions | Custom / semver | Requires timestamp parsing for pseudo-versions. |

## **8\. Conclusion**

The "dependency vulnerability database accuracy" problem is, at its core, a data structural problem. The NVD's centralized, human-curated model cannot scale to the decentralized, automated reality of open source. The operational collapse of the NVD in 2024 has merely accelerated the inevitable transition to distributed models like OSV.

For the user's SCA engine, this means that **accuracy** is no longer achieved by querying a government database for a vendor string. It is achieved by implementing precise, ecosystem-aware parsing logic that speaks the native language of npm, pip, and cargo. By combining this precise local parsing with the global intelligence of the deps.dev federated graph, the tool can achieve a level of fidelity that far exceeds legacy compliance scanners. The path forward is clear: Federated data, PURL identity, and strict ecosystem-specific version parsing.

#### **Works cited**

1. NVD Database Update Frequency: Understanding CVE Enrichment Timelines in 2025, accessed January 6, 2026, [https://inventivehq.com/blog/nvd-database-update-frequency-and-cve-enrichment-timeline](https://inventivehq.com/blog/nvd-database-update-frequency-and-cve-enrichment-timeline)  
2. When the System Breaks: What the NIST NVD Audit Means for Software Security \- HeroDevs, accessed January 6, 2026, [https://www.herodevs.com/blog-posts/when-the-system-breaks-what-the-nist-nvd-audit-means-for-software-security](https://www.herodevs.com/blog-posts/when-the-system-breaks-what-the-nist-nvd-audit-means-for-software-security)  
3. Making Sense of Open-Source Vulnerability Databases: NVD, OSV, and more, accessed January 6, 2026, [https://blog.gitguardian.com/open-source-vulnerability-databases-comparison/](https://blog.gitguardian.com/open-source-vulnerability-databases-comparison/)  
4. OSV \- Open Source Vulnerabilities, accessed January 6, 2026, [https://osv.dev/](https://osv.dev/)  
5. node-semver \- crates.io: Rust Package Registry, accessed January 6, 2026, [https://crates.io/crates/node-semver](https://crates.io/crates/node-semver)  
6. pep440\_rs — Rust parser // Lib.rs, accessed January 6, 2026, [https://lib.rs/crates/pep440\_rs](https://lib.rs/crates/pep440_rs)  
7. API | Open Source Insights, accessed January 6, 2026, [https://docs.deps.dev/api/v3alpha/](https://docs.deps.dev/api/v3alpha/)  
8. The Vulnerability Data Crisis: Why You Can't Trust Your Security Tools (And What to Do About It) | The Sequence, accessed January 6, 2026, [https://the-sequence.com/the-vulnerability-data-crisis](https://the-sequence.com/the-vulnerability-data-crisis)  
9. Mapping NVD Records to Their VFCs: How Hard is it? \- arXiv, accessed January 6, 2026, [https://arxiv.org/html/2506.09702v1](https://arxiv.org/html/2506.09702v1)  
10. Google's OSV: Open Source Vulnerabilities Database Explained \- GoCodeo, accessed January 6, 2026, [https://www.gocodeo.com/post/googles-osv-open-source-vulnerabilities-database-explained](https://www.gocodeo.com/post/googles-osv-open-source-vulnerabilities-database-explained)  
11. Properties of a High Quality OSV Record \- Google, accessed January 6, 2026, [https://google.github.io/osv.dev/data\_quality.html](https://google.github.io/osv.dev/data_quality.html)  
12. Open Source Vulnerability format \- GitHub Pages, accessed January 6, 2026, [https://ossf.github.io/osv-schema/](https://ossf.github.io/osv-schema/)  
13. The Year in Review \- OSV \- Open Source Vulnerabilities, accessed January 6, 2026, [https://osv.dev/blog/posts/2024-in-review/](https://osv.dev/blog/posts/2024-in-review/)  
14. OSV Scanner vs npm-audit: A detailed comparison of SCA tools \- Jit.io, accessed January 6, 2026, [https://www.jit.io/resources/appsec-tools/osv-scanner-vs-npm-audit-a-detailed-comparison-of-sca-tools](https://www.jit.io/resources/appsec-tools/osv-scanner-vs-npm-audit-a-detailed-comparison-of-sca-tools)  
15. nodejs-semver \- crates.io: Rust Package Registry, accessed January 6, 2026, [https://crates.io/crates/nodejs-semver](https://crates.io/crates/nodejs-semver)  
16. pep440\_rs \- Rust \- Docs.rs, accessed January 6, 2026, [https://docs.rs/pep440\_rs](https://docs.rs/pep440_rs)  
17. semver \- Rust \- Docs.rs, accessed January 6, 2026, [https://docs.rs/semver](https://docs.rs/semver)  
18. mvn\_version — Rust parser // Lib.rs, accessed January 6, 2026, [https://lib.rs/crates/mvn\_version](https://lib.rs/crates/mvn_version)  
19. osv package \- golang.org/x/vuln/internal/osv \- Go Packages, accessed January 6, 2026, [https://pkg.go.dev/golang.org/x/vuln/internal/osv](https://pkg.go.dev/golang.org/x/vuln/internal/osv)  
20. internal/osv/osv.go \- vuln \- Git at Google, accessed January 6, 2026, [https://go.googlesource.com/vuln/+/v1.1.4/internal/osv/osv.go](https://go.googlesource.com/vuln/+/v1.1.4/internal/osv/osv.go)  
21. Rate limits for \`api.deps.dev\` · Issue \#33 \- GitHub, accessed January 6, 2026, [https://github.com/google/deps.dev/issues/33](https://github.com/google/deps.dev/issues/33)  
22. Open Source Vulnerabilities | Dependency-Track, accessed January 6, 2026, [https://docs.dependencytrack.org/datasources/osv/](https://docs.dependencytrack.org/datasources/osv/)  
23. Top Open Source Dependency Scanners in 2025 \- Aikido, accessed January 6, 2026, [https://www.aikido.dev/blog/top-open-source-dependency-scanners](https://www.aikido.dev/blog/top-open-source-dependency-scanners)  
24. Software Composition Analysis (SCA) \- Arnica Documentation, accessed January 6, 2026, [https://docs.arnica.io/arnica-documentation/code-risks/software-composition-analysis-sca](https://docs.arnica.io/arnica-documentation/code-risks/software-composition-analysis-sca)